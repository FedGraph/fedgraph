apiVersion: ray.io/v1
kind: RayService
metadata:
  name: ray-service
spec:
  # More details here on some of the fields: https://docs.ray.io/en/latest/serve/production-guide/kubernetes.html#setting-up-a-rayservice-custom-resource-cr
  serviceUnhealthySecondThreshold: 720
  deploymentUnhealthySecondThreshold: 720
  serveConfigV2: |
    applications:
    - name: rayllm-serve
      route_prefix: /
      import_path: rayllm.backend:router_application
      args:
        models:
        - "./models/tiiuae/falcon-7b-instruct.yaml"
  rayClusterConfig:
    headGroupSpec:
      serviceType: NodePort
      rayStartParams:
        resources: '"{\"accelerator_type_cpu\": 2}"'
        dashboard-host: '0.0.0.0'
        block: 'true'
      template:
        spec:
          containers:
          - name: ray-head
            image: anyscale/ray-llm:0.5.0
            resources:
              limits:
                cpu: "2"
                memory: "8Gi"
              requests:
                cpu: "2"
                memory: "8Gi"
            volumeMounts:
            - mountPath: /home/ray/models
              name: model
            ports:
            - containerPort: 6379
              name: gcs-server
            - containerPort: 8265
              name: dashboard
            - containerPort: 10001
              name: client
            - containerPort: 8000
              name: serve
          volumes:
          - name: model
            configMap:
              name: falcon-7b-instruct-config
              items:
              - key: falcon-7b-instruct.yaml
                path: tiiuae/falcon-7b-instruct.yaml
    workerGroupSpecs:
    # Worker with Scale to 0 configuration
    - replicas: 1
      minReplicas: 1
      maxReplicas: 4
      groupName: gpu-worker-group
      rayStartParams:
        block: 'true'
        resources: '"{\"accelerator_type_cpu\": 20, \"accelerator_type_l4\": 1}"'
      # pod template
      template:
        spec:
          containers:
          - name: llm
            image: anyscale/ray-llm:0.5.0
            lifecycle:
              preStop:
                exec:
                  command: ["/bin/sh","-c","ray stop"]
            resources:
              limits:
                cpu: "20"
                memory: "64Gi"
                nvidia.com/gpu: "2"
              requests:
                cpu: "20"
                memory: "64Gi"
                nvidia.com/gpu: "2"
          # Here we are loading the model from the configmap with the name falcon-7b-instruct-config
            volumeMounts:
              - mountPath: /home/ray/models
                name: model
          # Please ensure the following taint has been applied to the GPU node in the cluster.
          tolerations:
            - key: "ray.io/node-type"
              operator: "Equal"
              value: "worker"
              effect: "NoSchedule"
            - key: "nvidia.com/gpu"
              operator: "Equal"
              value: "present"
              effect: "NoSchedule"
          nodeSelector:
            cloud.google.com/gke-accelerator: nvidia-l4
            # cloud.google.com/gke-accelerator: nvidia-tesla-t4  # to use: nvidia-tesla-t4 GPUs
          volumes:
          - name: model
            configMap:
              name: falcon-7b-instruct-config
              items:
              - key: falcon-7b-instruct.yaml
                path: tiiuae/falcon-7b-instruct.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: falcon-7b-instruct-config
data:
  falcon-7b-instruct.yaml: |
    deployment_config:
      autoscaling_config:
        min_replicas: 1
        initial_replicas: 1
        max_replicas: 4
        target_num_ongoing_requests_per_replica: 24
        metrics_interval_s: 10.0
        look_back_period_s: 60.0
        smoothing_factor: 0.5
        downscale_delay_s: 600.0
        upscale_delay_s: 30.0
      max_concurrent_queries: 24
      ray_actor_options:
        resources:
          accelerator_type_l4: 0.1 # use: accelerator_type_t4 for T4 GPUs
    engine_config:
      model_id: tiiuae/falcon-7b-instruct
      hf_model_id: tiiuae/falcon-7b-instruct
      type: VLLMEngine
      engine_kwargs:
        trust_remote_code: false
        max_num_batched_tokens: 2048
        max_num_seqs: 64
        gpu_memory_utilization: 0.9
      max_total_tokens: 2048
      generation:
        prompt_format:
          system: "{instruction}\n\n"
          assistant: " {instruction} </s><s>"
          trailing_assistant: ""
          user: "[INST] {system}{instruction}"
          system_in_user: true
          default_system_message: ""
        stopping_sequences: ["<unk>"]
    scaling_config:
      num_workers: 1
      num_gpus_per_worker: 1
      num_cpus_per_worker: 3
      placement_strategy: "PACK"
      resources_per_worker:
        accelerator_type_l4: 0.1 # use: accelerator_type_t4 for T4 GPUs
