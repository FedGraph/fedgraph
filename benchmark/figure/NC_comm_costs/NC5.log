2025-07-29 15:34:38,796	INFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_9fce5895dde9d456.zip.
2025-07-29 15:34:38,798	INFO packaging.py:575 -- Creating a file package for local module '.'.
Job submission server address: http://localhost:8265

-------------------------------------------------------
Job 'raysubmit_Pfutn9ATPerA72Wr' submitted successfully
-------------------------------------------------------

Next steps
  Query the logs of the job:
    ray job logs raysubmit_Pfutn9ATPerA72Wr
  Query the status of the job:
    ray job status raysubmit_Pfutn9ATPerA72Wr
  Request the job to be stopped:
    ray job stop raysubmit_Pfutn9ATPerA72Wr

Tailing logs until the job exits (disable with --no-wait):
INFO:matplotlib.font_manager:generated new fontManager

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: cora, Trainers: 5, Distribution: average, IID Beta: 10000.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'cora', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 5, 'num_hops': 0, 'iid_beta': 10000.0, 'distribution_type': 'average', 'gpu': False}
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x to ./data/cora/raw/ind.cora.x...
Downloaded ./data/cora/raw/ind.cora.x
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx to ./data/cora/raw/ind.cora.tx...
Downloaded ./data/cora/raw/ind.cora.tx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx to ./data/cora/raw/ind.cora.allx...
Downloaded ./data/cora/raw/ind.cora.allx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y to ./data/cora/raw/ind.cora.y...
Downloaded ./data/cora/raw/ind.cora.y
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty to ./data/cora/raw/ind.cora.ty...
Downloaded ./data/cora/raw/ind.cora.ty
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally to ./data/cora/raw/ind.cora.ally...
Downloaded ./data/cora/raw/ind.cora.ally
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph to ./data/cora/raw/ind.cora.graph...
Downloaded ./data/cora/raw/ind.cora.graph
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index to ./data/cora/raw/ind.cora.test.index...
Downloaded ./data/cora/raw/ind.cora.test.index
Initialization start: network data collected.
2025-07-29 22:34:53,257	INFO worker.py:1429 -- Using address 192.168.23.53:6379 set in the environment variable RAY_ADDRESS
2025-07-29 22:34:53,258	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.23.53:6379...
2025-07-29 22:34:53,267	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.23.53:8265 [39m[22m
Changing method to FedAvg
[36m(pid=4884, ip=192.168.0.191)[0m INFO:matplotlib.font_manager:generated new fontManager
[36m(Trainer pid=4884, ip=192.168.0.191)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=4884, ip=192.168.0.191)[0m   return torch.load(io.BytesIO(b))
/usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
//Log init_time: 10143.052000000001 ms //end
//Log Large1 init network: 1400989.0 //end
//Log Large2 init network: 192295.0 //end
//Log Large3 init network: 175868.0 //end
//Log Large4 init network: 156198.0 //end
//Log Server init network: 37116473.0 //end
//Log Initialization Communication Cost (MB): 37.23 //end
Pretrain start time recorded.
//pretrain_time: 6.407 ms//end
//Log Max memory for Large1: 3366023168.0 //end
//Log Max memory for Large2: 1142992896.0 //end
//Log Max memory for Large3: 714469376.0 //end
//Log Max memory for Large4: 712278016.0 //end
//Log Max memory for Server: 1567412224.0 //end
//Log Large1 network: 3668199.0 //end
//Log Large2 network: 688147.0 //end
//Log Large3 network: 519671.0 //end
//Log Large4 network: 510778.0 //end
//Log Server network: 1730318.0 //end
//Log Total Actual Pretrain Comm Cost: 6.79 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1600
Round 2: Global Test Accuracy = 0.1690
Round 3: Global Test Accuracy = 0.1700
Round 4: Global Test Accuracy = 0.1690
Round 5: Global Test Accuracy = 0.1710
Round 6: Global Test Accuracy = 0.1720
Round 7: Global Test Accuracy = 0.1780
Round 8: Global Test Accuracy = 0.1790
Round 9: Global Test Accuracy = 0.1870
Round 10: Global Test Accuracy = 0.1860
Round 11: Global Test Accuracy = 0.1920
Round 12: Global Test Accuracy = 0.2000
Round 13: Global Test Accuracy = 0.2080
Round 14: Global Test Accuracy = 0.2150
Round 15: Global Test Accuracy = 0.2230
Round 16: Global Test Accuracy = 0.2280
Round 17: Global Test Accuracy = 0.2380
Round 18: Global Test Accuracy = 0.2490
Round 19: Global Test Accuracy = 0.2510
Round 20: Global Test Accuracy = 0.2580
Round 21: Global Test Accuracy = 0.2640
Round 22: Global Test Accuracy = 0.2730
Round 23: Global Test Accuracy = 0.2760
Round 24: Global Test Accuracy = 0.2840
Round 25: Global Test Accuracy = 0.2970
Round 26: Global Test Accuracy = 0.3050
Round 27: Global Test Accuracy = 0.3010
Round 28: Global Test Accuracy = 0.3080
Round 29: Global Test Accuracy = 0.3120
Round 30: Global Test Accuracy = 0.3140
Round 31: Global Test Accuracy = 0.3210
Round 32: Global Test Accuracy = 0.3180
Round 33: Global Test Accuracy = 0.3250
Round 34: Global Test Accuracy = 0.3240
Round 35: Global Test Accuracy = 0.3280
Round 36: Global Test Accuracy = 0.3330
Round 37: Global Test Accuracy = 0.3370
Round 38: Global Test Accuracy = 0.3370
Round 39: Global Test Accuracy = 0.3470
Round 40: Global Test Accuracy = 0.3510
Round 41: Global Test Accuracy = 0.3640
Round 42: Global Test Accuracy = 0.3730
Round 43: Global Test Accuracy = 0.3720
Round 44: Global Test Accuracy = 0.3760
Round 45: Global Test Accuracy = 0.3830
Round 46: Global Test Accuracy = 0.3870
Round 47: Global Test Accuracy = 0.3890
Round 48: Global Test Accuracy = 0.3950
Round 49: Global Test Accuracy = 0.4010
Round 50: Global Test Accuracy = 0.4040
Round 51: Global Test Accuracy = 0.4120
Round 52: Global Test Accuracy = 0.4080
Round 53: Global Test Accuracy = 0.4170
Round 54: Global Test Accuracy = 0.4190
Round 55: Global Test Accuracy = 0.4200
Round 56: Global Test Accuracy = 0.4220
Round 57: Global Test Accuracy = 0.4260
Round 58: Global Test Accuracy = 0.4310
Round 59: Global Test Accuracy = 0.4360
Round 60: Global Test Accuracy = 0.4290
Round 61: Global Test Accuracy = 0.4310
Round 62: Global Test Accuracy = 0.4360
Round 63: Global Test Accuracy = 0.4380
Round 64: Global Test Accuracy = 0.4450
Round 65: Global Test Accuracy = 0.4480
Round 66: Global Test Accuracy = 0.4510
Round 67: Global Test Accuracy = 0.4530
Round 68: Global Test Accuracy = 0.4560
Round 69: Global Test Accuracy = 0.4570
Round 70: Global Test Accuracy = 0.4570
Round 71: Global Test Accuracy = 0.4640
Round 72: Global Test Accuracy = 0.4680
Round 73: Global Test Accuracy = 0.4750
Round 74: Global Test Accuracy = 0.4780
Round 75: Global Test Accuracy = 0.4790
Round 76: Global Test Accuracy = 0.4800
Round 77: Global Test Accuracy = 0.4810
Round 78: Global Test Accuracy = 0.4860
Round 79: Global Test Accuracy = 0.4890
Round 80: Global Test Accuracy = 0.4980
Round 81: Global Test Accuracy = 0.5040
Round 82: Global Test Accuracy = 0.5060
Round 83: Global Test Accuracy = 0.5090
Round 84: Global Test Accuracy = 0.5100
Round 85: Global Test Accuracy = 0.5070
Round 86: Global Test Accuracy = 0.5130
Round 87: Global Test Accuracy = 0.5180
Round 88: Global Test Accuracy = 0.5200
Round 89: Global Test Accuracy = 0.5230
Round 90: Global Test Accuracy = 0.5250
Round 91: Global Test Accuracy = 0.5250
Round 92: Global Test Accuracy = 0.5280
Round 93: Global Test Accuracy = 0.5340
Round 94: Global Test Accuracy = 0.5290
Round 95: Global Test Accuracy = 0.5420
Round 96: Global Test Accuracy = 0.5400
Round 97: Global Test Accuracy = 0.5440
Round 98: Global Test Accuracy = 0.5510
Round 99: Global Test Accuracy = 0.5530
Round 100: Global Test Accuracy = 0.5530
Round 101: Global Test Accuracy = 0.5540
Round 102: Global Test Accuracy = 0.5580
Round 103: Global Test Accuracy = 0.5590
Round 104: Global Test Accuracy = 0.5610
Round 105: Global Test Accuracy = 0.5640
Round 106: Global Test Accuracy = 0.5650
Round 107: Global Test Accuracy = 0.5710
Round 108: Global Test Accuracy = 0.5710
Round 109: Global Test Accuracy = 0.5760
Round 110: Global Test Accuracy = 0.5780
Round 111: Global Test Accuracy = 0.5770
Round 112: Global Test Accuracy = 0.5830
Round 113: Global Test Accuracy = 0.5840
Round 114: Global Test Accuracy = 0.5850
Round 115: Global Test Accuracy = 0.5890
Round 116: Global Test Accuracy = 0.5890
Round 117: Global Test Accuracy = 0.5900
Round 118: Global Test Accuracy = 0.5910
Round 119: Global Test Accuracy = 0.5920
Round 120: Global Test Accuracy = 0.5940
Round 121: Global Test Accuracy = 0.5970
Round 122: Global Test Accuracy = 0.5970
Round 123: Global Test Accuracy = 0.5960
Round 124: Global Test Accuracy = 0.5950
Round 125: Global Test Accuracy = 0.6000
Round 126: Global Test Accuracy = 0.5990
Round 127: Global Test Accuracy = 0.5990
Round 128: Global Test Accuracy = 0.6030
Round 129: Global Test Accuracy = 0.6010
Round 130: Global Test Accuracy = 0.6000
Round 131: Global Test Accuracy = 0.6040
Round 132: Global Test Accuracy = 0.6020
Round 133: Global Test Accuracy = 0.6020
Round 134: Global Test Accuracy = 0.5990
Round 135: Global Test Accuracy = 0.5990
Round 136: Global Test Accuracy = 0.6020
Round 137: Global Test Accuracy = 0.6010
Round 138: Global Test Accuracy = 0.6050
Round 139: Global Test Accuracy = 0.6030
Round 140: Global Test Accuracy = 0.6060
Round 141: Global Test Accuracy = 0.6030
Round 142: Global Test Accuracy = 0.6060
Round 143: Global Test Accuracy = 0.6090
Round 144: Global Test Accuracy = 0.6040
Round 145: Global Test Accuracy = 0.6090
Round 146: Global Test Accuracy = 0.6110
Round 147: Global Test Accuracy = 0.6130
Round 148: Global Test Accuracy = 0.6150
Round 149: Global Test Accuracy = 0.6150
Round 150: Global Test Accuracy = 0.6150
Round 151: Global Test Accuracy = 0.6230
Round 152: Global Test Accuracy = 0.6200
Round 153: Global Test Accuracy = 0.6210
Round 154: Global Test Accuracy = 0.6200
Round 155: Global Test Accuracy = 0.6220
Round 156: Global Test Accuracy = 0.6250
Round 157: Global Test Accuracy = 0.6250
Round 158: Global Test Accuracy = 0.6260
Round 159: Global Test Accuracy = 0.6260
Round 160: Global Test Accuracy = 0.6260
Round 161: Global Test Accuracy = 0.6300
Round 162: Global Test Accuracy = 0.6290
Round 163: Global Test Accuracy = 0.6350
Round 164: Global Test Accuracy = 0.6310
Round 165: Global Test Accuracy = 0.6360
Round 166: Global Test Accuracy = 0.6330
Round 167: Global Test Accuracy = 0.6360
Round 168: Global Test Accuracy = 0.6380
Round 169: Global Test Accuracy = 0.6380
Round 170: Global Test Accuracy = 0.6390
Round 171: Global Test Accuracy = 0.6410
Round 172: Global Test Accuracy = 0.6380
Round 173: Global Test Accuracy = 0.6390
Round 174: Global Test Accuracy = 0.6390
Round 175: Global Test Accuracy = 0.6400
Round 176: Global Test Accuracy = 0.6450
Round 177: Global Test Accuracy = 0.6440
Round 178: Global Test Accuracy = 0.6400
Round 179: Global Test Accuracy = 0.6390
Round 180: Global Test Accuracy = 0.6430
Round 181: Global Test Accuracy = 0.6430
Round 182: Global Test Accuracy = 0.6440
Round 183: Global Test Accuracy = 0.6450
Round 184: Global Test Accuracy = 0.6440
Round 185: Global Test Accuracy = 0.6440
Round 186: Global Test Accuracy = 0.6440
Round 187: Global Test Accuracy = 0.6460
Round 188: Global Test Accuracy = 0.6450
Round 189: Global Test Accuracy = 0.6450
Round 190: Global Test Accuracy = 0.6440
Round 191: Global Test Accuracy = 0.6440
Round 192: Global Test Accuracy = 0.6410
Round 193: Global Test Accuracy = 0.6400
Round 194: Global Test Accuracy = 0.6390
Round 195: Global Test Accuracy = 0.6420
Round 196: Global Test Accuracy = 0.6420
Round 197: Global Test Accuracy = 0.6390
Round 198: Global Test Accuracy = 0.6400
Round 199: Global Test Accuracy = 0.6370
Round 200: Global Test Accuracy = 0.6380
//train_time: 3493.386 ms//end
//Log Max memory for Large1: 3392442368.0 //end
//Log Max memory for Large2: 1173868544.0 //end
//Log Max memory for Large3: 729354240.0 //end
//Log Max memory for Large4: 724312064.0 //end
//Log Max memory for Server: 1651326976.0 //end
//Log Large1 network: 23440830.0 //end
//Log Large2 network: 39165585.0 //end
//Log Large3 network: 19775671.0 //end
//Log Large4 network: 19812876.0 //end
//Log Server network: 98318312.0 //end
//Log Total Actual Train Comm Cost: 191.22 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: cora, Batch Size: -1, Trainers: 5, Hops: 0, IID Beta: 10000.0 => Training Time = 33.50 seconds
average_final_test_loss, 1.2085412887334823
Average test accuracy, 0.638

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge 
----------------------------------------------------------------------------------------------------
0        668.8        536      424      1.248        1.577       
1        667.6        536      384      1.245        1.738       
2        667.5        543      420      1.229        1.589       
3        670.4        545      412      1.230        1.627       
4        668.1        548      416      1.219        1.606       
====================================================================================================
Total Memory Usage: 3342.3 MB (3.26 GB)
Total Nodes: 2708, Total Edges: 2056
Average Memory per Trainer: 668.5 MB
Average Nodes per Trainer: 541.6
Average Edges per Trainer: 411.2
Max Memory: 670.4 MB (Trainer 3)
Min Memory: 667.5 MB (Trainer 2)
Overall Memory/Node Ratio: 1.234 MB/node
Overall Memory/Edge Ratio: 1.626 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 175.96 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
cora,10000.0,-1,73.6,0.64,33.5,176.0,670.4,0.168,0.088,0
================================================================================

================================================================================
EXPERIMENT SUMMARY
================================================================================
Dataset: cora
Method: FedAvg
Trainers: 5
IID Beta: 10000.0
Batch Size: -1
Hops: 0
Total Execution Time: 73.61 seconds
Training Time: 33.51 seconds
Pretrain Comm Cost: 0.00 MB
Training Comm Cost: 175.96 MB
================================================================================

[36m(pid=1219, ip=192.168.39.47)[0m INFO:matplotlib.font_manager:generated new fontManager[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(Trainer pid=1219, ip=192.168.39.47)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 4x across cluster][0m
[36m(Trainer pid=1219, ip=192.168.39.47)[0m   return torch.load(io.BytesIO(b))[32m [repeated 4x across cluster][0m
Experiment 1/1 completed for:
  Dataset: cora, Trainers: 5, IID Beta: 10000.0
  Method: fedgcn if 0 > 0 else FedAvg, Batch Size: -1

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: cora, Trainers: 5, Distribution: average, IID Beta: 100.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'cora', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 5, 'num_hops': 0, 'iid_beta': 100.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/cora/raw/ind.cora.x
File already exists: ./data/cora/raw/ind.cora.tx
File already exists: ./data/cora/raw/ind.cora.allx
File already exists: ./data/cora/raw/ind.cora.y
File already exists: ./data/cora/raw/ind.cora.ty
File already exists: ./data/cora/raw/ind.cora.ally
File already exists: ./data/cora/raw/ind.cora.graph
File already exists: ./data/cora/raw/ind.cora.test.index
Initialization start: network data collected.
2025-07-29 22:36:12,381	INFO worker.py:1429 -- Using address 192.168.23.53:6379 set in the environment variable RAY_ADDRESS
2025-07-29 22:36:12,381	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.23.53:6379...
2025-07-29 22:36:12,387	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.23.53:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=1722, ip=192.168.2.152)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=1722, ip=192.168.2.152)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5211.286 ms //end
//Log Large1 init network: 649128.0 //end
//Log Large2 init network: 83311.0 //end
//Log Large3 init network: 79811.0 //end
//Log Large4 init network: 110598.0 //end
//Log Server init network: 16188007.0 //end
//Log Initialization Communication Cost (MB): 16.32 //end
Pretrain start time recorded.
//pretrain_time: 7.445 ms//end
//Log Max memory for Large1: 3389800448.0 //end
//Log Max memory for Large2: 732229632.0 //end
//Log Max memory for Large3: 723922944.0 //end
//Log Max memory for Large4: 1146318848.0 //end
//Log Max memory for Server: 1702191104.0 //end
//Log Large1 network: 3875988.0 //end
//Log Large2 network: 517235.0 //end
//Log Large3 network: 521495.0 //end
//Log Large4 network: 599765.0 //end
//Log Server network: 1609350.0 //end
//Log Total Actual Pretrain Comm Cost: 6.79 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1430
Round 2: Global Test Accuracy = 0.1450
Round 3: Global Test Accuracy = 0.1510
Round 4: Global Test Accuracy = 0.1520
Round 5: Global Test Accuracy = 0.1550
Round 6: Global Test Accuracy = 0.1570
Round 7: Global Test Accuracy = 0.1600
Round 8: Global Test Accuracy = 0.1640
Round 9: Global Test Accuracy = 0.1660
Round 10: Global Test Accuracy = 0.1730
Round 11: Global Test Accuracy = 0.1810
Round 12: Global Test Accuracy = 0.1850
Round 13: Global Test Accuracy = 0.1880
Round 14: Global Test Accuracy = 0.1900
Round 15: Global Test Accuracy = 0.1920
Round 16: Global Test Accuracy = 0.1960
Round 17: Global Test Accuracy = 0.2010
Round 18: Global Test Accuracy = 0.2020
Round 19: Global Test Accuracy = 0.2070
Round 20: Global Test Accuracy = 0.2160
Round 21: Global Test Accuracy = 0.2240
Round 22: Global Test Accuracy = 0.2270
Round 23: Global Test Accuracy = 0.2340
Round 24: Global Test Accuracy = 0.2460
Round 25: Global Test Accuracy = 0.2530
Round 26: Global Test Accuracy = 0.2610
Round 27: Global Test Accuracy = 0.2650
Round 28: Global Test Accuracy = 0.2650
Round 29: Global Test Accuracy = 0.2740
Round 30: Global Test Accuracy = 0.2820
Round 31: Global Test Accuracy = 0.2870
Round 32: Global Test Accuracy = 0.2920
Round 33: Global Test Accuracy = 0.2940
Round 34: Global Test Accuracy = 0.2950
Round 35: Global Test Accuracy = 0.3000
Round 36: Global Test Accuracy = 0.2990
Round 37: Global Test Accuracy = 0.3120
Round 38: Global Test Accuracy = 0.3190
Round 39: Global Test Accuracy = 0.3260
Round 40: Global Test Accuracy = 0.3310
Round 41: Global Test Accuracy = 0.3320
Round 42: Global Test Accuracy = 0.3330
Round 43: Global Test Accuracy = 0.3490
Round 44: Global Test Accuracy = 0.3440
Round 45: Global Test Accuracy = 0.3470
Round 46: Global Test Accuracy = 0.3510
Round 47: Global Test Accuracy = 0.3590
Round 48: Global Test Accuracy = 0.3700
Round 49: Global Test Accuracy = 0.3750
Round 50: Global Test Accuracy = 0.3870
Round 51: Global Test Accuracy = 0.3870
Round 52: Global Test Accuracy = 0.4020
Round 53: Global Test Accuracy = 0.4090
Round 54: Global Test Accuracy = 0.4160
Round 55: Global Test Accuracy = 0.4240
Round 56: Global Test Accuracy = 0.4280
Round 57: Global Test Accuracy = 0.4340
Round 58: Global Test Accuracy = 0.4380
Round 59: Global Test Accuracy = 0.4400
Round 60: Global Test Accuracy = 0.4460
Round 61: Global Test Accuracy = 0.4470
Round 62: Global Test Accuracy = 0.4550
Round 63: Global Test Accuracy = 0.4610
Round 64: Global Test Accuracy = 0.4640
Round 65: Global Test Accuracy = 0.4730
Round 66: Global Test Accuracy = 0.4710
Round 67: Global Test Accuracy = 0.4760
Round 68: Global Test Accuracy = 0.4810
Round 69: Global Test Accuracy = 0.4890
Round 70: Global Test Accuracy = 0.4930
Round 71: Global Test Accuracy = 0.4950
Round 72: Global Test Accuracy = 0.5070
Round 73: Global Test Accuracy = 0.5080
Round 74: Global Test Accuracy = 0.5100
Round 75: Global Test Accuracy = 0.5160
Round 76: Global Test Accuracy = 0.5220
Round 77: Global Test Accuracy = 0.5270
Round 78: Global Test Accuracy = 0.5310
Round 79: Global Test Accuracy = 0.5380
Round 80: Global Test Accuracy = 0.5470
Round 81: Global Test Accuracy = 0.5470
Round 82: Global Test Accuracy = 0.5520
Round 83: Global Test Accuracy = 0.5540
Round 84: Global Test Accuracy = 0.5560
Round 85: Global Test Accuracy = 0.5580
Round 86: Global Test Accuracy = 0.5600
Round 87: Global Test Accuracy = 0.5600
Round 88: Global Test Accuracy = 0.5640
Round 89: Global Test Accuracy = 0.5640
Round 90: Global Test Accuracy = 0.5650
Round 91: Global Test Accuracy = 0.5650
Round 92: Global Test Accuracy = 0.5660
Round 93: Global Test Accuracy = 0.5690
Round 94: Global Test Accuracy = 0.5700
Round 95: Global Test Accuracy = 0.5690
Round 96: Global Test Accuracy = 0.5720
Round 97: Global Test Accuracy = 0.5780
Round 98: Global Test Accuracy = 0.5800
Round 99: Global Test Accuracy = 0.5790
Round 100: Global Test Accuracy = 0.5780
Round 101: Global Test Accuracy = 0.5810
Round 102: Global Test Accuracy = 0.5820
Round 103: Global Test Accuracy = 0.5850
Round 104: Global Test Accuracy = 0.5880
Round 105: Global Test Accuracy = 0.5900
Round 106: Global Test Accuracy = 0.5970
Round 107: Global Test Accuracy = 0.6000
Round 108: Global Test Accuracy = 0.6030
Round 109: Global Test Accuracy = 0.6020
Round 110: Global Test Accuracy = 0.6040
Round 111: Global Test Accuracy = 0.6040
Round 112: Global Test Accuracy = 0.6050
Round 113: Global Test Accuracy = 0.6040
Round 114: Global Test Accuracy = 0.6080
Round 115: Global Test Accuracy = 0.6100
Round 116: Global Test Accuracy = 0.6120
Round 117: Global Test Accuracy = 0.6120
Round 118: Global Test Accuracy = 0.6100
Round 119: Global Test Accuracy = 0.6110
Round 120: Global Test Accuracy = 0.6140
Round 121: Global Test Accuracy = 0.6140
Round 122: Global Test Accuracy = 0.6150
Round 123: Global Test Accuracy = 0.6160
Round 124: Global Test Accuracy = 0.6180
Round 125: Global Test Accuracy = 0.6160
Round 126: Global Test Accuracy = 0.6180
Round 127: Global Test Accuracy = 0.6210
Round 128: Global Test Accuracy = 0.6210
Round 129: Global Test Accuracy = 0.6200
Round 130: Global Test Accuracy = 0.6220
Round 131: Global Test Accuracy = 0.6230
Round 132: Global Test Accuracy = 0.6250
Round 133: Global Test Accuracy = 0.6250
Round 134: Global Test Accuracy = 0.6300
Round 135: Global Test Accuracy = 0.6280
Round 136: Global Test Accuracy = 0.6280
Round 137: Global Test Accuracy = 0.6300
Round 138: Global Test Accuracy = 0.6300
Round 139: Global Test Accuracy = 0.6320
Round 140: Global Test Accuracy = 0.6300
Round 141: Global Test Accuracy = 0.6310
Round 142: Global Test Accuracy = 0.6330
Round 143: Global Test Accuracy = 0.6340
Round 144: Global Test Accuracy = 0.6370
Round 145: Global Test Accuracy = 0.6370
Round 146: Global Test Accuracy = 0.6350
Round 147: Global Test Accuracy = 0.6390
Round 148: Global Test Accuracy = 0.6390
Round 149: Global Test Accuracy = 0.6410
Round 150: Global Test Accuracy = 0.6420
Round 151: Global Test Accuracy = 0.6410
Round 152: Global Test Accuracy = 0.6410
Round 153: Global Test Accuracy = 0.6400
Round 154: Global Test Accuracy = 0.6430
Round 155: Global Test Accuracy = 0.6410
Round 156: Global Test Accuracy = 0.6430
Round 157: Global Test Accuracy = 0.6400
Round 158: Global Test Accuracy = 0.6420
Round 159: Global Test Accuracy = 0.6430
Round 160: Global Test Accuracy = 0.6420
Round 161: Global Test Accuracy = 0.6460
Round 162: Global Test Accuracy = 0.6480
Round 163: Global Test Accuracy = 0.6480
Round 164: Global Test Accuracy = 0.6490
Round 165: Global Test Accuracy = 0.6450
Round 166: Global Test Accuracy = 0.6430
Round 167: Global Test Accuracy = 0.6460
Round 168: Global Test Accuracy = 0.6450
Round 169: Global Test Accuracy = 0.6440
Round 170: Global Test Accuracy = 0.6470
Round 171: Global Test Accuracy = 0.6480
Round 172: Global Test Accuracy = 0.6470
Round 173: Global Test Accuracy = 0.6490
Round 174: Global Test Accuracy = 0.6470
Round 175: Global Test Accuracy = 0.6470
Round 176: Global Test Accuracy = 0.6480
Round 177: Global Test Accuracy = 0.6520
Round 178: Global Test Accuracy = 0.6520
Round 179: Global Test Accuracy = 0.6460
Round 180: Global Test Accuracy = 0.6500
Round 181: Global Test Accuracy = 0.6490
Round 182: Global Test Accuracy = 0.6480
Round 183: Global Test Accuracy = 0.6490
Round 184: Global Test Accuracy = 0.6490
Round 185: Global Test Accuracy = 0.6490
Round 186: Global Test Accuracy = 0.6490
Round 187: Global Test Accuracy = 0.6490
Round 188: Global Test Accuracy = 0.6500
Round 189: Global Test Accuracy = 0.6480
Round 190: Global Test Accuracy = 0.6490
Round 191: Global Test Accuracy = 0.6470
Round 192: Global Test Accuracy = 0.6470
Round 193: Global Test Accuracy = 0.6470
Round 194: Global Test Accuracy = 0.6480
Round 195: Global Test Accuracy = 0.6480
Round 196: Global Test Accuracy = 0.6470
Round 197: Global Test Accuracy = 0.6470
Round 198: Global Test Accuracy = 0.6460
Round 199: Global Test Accuracy = 0.6460
Round 200: Global Test Accuracy = 0.6470
//train_time: 3522.1490000000003 ms//end
//Log Max memory for Large1: 3403329536.0 //end
//Log Max memory for Large2: 745381888.0 //end
//Log Max memory for Large3: 736538624.0 //end
//Log Max memory for Large4: 1172008960.0 //end
//Log Max memory for Server: 1728270336.0 //end
//Log Large1 network: 23070052.0 //end
//Log Large2 network: 19842183.0 //end
//Log Large3 network: 19814394.0 //end
//Log Large4 network: 39220283.0 //end
//Log Server network: 98362647.0 //end
//Log Total Actual Train Comm Cost: 191.03 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: cora, Batch Size: -1, Trainers: 5, Hops: 0, IID Beta: 100.0 => Training Time = 33.53 seconds
average_final_test_loss, 1.1998759814500808
Average test accuracy, 0.647

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge 
----------------------------------------------------------------------------------------------------
0        666.3        538      376      1.239        1.772       
1        666.0        547      334      1.218        1.994       
2        667.1        566      490      1.179        1.362       
3        666.3        522      414      1.276        1.609       
4        666.4        535      518      1.246        1.287       
====================================================================================================
Total Memory Usage: 3332.2 MB (3.25 GB)
Total Nodes: 2708, Total Edges: 2132
Average Memory per Trainer: 666.4 MB
Average Nodes per Trainer: 541.6
Average Edges per Trainer: 426.4
Max Memory: 667.1 MB (Trainer 2)
Min Memory: 666.0 MB (Trainer 1)
Overall Memory/Node Ratio: 1.231 MB/node
Overall Memory/Edge Ratio: 1.563 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 175.96 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
cora,100.0,-1,68.7,0.65,33.5,176.0,667.1,0.168,0.088,0
================================================================================

================================================================================
EXPERIMENT SUMMARY
================================================================================
Dataset: cora
Method: FedAvg
Trainers: 5
IID Beta: 100.0
Batch Size: -1
Hops: 0
Total Execution Time: 68.75 seconds
Training Time: 33.54 seconds
Pretrain Comm Cost: 0.00 MB
Training Comm Cost: 175.96 MB
================================================================================

[36m(Trainer pid=1634, ip=192.168.52.140)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 4x across cluster][0m
[36m(Trainer pid=1634, ip=192.168.52.140)[0m   return torch.load(io.BytesIO(b))[32m [repeated 4x across cluster][0m
Experiment 1/1 completed for:
  Dataset: cora, Trainers: 5, IID Beta: 100.0
  Method: fedgcn if 0 > 0 else FedAvg, Batch Size: -1

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: cora, Trainers: 5, Distribution: average, IID Beta: 10.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'cora', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 5, 'num_hops': 0, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/cora/raw/ind.cora.x
File already exists: ./data/cora/raw/ind.cora.tx
File already exists: ./data/cora/raw/ind.cora.allx
File already exists: ./data/cora/raw/ind.cora.y
File already exists: ./data/cora/raw/ind.cora.ty
File already exists: ./data/cora/raw/ind.cora.ally
File already exists: ./data/cora/raw/ind.cora.graph
File already exists: ./data/cora/raw/ind.cora.test.index
Initialization start: network data collected.
2025-07-29 22:37:26,502	INFO worker.py:1429 -- Using address 192.168.23.53:6379 set in the environment variable RAY_ADDRESS
2025-07-29 22:37:26,502	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.23.53:6379...
2025-07-29 22:37:26,510	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.23.53:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=2087, ip=192.168.52.140)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=2087, ip=192.168.52.140)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5114.822 ms //end
//Log Large1 init network: 650902.0 //end
//Log Large2 init network: 88340.0 //end
//Log Large3 init network: 87302.0 //end
//Log Large4 init network: 87329.0 //end
//Log Server init network: 16164244.0 //end
//Log Initialization Communication Cost (MB): 16.29 //end
Pretrain start time recorded.
//pretrain_time: 7.955 ms//end
//Log Max memory for Large1: 3680997376.0 //end
//Log Max memory for Large2: 737611776.0 //end
//Log Max memory for Large3: 728162304.0 //end
//Log Max memory for Large4: 734289920.0 //end
//Log Max memory for Server: 1736777728.0 //end
//Log Large1 network: 3801497.0 //end
//Log Large2 network: 495712.0 //end
//Log Large3 network: 527318.0 //end
//Log Large4 network: 548731.0 //end
//Log Server network: 1556165.0 //end
//Log Total Actual Pretrain Comm Cost: 6.61 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1450
Round 2: Global Test Accuracy = 0.1520
Round 3: Global Test Accuracy = 0.1520
Round 4: Global Test Accuracy = 0.1590
Round 5: Global Test Accuracy = 0.1570
Round 6: Global Test Accuracy = 0.1620
Round 7: Global Test Accuracy = 0.1600
Round 8: Global Test Accuracy = 0.1620
Round 9: Global Test Accuracy = 0.1700
Round 10: Global Test Accuracy = 0.1690
Round 11: Global Test Accuracy = 0.1760
Round 12: Global Test Accuracy = 0.1820
Round 13: Global Test Accuracy = 0.1890
Round 14: Global Test Accuracy = 0.1960
Round 15: Global Test Accuracy = 0.2000
Round 16: Global Test Accuracy = 0.2090
Round 17: Global Test Accuracy = 0.2080
Round 18: Global Test Accuracy = 0.2150
Round 19: Global Test Accuracy = 0.2250
Round 20: Global Test Accuracy = 0.2310
Round 21: Global Test Accuracy = 0.2360
Round 22: Global Test Accuracy = 0.2440
Round 23: Global Test Accuracy = 0.2510
Round 24: Global Test Accuracy = 0.2620
Round 25: Global Test Accuracy = 0.2710
Round 26: Global Test Accuracy = 0.2730
Round 27: Global Test Accuracy = 0.2750
Round 28: Global Test Accuracy = 0.2820
Round 29: Global Test Accuracy = 0.2910
Round 30: Global Test Accuracy = 0.2980
Round 31: Global Test Accuracy = 0.3030
Round 32: Global Test Accuracy = 0.3060
Round 33: Global Test Accuracy = 0.3110
Round 34: Global Test Accuracy = 0.3130
Round 35: Global Test Accuracy = 0.3160
Round 36: Global Test Accuracy = 0.3180
Round 37: Global Test Accuracy = 0.3220
Round 38: Global Test Accuracy = 0.3280
Round 39: Global Test Accuracy = 0.3290
Round 40: Global Test Accuracy = 0.3310
Round 41: Global Test Accuracy = 0.3300
Round 42: Global Test Accuracy = 0.3350
Round 43: Global Test Accuracy = 0.3430
Round 44: Global Test Accuracy = 0.3430
Round 45: Global Test Accuracy = 0.3490
Round 46: Global Test Accuracy = 0.3510
Round 47: Global Test Accuracy = 0.3580
Round 48: Global Test Accuracy = 0.3670
Round 49: Global Test Accuracy = 0.3730
Round 50: Global Test Accuracy = 0.3850
Round 51: Global Test Accuracy = 0.3910
Round 52: Global Test Accuracy = 0.3940
Round 53: Global Test Accuracy = 0.3920
Round 54: Global Test Accuracy = 0.3910
Round 55: Global Test Accuracy = 0.3930
Round 56: Global Test Accuracy = 0.3950
Round 57: Global Test Accuracy = 0.4010
Round 58: Global Test Accuracy = 0.4000
Round 59: Global Test Accuracy = 0.4070
Round 60: Global Test Accuracy = 0.4170
Round 61: Global Test Accuracy = 0.4180
Round 62: Global Test Accuracy = 0.4150
Round 63: Global Test Accuracy = 0.4170
Round 64: Global Test Accuracy = 0.4210
Round 65: Global Test Accuracy = 0.4240
Round 66: Global Test Accuracy = 0.4260
Round 67: Global Test Accuracy = 0.4340
Round 68: Global Test Accuracy = 0.4420
Round 69: Global Test Accuracy = 0.4450
Round 70: Global Test Accuracy = 0.4530
Round 71: Global Test Accuracy = 0.4550
Round 72: Global Test Accuracy = 0.4670
Round 73: Global Test Accuracy = 0.4710
Round 74: Global Test Accuracy = 0.4730
Round 75: Global Test Accuracy = 0.4790
Round 76: Global Test Accuracy = 0.4820
Round 77: Global Test Accuracy = 0.4840
Round 78: Global Test Accuracy = 0.4890
Round 79: Global Test Accuracy = 0.4940
Round 80: Global Test Accuracy = 0.4970
Round 81: Global Test Accuracy = 0.5040
Round 82: Global Test Accuracy = 0.5060
Round 83: Global Test Accuracy = 0.5120
Round 84: Global Test Accuracy = 0.5180
Round 85: Global Test Accuracy = 0.5130
Round 86: Global Test Accuracy = 0.5240
Round 87: Global Test Accuracy = 0.5280
Round 88: Global Test Accuracy = 0.5300
Round 89: Global Test Accuracy = 0.5400
Round 90: Global Test Accuracy = 0.5440
Round 91: Global Test Accuracy = 0.5480
Round 92: Global Test Accuracy = 0.5520
Round 93: Global Test Accuracy = 0.5560
Round 94: Global Test Accuracy = 0.5540
Round 95: Global Test Accuracy = 0.5570
Round 96: Global Test Accuracy = 0.5600
Round 97: Global Test Accuracy = 0.5620
Round 98: Global Test Accuracy = 0.5610
Round 99: Global Test Accuracy = 0.5650
Round 100: Global Test Accuracy = 0.5690
Round 101: Global Test Accuracy = 0.5700
Round 102: Global Test Accuracy = 0.5700
Round 103: Global Test Accuracy = 0.5700
Round 104: Global Test Accuracy = 0.5760
Round 105: Global Test Accuracy = 0.5750
Round 106: Global Test Accuracy = 0.5770
Round 107: Global Test Accuracy = 0.5790
Round 108: Global Test Accuracy = 0.5790
Round 109: Global Test Accuracy = 0.5800
Round 110: Global Test Accuracy = 0.5840
Round 111: Global Test Accuracy = 0.5860
Round 112: Global Test Accuracy = 0.5890
Round 113: Global Test Accuracy = 0.5870
Round 114: Global Test Accuracy = 0.5940
Round 115: Global Test Accuracy = 0.5960
Round 116: Global Test Accuracy = 0.5940
Round 117: Global Test Accuracy = 0.5980
Round 118: Global Test Accuracy = 0.5990
Round 119: Global Test Accuracy = 0.6000
Round 120: Global Test Accuracy = 0.6020
Round 121: Global Test Accuracy = 0.6050
Round 122: Global Test Accuracy = 0.6050
Round 123: Global Test Accuracy = 0.6060
Round 124: Global Test Accuracy = 0.6110
Round 125: Global Test Accuracy = 0.6120
Round 126: Global Test Accuracy = 0.6110
Round 127: Global Test Accuracy = 0.6110
Round 128: Global Test Accuracy = 0.6130
Round 129: Global Test Accuracy = 0.6140
Round 130: Global Test Accuracy = 0.6130
Round 131: Global Test Accuracy = 0.6140
Round 132: Global Test Accuracy = 0.6140
Round 133: Global Test Accuracy = 0.6170
Round 134: Global Test Accuracy = 0.6190
Round 135: Global Test Accuracy = 0.6190
Round 136: Global Test Accuracy = 0.6190
Round 137: Global Test Accuracy = 0.6200
Round 138: Global Test Accuracy = 0.6200
Round 139: Global Test Accuracy = 0.6270
Round 140: Global Test Accuracy = 0.6290
Round 141: Global Test Accuracy = 0.6270
Round 142: Global Test Accuracy = 0.6260
Round 143: Global Test Accuracy = 0.6310
Round 144: Global Test Accuracy = 0.6250
Round 145: Global Test Accuracy = 0.6300
Round 146: Global Test Accuracy = 0.6260
Round 147: Global Test Accuracy = 0.6310
Round 148: Global Test Accuracy = 0.6290
Round 149: Global Test Accuracy = 0.6210
Round 150: Global Test Accuracy = 0.6300
Round 151: Global Test Accuracy = 0.6320
Round 152: Global Test Accuracy = 0.6300
Round 153: Global Test Accuracy = 0.6300
Round 154: Global Test Accuracy = 0.6280
Round 155: Global Test Accuracy = 0.6300
Round 156: Global Test Accuracy = 0.6300
Round 157: Global Test Accuracy = 0.6360
Round 158: Global Test Accuracy = 0.6320
Round 159: Global Test Accuracy = 0.6340
Round 160: Global Test Accuracy = 0.6360
Round 161: Global Test Accuracy = 0.6370
Round 162: Global Test Accuracy = 0.6380
Round 163: Global Test Accuracy = 0.6380
Round 164: Global Test Accuracy = 0.6380
Round 165: Global Test Accuracy = 0.6400
Round 166: Global Test Accuracy = 0.6410
Round 167: Global Test Accuracy = 0.6390
Round 168: Global Test Accuracy = 0.6410
Round 169: Global Test Accuracy = 0.6420
Round 170: Global Test Accuracy = 0.6430
Round 171: Global Test Accuracy = 0.6440
Round 172: Global Test Accuracy = 0.6450
Round 173: Global Test Accuracy = 0.6460
Round 174: Global Test Accuracy = 0.6460
Round 175: Global Test Accuracy = 0.6460
Round 176: Global Test Accuracy = 0.6450
Round 177: Global Test Accuracy = 0.6460
Round 178: Global Test Accuracy = 0.6470
Round 179: Global Test Accuracy = 0.6450
Round 180: Global Test Accuracy = 0.6450
Round 181: Global Test Accuracy = 0.6460
Round 182: Global Test Accuracy = 0.6450
Round 183: Global Test Accuracy = 0.6460
Round 184: Global Test Accuracy = 0.6470
Round 185: Global Test Accuracy = 0.6470
Round 186: Global Test Accuracy = 0.6460
Round 187: Global Test Accuracy = 0.6470
Round 188: Global Test Accuracy = 0.6510
Round 189: Global Test Accuracy = 0.6490
Round 190: Global Test Accuracy = 0.6470
Round 191: Global Test Accuracy = 0.6480
Round 192: Global Test Accuracy = 0.6500
Round 193: Global Test Accuracy = 0.6500
Round 194: Global Test Accuracy = 0.6530
Round 195: Global Test Accuracy = 0.6510
Round 196: Global Test Accuracy = 0.6530
Round 197: Global Test Accuracy = 0.6530
Round 198: Global Test Accuracy = 0.6540
Round 199: Global Test Accuracy = 0.6530
Round 200: Global Test Accuracy = 0.6540
//train_time: 3292.656 ms//end
//Log Max memory for Large1: 3710173184.0 //end
//Log Max memory for Large2: 751128576.0 //end
//Log Max memory for Large3: 739958784.0 //end
//Log Max memory for Large4: 750141440.0 //end
//Log Max memory for Server: 1784287232.0 //end
//Log Large1 network: 42279649.0 //end
//Log Large2 network: 19825945.0 //end
//Log Large3 network: 19812151.0 //end
//Log Large4 network: 19804791.0 //end
//Log Server network: 98333200.0 //end
//Log Total Actual Train Comm Cost: 190.79 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: cora, Batch Size: -1, Trainers: 5, Hops: 0, IID Beta: 10.0 => Training Time = 33.30 seconds
average_final_test_loss, 1.2211620911359786
Average test accuracy, 0.654

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge 
----------------------------------------------------------------------------------------------------
0        664.2        571      580      1.163        1.145       
1        666.8        556      434      1.199        1.536       
2        668.0        534      340      1.251        1.965       
3        669.3        535      420      1.251        1.594       
4        666.0        512      356      1.301        1.871       
====================================================================================================
Total Memory Usage: 3334.3 MB (3.26 GB)
Total Nodes: 2708, Total Edges: 2130
Average Memory per Trainer: 666.9 MB
Average Nodes per Trainer: 541.6
Average Edges per Trainer: 426.0
Max Memory: 669.3 MB (Trainer 3)
Min Memory: 664.2 MB (Trainer 0)
Overall Memory/Node Ratio: 1.231 MB/node
Overall Memory/Edge Ratio: 1.565 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 175.96 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
cora,10.0,-1,68.4,0.65,33.3,176.0,669.3,0.167,0.088,0
================================================================================

================================================================================
EXPERIMENT SUMMARY
================================================================================
Dataset: cora
Method: FedAvg
Trainers: 5
IID Beta: 10.0
Batch Size: -1
Hops: 0
Total Execution Time: 68.42 seconds
Training Time: 33.31 seconds
Pretrain Comm Cost: 0.00 MB
Training Comm Cost: 175.96 MB
================================================================================

[36m(Trainer pid=2000, ip=192.168.39.47)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 4x across cluster][0m
[36m(Trainer pid=2000, ip=192.168.39.47)[0m   return torch.load(io.BytesIO(b))[32m [repeated 4x across cluster][0m
Experiment 1/1 completed for:
  Dataset: cora, Trainers: 5, IID Beta: 10.0
  Method: fedgcn if 0 > 0 else FedAvg, Batch Size: -1

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: citeseer, Trainers: 5, Distribution: average, IID Beta: 10000.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'citeseer', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 5, 'num_hops': 0, 'iid_beta': 10000.0, 'distribution_type': 'average', 'gpu': False}
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x to ./data/citeseer/raw/ind.citeseer.x...
Downloaded ./data/citeseer/raw/ind.citeseer.x
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx to ./data/citeseer/raw/ind.citeseer.tx...
Downloaded ./data/citeseer/raw/ind.citeseer.tx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx to ./data/citeseer/raw/ind.citeseer.allx...
Downloaded ./data/citeseer/raw/ind.citeseer.allx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y to ./data/citeseer/raw/ind.citeseer.y...
Downloaded ./data/citeseer/raw/ind.citeseer.y
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty to ./data/citeseer/raw/ind.citeseer.ty...
Downloaded ./data/citeseer/raw/ind.citeseer.ty
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally to ./data/citeseer/raw/ind.citeseer.ally...
Downloaded ./data/citeseer/raw/ind.citeseer.ally
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph to ./data/citeseer/raw/ind.citeseer.graph...
Downloaded ./data/citeseer/raw/ind.citeseer.graph
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index to ./data/citeseer/raw/ind.citeseer.test.index...
Downloaded ./data/citeseer/raw/ind.citeseer.test.index
Initialization start: network data collected.
2025-07-29 22:38:42,511	INFO worker.py:1429 -- Using address 192.168.23.53:6379 set in the environment variable RAY_ADDRESS
2025-07-29 22:38:42,511	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.23.53:6379...
2025-07-29 22:38:42,518	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.23.53:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=2507, ip=192.168.2.152)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=2507, ip=192.168.2.152)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5131.337 ms //end
//Log Large1 init network: 626924.0 //end
//Log Large2 init network: 92514.0 //end
//Log Large3 init network: 127400.0 //end
//Log Large4 init network: 100573.0 //end
//Log Server init network: 49880039.0 //end
//Log Initialization Communication Cost (MB): 48.47 //end
Pretrain start time recorded.
//pretrain_time: 6.888 ms//end
//Log Max memory for Large1: 3298422784.0 //end
//Log Max memory for Large2: 768016384.0 //end
//Log Max memory for Large3: 1210048512.0 //end
//Log Max memory for Large4: 763445248.0 //end
//Log Max memory for Server: 1864753152.0 //end
//Log Large1 network: 3696438.0 //end
//Log Large2 network: 544589.0 //end
//Log Large3 network: 738027.0 //end
//Log Large4 network: 544312.0 //end
//Log Server network: 2385479.0 //end
//Log Total Actual Pretrain Comm Cost: 7.54 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1560
Round 2: Global Test Accuracy = 0.1590
Round 3: Global Test Accuracy = 0.1640
Round 4: Global Test Accuracy = 0.1680
Round 5: Global Test Accuracy = 0.1710
Round 6: Global Test Accuracy = 0.1810
Round 7: Global Test Accuracy = 0.1840
Round 8: Global Test Accuracy = 0.1890
Round 9: Global Test Accuracy = 0.1940
Round 10: Global Test Accuracy = 0.2010
Round 11: Global Test Accuracy = 0.2020
Round 12: Global Test Accuracy = 0.2070
Round 13: Global Test Accuracy = 0.2160
Round 14: Global Test Accuracy = 0.2250
Round 15: Global Test Accuracy = 0.2290
Round 16: Global Test Accuracy = 0.2330
Round 17: Global Test Accuracy = 0.2400
Round 18: Global Test Accuracy = 0.2500
Round 19: Global Test Accuracy = 0.2540
Round 20: Global Test Accuracy = 0.2630
Round 21: Global Test Accuracy = 0.2740
Round 22: Global Test Accuracy = 0.2680
Round 23: Global Test Accuracy = 0.2840
Round 24: Global Test Accuracy = 0.2940
Round 25: Global Test Accuracy = 0.2950
Round 26: Global Test Accuracy = 0.3070
Round 27: Global Test Accuracy = 0.3140
Round 28: Global Test Accuracy = 0.3140
Round 29: Global Test Accuracy = 0.3290
Round 30: Global Test Accuracy = 0.3430
Round 31: Global Test Accuracy = 0.3360
Round 32: Global Test Accuracy = 0.3530
Round 33: Global Test Accuracy = 0.3700
Round 34: Global Test Accuracy = 0.3800
Round 35: Global Test Accuracy = 0.3950
Round 36: Global Test Accuracy = 0.3970
Round 37: Global Test Accuracy = 0.4040
Round 38: Global Test Accuracy = 0.4120
Round 39: Global Test Accuracy = 0.4130
Round 40: Global Test Accuracy = 0.4210
Round 41: Global Test Accuracy = 0.4210
Round 42: Global Test Accuracy = 0.4330
Round 43: Global Test Accuracy = 0.4440
Round 44: Global Test Accuracy = 0.4490
Round 45: Global Test Accuracy = 0.4520
Round 46: Global Test Accuracy = 0.4650
Round 47: Global Test Accuracy = 0.4710
Round 48: Global Test Accuracy = 0.4760
Round 49: Global Test Accuracy = 0.4810
Round 50: Global Test Accuracy = 0.4770
Round 51: Global Test Accuracy = 0.4920
Round 52: Global Test Accuracy = 0.4860
Round 53: Global Test Accuracy = 0.5020
Round 54: Global Test Accuracy = 0.4980
Round 55: Global Test Accuracy = 0.5080
Round 56: Global Test Accuracy = 0.5170
Round 57: Global Test Accuracy = 0.5140
Round 58: Global Test Accuracy = 0.5180
Round 59: Global Test Accuracy = 0.5270
Round 60: Global Test Accuracy = 0.5210
Round 61: Global Test Accuracy = 0.5210
Round 62: Global Test Accuracy = 0.5330
Round 63: Global Test Accuracy = 0.5320
Round 64: Global Test Accuracy = 0.5340
Round 65: Global Test Accuracy = 0.5330
Round 66: Global Test Accuracy = 0.5390
Round 67: Global Test Accuracy = 0.5460
Round 68: Global Test Accuracy = 0.5460
Round 69: Global Test Accuracy = 0.5440
Round 70: Global Test Accuracy = 0.5500
Round 71: Global Test Accuracy = 0.5510
Round 72: Global Test Accuracy = 0.5540
Round 73: Global Test Accuracy = 0.5550
Round 74: Global Test Accuracy = 0.5520
Round 75: Global Test Accuracy = 0.5520
Round 76: Global Test Accuracy = 0.5490
Round 77: Global Test Accuracy = 0.5560
Round 78: Global Test Accuracy = 0.5550
Round 79: Global Test Accuracy = 0.5560
Round 80: Global Test Accuracy = 0.5560
Round 81: Global Test Accuracy = 0.5580
Round 82: Global Test Accuracy = 0.5590
Round 83: Global Test Accuracy = 0.5580
Round 84: Global Test Accuracy = 0.5560
Round 85: Global Test Accuracy = 0.5640
Round 86: Global Test Accuracy = 0.5620
Round 87: Global Test Accuracy = 0.5650
Round 88: Global Test Accuracy = 0.5620
Round 89: Global Test Accuracy = 0.5660
Round 90: Global Test Accuracy = 0.5640
Round 91: Global Test Accuracy = 0.5660
Round 92: Global Test Accuracy = 0.5630
Round 93: Global Test Accuracy = 0.5660
Round 94: Global Test Accuracy = 0.5650
Round 95: Global Test Accuracy = 0.5710
Round 96: Global Test Accuracy = 0.5760
Round 97: Global Test Accuracy = 0.5750
Round 98: Global Test Accuracy = 0.5710
Round 99: Global Test Accuracy = 0.5730
Round 100: Global Test Accuracy = 0.5730
Round 101: Global Test Accuracy = 0.5740
Round 102: Global Test Accuracy = 0.5760
Round 103: Global Test Accuracy = 0.5730
Round 104: Global Test Accuracy = 0.5750
Round 105: Global Test Accuracy = 0.5710
Round 106: Global Test Accuracy = 0.5740
Round 107: Global Test Accuracy = 0.5750
Round 108: Global Test Accuracy = 0.5770
Round 109: Global Test Accuracy = 0.5750
Round 110: Global Test Accuracy = 0.5800
Round 111: Global Test Accuracy = 0.5790
Round 112: Global Test Accuracy = 0.5760
Round 113: Global Test Accuracy = 0.5780
Round 114: Global Test Accuracy = 0.5790
Round 115: Global Test Accuracy = 0.5790
Round 116: Global Test Accuracy = 0.5780
Round 117: Global Test Accuracy = 0.5810
Round 118: Global Test Accuracy = 0.5800
Round 119: Global Test Accuracy = 0.5800
Round 120: Global Test Accuracy = 0.5810
Round 121: Global Test Accuracy = 0.5820
Round 122: Global Test Accuracy = 0.5820
Round 123: Global Test Accuracy = 0.5840
Round 124: Global Test Accuracy = 0.5840
Round 125: Global Test Accuracy = 0.5820
Round 126: Global Test Accuracy = 0.5810
Round 127: Global Test Accuracy = 0.5840
Round 128: Global Test Accuracy = 0.5810
Round 129: Global Test Accuracy = 0.5850
Round 130: Global Test Accuracy = 0.5850
Round 131: Global Test Accuracy = 0.5830
Round 132: Global Test Accuracy = 0.5820
Round 133: Global Test Accuracy = 0.5810
Round 134: Global Test Accuracy = 0.5830
Round 135: Global Test Accuracy = 0.5840
Round 136: Global Test Accuracy = 0.5820
Round 137: Global Test Accuracy = 0.5850
Round 138: Global Test Accuracy = 0.5850
Round 139: Global Test Accuracy = 0.5810
Round 140: Global Test Accuracy = 0.5790
Round 141: Global Test Accuracy = 0.5830
Round 142: Global Test Accuracy = 0.5910
Round 143: Global Test Accuracy = 0.5890
Round 144: Global Test Accuracy = 0.5880
Round 145: Global Test Accuracy = 0.5850
Round 146: Global Test Accuracy = 0.5870
Round 147: Global Test Accuracy = 0.5870
Round 148: Global Test Accuracy = 0.5910
Round 149: Global Test Accuracy = 0.5880
Round 150: Global Test Accuracy = 0.5880
Round 151: Global Test Accuracy = 0.5880
Round 152: Global Test Accuracy = 0.5850
Round 153: Global Test Accuracy = 0.5860
Round 154: Global Test Accuracy = 0.5880
Round 155: Global Test Accuracy = 0.5910
Round 156: Global Test Accuracy = 0.5890
Round 157: Global Test Accuracy = 0.5890
Round 158: Global Test Accuracy = 0.5870
Round 159: Global Test Accuracy = 0.5920
Round 160: Global Test Accuracy = 0.5920
Round 161: Global Test Accuracy = 0.5910
Round 162: Global Test Accuracy = 0.5910
Round 163: Global Test Accuracy = 0.5930
Round 164: Global Test Accuracy = 0.5940
Round 165: Global Test Accuracy = 0.5940
Round 166: Global Test Accuracy = 0.5900
Round 167: Global Test Accuracy = 0.5930
Round 168: Global Test Accuracy = 0.5940
Round 169: Global Test Accuracy = 0.5950
Round 170: Global Test Accuracy = 0.5910
Round 171: Global Test Accuracy = 0.5900
Round 172: Global Test Accuracy = 0.5920
Round 173: Global Test Accuracy = 0.5910
Round 174: Global Test Accuracy = 0.5960
Round 175: Global Test Accuracy = 0.5910
Round 176: Global Test Accuracy = 0.5910
Round 177: Global Test Accuracy = 0.5890
Round 178: Global Test Accuracy = 0.5880
Round 179: Global Test Accuracy = 0.5880
Round 180: Global Test Accuracy = 0.5890
Round 181: Global Test Accuracy = 0.5920
Round 182: Global Test Accuracy = 0.5950
Round 183: Global Test Accuracy = 0.5920
Round 184: Global Test Accuracy = 0.5910
Round 185: Global Test Accuracy = 0.5910
Round 186: Global Test Accuracy = 0.5910
Round 187: Global Test Accuracy = 0.5910
Round 188: Global Test Accuracy = 0.5920
Round 189: Global Test Accuracy = 0.5910
Round 190: Global Test Accuracy = 0.5910
Round 191: Global Test Accuracy = 0.5860
Round 192: Global Test Accuracy = 0.5860
Round 193: Global Test Accuracy = 0.5870
Round 194: Global Test Accuracy = 0.5870
Round 195: Global Test Accuracy = 0.5850
Round 196: Global Test Accuracy = 0.5860
Round 197: Global Test Accuracy = 0.5890
Round 198: Global Test Accuracy = 0.5890
Round 199: Global Test Accuracy = 0.5890
Round 200: Global Test Accuracy = 0.5860
//train_time: 7036.54 ms//end
//Log Max memory for Large1: 3330682880.0 //end
//Log Max memory for Large2: 792727552.0 //end
//Log Max memory for Large3: 1241608192.0 //end
//Log Max memory for Large4: 798912512.0 //end
//Log Max memory for Server: 1942355968.0 //end
//Log Large1 network: 53475493.0 //end
//Log Large2 network: 49779511.0 //end
//Log Large3 network: 98969161.0 //end
//Log Large4 network: 49793386.0 //end
//Log Server network: 247407934.0 //end
//Log Total Actual Train Comm Cost: 476.29 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: citeseer, Batch Size: -1, Trainers: 5, Hops: 0, IID Beta: 10000.0 => Training Time = 37.04 seconds
average_final_test_loss, 1.1874494450092317
Average test accuracy, 0.586

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge 
----------------------------------------------------------------------------------------------------
0        698.6        665      375      1.050        1.863       
1        690.4        662      387      1.043        1.784       
2        690.8        665      344      1.039        2.008       
3        686.8        662      398      1.038        1.726       
4        699.2        673      392      1.039        1.784       
====================================================================================================
Total Memory Usage: 3465.9 MB (3.38 GB)
Total Nodes: 3327, Total Edges: 1896
Average Memory per Trainer: 693.2 MB
Average Nodes per Trainer: 665.4
Average Edges per Trainer: 379.2
Max Memory: 699.2 MB (Trainer 4)
Min Memory: 686.8 MB (Trainer 3)
Overall Memory/Node Ratio: 1.042 MB/node
Overall Memory/Edge Ratio: 1.828 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 452.93 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
citeseer,10000.0,-1,72.2,0.59,37.1,452.9,699.2,0.185,0.226,0
================================================================================

================================================================================
EXPERIMENT SUMMARY
================================================================================
Dataset: citeseer
Method: FedAvg
Trainers: 5
IID Beta: 10000.0
Batch Size: -1
Hops: 0
Total Execution Time: 72.19 seconds
Training Time: 37.06 seconds
Pretrain Comm Cost: 0.00 MB
Training Comm Cost: 452.93 MB
================================================================================

[36m(Trainer pid=2406, ip=192.168.39.47)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 4x across cluster][0m
[36m(Trainer pid=2406, ip=192.168.39.47)[0m   return torch.load(io.BytesIO(b))[32m [repeated 4x across cluster][0m
Experiment 1/1 completed for:
  Dataset: citeseer, Trainers: 5, IID Beta: 10000.0
  Method: fedgcn if 0 > 0 else FedAvg, Batch Size: -1

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: citeseer, Trainers: 5, Distribution: average, IID Beta: 100.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'citeseer', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 5, 'num_hops': 0, 'iid_beta': 100.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/citeseer/raw/ind.citeseer.x
File already exists: ./data/citeseer/raw/ind.citeseer.tx
File already exists: ./data/citeseer/raw/ind.citeseer.allx
File already exists: ./data/citeseer/raw/ind.citeseer.y
File already exists: ./data/citeseer/raw/ind.citeseer.ty
File already exists: ./data/citeseer/raw/ind.citeseer.ally
File already exists: ./data/citeseer/raw/ind.citeseer.graph
File already exists: ./data/citeseer/raw/ind.citeseer.test.index
Initialization start: network data collected.
2025-07-29 22:40:00,672	INFO worker.py:1429 -- Using address 192.168.23.53:6379 set in the environment variable RAY_ADDRESS
2025-07-29 22:40:00,672	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.23.53:6379...
2025-07-29 22:40:00,677	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.23.53:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=6552, ip=192.168.0.191)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=6552, ip=192.168.0.191)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5325.8279999999995 ms //end
//Log Large1 init network: 623348.0 //end
//Log Large2 init network: 121951.0 //end
//Log Large3 init network: 94216.0 //end
//Log Large4 init network: 95787.0 //end
//Log Server init network: 49892076.0 //end
//Log Initialization Communication Cost (MB): 48.47 //end
Pretrain start time recorded.
//pretrain_time: 8.389000000000001 ms//end
//Log Max memory for Large1: 3340161024.0 //end
//Log Max memory for Large2: 1228251136.0 //end
//Log Max memory for Large3: 812961792.0 //end
//Log Max memory for Large4: 800542720.0 //end
//Log Max memory for Server: 1981427712.0 //end
//Log Large1 network: 3386362.0 //end
//Log Large2 network: 697495.0 //end
//Log Large3 network: 542363.0 //end
//Log Large4 network: 550666.0 //end
//Log Server network: 2407116.0 //end
//Log Total Actual Pretrain Comm Cost: 7.23 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1550
Round 2: Global Test Accuracy = 0.1640
Round 3: Global Test Accuracy = 0.1750
Round 4: Global Test Accuracy = 0.1800
Round 5: Global Test Accuracy = 0.1960
Round 6: Global Test Accuracy = 0.2000
Round 7: Global Test Accuracy = 0.2010
Round 8: Global Test Accuracy = 0.2160
Round 9: Global Test Accuracy = 0.2160
Round 10: Global Test Accuracy = 0.2260
Round 11: Global Test Accuracy = 0.2330
Round 12: Global Test Accuracy = 0.2440
Round 13: Global Test Accuracy = 0.2500
Round 14: Global Test Accuracy = 0.2570
Round 15: Global Test Accuracy = 0.2560
Round 16: Global Test Accuracy = 0.2660
Round 17: Global Test Accuracy = 0.2710
Round 18: Global Test Accuracy = 0.2790
Round 19: Global Test Accuracy = 0.2900
Round 20: Global Test Accuracy = 0.2980
Round 21: Global Test Accuracy = 0.3000
Round 22: Global Test Accuracy = 0.3070
Round 23: Global Test Accuracy = 0.3150
Round 24: Global Test Accuracy = 0.3260
Round 25: Global Test Accuracy = 0.3370
Round 26: Global Test Accuracy = 0.3430
Round 27: Global Test Accuracy = 0.3610
Round 28: Global Test Accuracy = 0.3640
Round 29: Global Test Accuracy = 0.3840
Round 30: Global Test Accuracy = 0.3960
Round 31: Global Test Accuracy = 0.3970
Round 32: Global Test Accuracy = 0.4040
Round 33: Global Test Accuracy = 0.4150
Round 34: Global Test Accuracy = 0.4290
Round 35: Global Test Accuracy = 0.4330
Round 36: Global Test Accuracy = 0.4430
Round 37: Global Test Accuracy = 0.4500
Round 38: Global Test Accuracy = 0.4550
Round 39: Global Test Accuracy = 0.4620
Round 40: Global Test Accuracy = 0.4660
Round 41: Global Test Accuracy = 0.4750
Round 42: Global Test Accuracy = 0.4710
Round 43: Global Test Accuracy = 0.4820
Round 44: Global Test Accuracy = 0.4860
Round 45: Global Test Accuracy = 0.4880
Round 46: Global Test Accuracy = 0.4890
Round 47: Global Test Accuracy = 0.4940
Round 48: Global Test Accuracy = 0.5010
Round 49: Global Test Accuracy = 0.5050
Round 50: Global Test Accuracy = 0.5090
Round 51: Global Test Accuracy = 0.5150
Round 52: Global Test Accuracy = 0.5150
Round 53: Global Test Accuracy = 0.5230
Round 54: Global Test Accuracy = 0.5230
Round 55: Global Test Accuracy = 0.5250
Round 56: Global Test Accuracy = 0.5320
Round 57: Global Test Accuracy = 0.5340
Round 58: Global Test Accuracy = 0.5390
Round 59: Global Test Accuracy = 0.5380
Round 60: Global Test Accuracy = 0.5400
Round 61: Global Test Accuracy = 0.5420
Round 62: Global Test Accuracy = 0.5430
Round 63: Global Test Accuracy = 0.5470
Round 64: Global Test Accuracy = 0.5500
Round 65: Global Test Accuracy = 0.5510
Round 66: Global Test Accuracy = 0.5490
Round 67: Global Test Accuracy = 0.5540
Round 68: Global Test Accuracy = 0.5540
Round 69: Global Test Accuracy = 0.5580
Round 70: Global Test Accuracy = 0.5580
Round 71: Global Test Accuracy = 0.5590
Round 72: Global Test Accuracy = 0.5630
Round 73: Global Test Accuracy = 0.5620
Round 74: Global Test Accuracy = 0.5620
Round 75: Global Test Accuracy = 0.5680
Round 76: Global Test Accuracy = 0.5660
Round 77: Global Test Accuracy = 0.5710
Round 78: Global Test Accuracy = 0.5680
Round 79: Global Test Accuracy = 0.5670
Round 80: Global Test Accuracy = 0.5680
Round 81: Global Test Accuracy = 0.5660
Round 82: Global Test Accuracy = 0.5690
Round 83: Global Test Accuracy = 0.5700
Round 84: Global Test Accuracy = 0.5690
Round 85: Global Test Accuracy = 0.5690
Round 86: Global Test Accuracy = 0.5730
Round 87: Global Test Accuracy = 0.5710
Round 88: Global Test Accuracy = 0.5750
Round 89: Global Test Accuracy = 0.5770
Round 90: Global Test Accuracy = 0.5780
Round 91: Global Test Accuracy = 0.5740
Round 92: Global Test Accuracy = 0.5800
Round 93: Global Test Accuracy = 0.5820
Round 94: Global Test Accuracy = 0.5820
Round 95: Global Test Accuracy = 0.5850
Round 96: Global Test Accuracy = 0.5800
Round 97: Global Test Accuracy = 0.5830
Round 98: Global Test Accuracy = 0.5800
Round 99: Global Test Accuracy = 0.5840
Round 100: Global Test Accuracy = 0.5830
Round 101: Global Test Accuracy = 0.5850
Round 102: Global Test Accuracy = 0.5830
Round 103: Global Test Accuracy = 0.5830
Round 104: Global Test Accuracy = 0.5820
Round 105: Global Test Accuracy = 0.5780
Round 106: Global Test Accuracy = 0.5870
Round 107: Global Test Accuracy = 0.5810
Round 108: Global Test Accuracy = 0.5850
Round 109: Global Test Accuracy = 0.5840
Round 110: Global Test Accuracy = 0.5850
Round 111: Global Test Accuracy = 0.5860
Round 112: Global Test Accuracy = 0.5840
Round 113: Global Test Accuracy = 0.5800
Round 114: Global Test Accuracy = 0.5880
Round 115: Global Test Accuracy = 0.5900
Round 116: Global Test Accuracy = 0.5870
Round 117: Global Test Accuracy = 0.5820
Round 118: Global Test Accuracy = 0.5910
Round 119: Global Test Accuracy = 0.5890
Round 120: Global Test Accuracy = 0.5890
Round 121: Global Test Accuracy = 0.5890
Round 122: Global Test Accuracy = 0.5860
Round 123: Global Test Accuracy = 0.5860
Round 124: Global Test Accuracy = 0.5890
Round 125: Global Test Accuracy = 0.5900
Round 126: Global Test Accuracy = 0.5910
Round 127: Global Test Accuracy = 0.5910
Round 128: Global Test Accuracy = 0.5900
Round 129: Global Test Accuracy = 0.5890
Round 130: Global Test Accuracy = 0.5900
Round 131: Global Test Accuracy = 0.5920
Round 132: Global Test Accuracy = 0.5920
Round 133: Global Test Accuracy = 0.5890
Round 134: Global Test Accuracy = 0.5890
Round 135: Global Test Accuracy = 0.5910
Round 136: Global Test Accuracy = 0.5910
Round 137: Global Test Accuracy = 0.5900
Round 138: Global Test Accuracy = 0.5910
Round 139: Global Test Accuracy = 0.5920
Round 140: Global Test Accuracy = 0.5880
Round 141: Global Test Accuracy = 0.5890
Round 142: Global Test Accuracy = 0.5910
Round 143: Global Test Accuracy = 0.5900
Round 144: Global Test Accuracy = 0.5900
Round 145: Global Test Accuracy = 0.5900
Round 146: Global Test Accuracy = 0.5950
Round 147: Global Test Accuracy = 0.5910
Round 148: Global Test Accuracy = 0.5970
Round 149: Global Test Accuracy = 0.5970
Round 150: Global Test Accuracy = 0.5950
Round 151: Global Test Accuracy = 0.5910
Round 152: Global Test Accuracy = 0.5910
Round 153: Global Test Accuracy = 0.5920
Round 154: Global Test Accuracy = 0.5930
Round 155: Global Test Accuracy = 0.5940
Round 156: Global Test Accuracy = 0.5960
Round 157: Global Test Accuracy = 0.5970
Round 158: Global Test Accuracy = 0.5940
Round 159: Global Test Accuracy = 0.5930
Round 160: Global Test Accuracy = 0.5950
Round 161: Global Test Accuracy = 0.5900
Round 162: Global Test Accuracy = 0.5890
Round 163: Global Test Accuracy = 0.5890
Round 164: Global Test Accuracy = 0.5890
Round 165: Global Test Accuracy = 0.5900
Round 166: Global Test Accuracy = 0.5930
Round 167: Global Test Accuracy = 0.5890
Round 168: Global Test Accuracy = 0.5910
Round 169: Global Test Accuracy = 0.5900
Round 170: Global Test Accuracy = 0.5910
Round 171: Global Test Accuracy = 0.5920
Round 172: Global Test Accuracy = 0.5920
Round 173: Global Test Accuracy = 0.5910
Round 174: Global Test Accuracy = 0.5950
Round 175: Global Test Accuracy = 0.5930
Round 176: Global Test Accuracy = 0.5950
Round 177: Global Test Accuracy = 0.5930
Round 178: Global Test Accuracy = 0.5910
Round 179: Global Test Accuracy = 0.5900
Round 180: Global Test Accuracy = 0.5890
Round 181: Global Test Accuracy = 0.5870
Round 182: Global Test Accuracy = 0.5880
Round 183: Global Test Accuracy = 0.5910
Round 184: Global Test Accuracy = 0.5890
Round 185: Global Test Accuracy = 0.5880
Round 186: Global Test Accuracy = 0.5890
Round 187: Global Test Accuracy = 0.5860
Round 188: Global Test Accuracy = 0.5850
Round 189: Global Test Accuracy = 0.5880
Round 190: Global Test Accuracy = 0.5880
Round 191: Global Test Accuracy = 0.5870
Round 192: Global Test Accuracy = 0.5850
Round 193: Global Test Accuracy = 0.5840
Round 194: Global Test Accuracy = 0.5850
Round 195: Global Test Accuracy = 0.5870
Round 196: Global Test Accuracy = 0.5890
Round 197: Global Test Accuracy = 0.5870
Round 198: Global Test Accuracy = 0.5830
Round 199: Global Test Accuracy = 0.5860
Round 200: Global Test Accuracy = 0.5830
//train_time: 6995.883 ms//end
//Log Max memory for Large1: 3329093632.0 //end
//Log Max memory for Large2: 1238515712.0 //end
//Log Max memory for Large3: 810237952.0 //end
//Log Max memory for Large4: 777994240.0 //end
//Log Max memory for Server: 1981075456.0 //end
//Log Large1 network: 53376580.0 //end
//Log Large2 network: 98985964.0 //end
//Log Large3 network: 49812216.0 //end
//Log Large4 network: 49793328.0 //end
//Log Server network: 247453451.0 //end
//Log Total Actual Train Comm Cost: 476.29 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: citeseer, Batch Size: -1, Trainers: 5, Hops: 0, IID Beta: 100.0 => Training Time = 37.00 seconds
average_final_test_loss, 1.191719984292984
Average test accuracy, 0.583

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge 
----------------------------------------------------------------------------------------------------
0        699.3        657      390      1.064        1.793       
1        688.8        683      327      1.008        2.106       
2        685.8        627      378      1.094        1.814       
3        689.4        673      420      1.024        1.641       
4        697.0        687      385      1.015        1.810       
====================================================================================================
Total Memory Usage: 3460.2 MB (3.38 GB)
Total Nodes: 3327, Total Edges: 1900
Average Memory per Trainer: 692.0 MB
Average Nodes per Trainer: 665.4
Average Edges per Trainer: 380.0
Max Memory: 699.3 MB (Trainer 0)
Min Memory: 685.8 MB (Trainer 2)
Overall Memory/Node Ratio: 1.040 MB/node
Overall Memory/Edge Ratio: 1.821 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 452.93 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
citeseer,100.0,-1,72.3,0.58,37.0,452.9,699.3,0.185,0.226,0
================================================================================

================================================================================
EXPERIMENT SUMMARY
================================================================================
Dataset: citeseer
Method: FedAvg
Trainers: 5
IID Beta: 100.0
Batch Size: -1
Hops: 0
Total Execution Time: 72.34 seconds
Training Time: 37.01 seconds
Pretrain Comm Cost: 0.00 MB
Training Comm Cost: 452.93 MB
================================================================================

[36m(Trainer pid=2889, ip=192.168.39.47)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 4x across cluster][0m
[36m(Trainer pid=2889, ip=192.168.39.47)[0m   return torch.load(io.BytesIO(b))[32m [repeated 4x across cluster][0m
Experiment 1/1 completed for:
  Dataset: citeseer, Trainers: 5, IID Beta: 100.0
  Method: fedgcn if 0 > 0 else FedAvg, Batch Size: -1

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: citeseer, Trainers: 5, Distribution: average, IID Beta: 10.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'citeseer', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 5, 'num_hops': 0, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/citeseer/raw/ind.citeseer.x
File already exists: ./data/citeseer/raw/ind.citeseer.tx
File already exists: ./data/citeseer/raw/ind.citeseer.allx
File already exists: ./data/citeseer/raw/ind.citeseer.y
File already exists: ./data/citeseer/raw/ind.citeseer.ty
File already exists: ./data/citeseer/raw/ind.citeseer.ally
File already exists: ./data/citeseer/raw/ind.citeseer.graph
File already exists: ./data/citeseer/raw/ind.citeseer.test.index
Initialization start: network data collected.
2025-07-29 22:41:19,123	INFO worker.py:1429 -- Using address 192.168.23.53:6379 set in the environment variable RAY_ADDRESS
2025-07-29 22:41:19,124	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.23.53:6379...
2025-07-29 22:41:19,130	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.23.53:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=3401, ip=192.168.2.152)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=3401, ip=192.168.2.152)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5222.339 ms //end
//Log Large1 init network: 605101.0 //end
//Log Large2 init network: 97125.0 //end
//Log Large3 init network: 131219.0 //end
//Log Large4 init network: 125113.0 //end
//Log Server init network: 49992015.0 //end
//Log Initialization Communication Cost (MB): 48.59 //end
Pretrain start time recorded.
//pretrain_time: 7.273 ms//end
//Log Max memory for Large1: 3323432960.0 //end
//Log Max memory for Large2: 807481344.0 //end
//Log Max memory for Large3: 813932544.0 //end
//Log Max memory for Large4: 1218134016.0 //end
//Log Max memory for Server: 2003259392.0 //end
//Log Large1 network: 3369966.0 //end
//Log Large2 network: 558920.0 //end
//Log Large3 network: 577209.0 //end
//Log Large4 network: 719064.0 //end
//Log Server network: 2320587.0 //end
//Log Total Actual Pretrain Comm Cost: 7.20 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1540
Round 2: Global Test Accuracy = 0.1570
Round 3: Global Test Accuracy = 0.1660
Round 4: Global Test Accuracy = 0.1730
Round 5: Global Test Accuracy = 0.1780
Round 6: Global Test Accuracy = 0.1890
Round 7: Global Test Accuracy = 0.2020
Round 8: Global Test Accuracy = 0.2100
Round 9: Global Test Accuracy = 0.2200
Round 10: Global Test Accuracy = 0.2300
Round 11: Global Test Accuracy = 0.2330
Round 12: Global Test Accuracy = 0.2440
Round 13: Global Test Accuracy = 0.2540
Round 14: Global Test Accuracy = 0.2610
Round 15: Global Test Accuracy = 0.2730
Round 16: Global Test Accuracy = 0.2760
Round 17: Global Test Accuracy = 0.2890
Round 18: Global Test Accuracy = 0.3020
Round 19: Global Test Accuracy = 0.3110
Round 20: Global Test Accuracy = 0.3200
Round 21: Global Test Accuracy = 0.3280
Round 22: Global Test Accuracy = 0.3360
Round 23: Global Test Accuracy = 0.3390
Round 24: Global Test Accuracy = 0.3510
Round 25: Global Test Accuracy = 0.3560
Round 26: Global Test Accuracy = 0.3570
Round 27: Global Test Accuracy = 0.3780
Round 28: Global Test Accuracy = 0.3840
Round 29: Global Test Accuracy = 0.3900
Round 30: Global Test Accuracy = 0.4040
Round 31: Global Test Accuracy = 0.4080
Round 32: Global Test Accuracy = 0.4200
Round 33: Global Test Accuracy = 0.4200
Round 34: Global Test Accuracy = 0.4240
Round 35: Global Test Accuracy = 0.4270
Round 36: Global Test Accuracy = 0.4350
Round 37: Global Test Accuracy = 0.4400
Round 38: Global Test Accuracy = 0.4510
Round 39: Global Test Accuracy = 0.4580
Round 40: Global Test Accuracy = 0.4680
Round 41: Global Test Accuracy = 0.4750
Round 42: Global Test Accuracy = 0.4750
Round 43: Global Test Accuracy = 0.4850
Round 44: Global Test Accuracy = 0.4890
Round 45: Global Test Accuracy = 0.4880
Round 46: Global Test Accuracy = 0.4950
Round 47: Global Test Accuracy = 0.4950
Round 48: Global Test Accuracy = 0.4970
Round 49: Global Test Accuracy = 0.5020
Round 50: Global Test Accuracy = 0.5030
Round 51: Global Test Accuracy = 0.5120
Round 52: Global Test Accuracy = 0.5150
Round 53: Global Test Accuracy = 0.5170
Round 54: Global Test Accuracy = 0.5270
Round 55: Global Test Accuracy = 0.5260
Round 56: Global Test Accuracy = 0.5310
Round 57: Global Test Accuracy = 0.5340
Round 58: Global Test Accuracy = 0.5410
Round 59: Global Test Accuracy = 0.5480
Round 60: Global Test Accuracy = 0.5490
Round 61: Global Test Accuracy = 0.5530
Round 62: Global Test Accuracy = 0.5560
Round 63: Global Test Accuracy = 0.5550
Round 64: Global Test Accuracy = 0.5560
Round 65: Global Test Accuracy = 0.5620
Round 66: Global Test Accuracy = 0.5570
Round 67: Global Test Accuracy = 0.5630
Round 68: Global Test Accuracy = 0.5680
Round 69: Global Test Accuracy = 0.5790
Round 70: Global Test Accuracy = 0.5820
Round 71: Global Test Accuracy = 0.5810
Round 72: Global Test Accuracy = 0.5850
Round 73: Global Test Accuracy = 0.5890
Round 74: Global Test Accuracy = 0.5910
Round 75: Global Test Accuracy = 0.5940
Round 76: Global Test Accuracy = 0.5960
Round 77: Global Test Accuracy = 0.5980
Round 78: Global Test Accuracy = 0.5980
Round 79: Global Test Accuracy = 0.5990
Round 80: Global Test Accuracy = 0.5940
Round 81: Global Test Accuracy = 0.5960
Round 82: Global Test Accuracy = 0.5980
Round 83: Global Test Accuracy = 0.5960
Round 84: Global Test Accuracy = 0.5990
Round 85: Global Test Accuracy = 0.6010
Round 86: Global Test Accuracy = 0.6020
Round 87: Global Test Accuracy = 0.6020
Round 88: Global Test Accuracy = 0.6040
Round 89: Global Test Accuracy = 0.6040
Round 90: Global Test Accuracy = 0.6050
Round 91: Global Test Accuracy = 0.6030
Round 92: Global Test Accuracy = 0.6040
Round 93: Global Test Accuracy = 0.6050
Round 94: Global Test Accuracy = 0.6040
Round 95: Global Test Accuracy = 0.6030
Round 96: Global Test Accuracy = 0.6070
Round 97: Global Test Accuracy = 0.6050
Round 98: Global Test Accuracy = 0.6080
Round 99: Global Test Accuracy = 0.6040
Round 100: Global Test Accuracy = 0.6050
Round 101: Global Test Accuracy = 0.6050
Round 102: Global Test Accuracy = 0.6080
Round 103: Global Test Accuracy = 0.6090
Round 104: Global Test Accuracy = 0.6060
Round 105: Global Test Accuracy = 0.6080
Round 106: Global Test Accuracy = 0.6080
Round 107: Global Test Accuracy = 0.6080
Round 108: Global Test Accuracy = 0.6120
Round 109: Global Test Accuracy = 0.6170
Round 110: Global Test Accuracy = 0.6180
Round 111: Global Test Accuracy = 0.6190
Round 112: Global Test Accuracy = 0.6180
Round 113: Global Test Accuracy = 0.6190
Round 114: Global Test Accuracy = 0.6170
Round 115: Global Test Accuracy = 0.6200
Round 116: Global Test Accuracy = 0.6200
Round 117: Global Test Accuracy = 0.6220
Round 118: Global Test Accuracy = 0.6200
Round 119: Global Test Accuracy = 0.6200
Round 120: Global Test Accuracy = 0.6190
Round 121: Global Test Accuracy = 0.6190
Round 122: Global Test Accuracy = 0.6200
Round 123: Global Test Accuracy = 0.6200
Round 124: Global Test Accuracy = 0.6200
Round 125: Global Test Accuracy = 0.6220
Round 126: Global Test Accuracy = 0.6180
Round 127: Global Test Accuracy = 0.6220
Round 128: Global Test Accuracy = 0.6250
Round 129: Global Test Accuracy = 0.6260
Round 130: Global Test Accuracy = 0.6280
Round 131: Global Test Accuracy = 0.6260
Round 132: Global Test Accuracy = 0.6270
Round 133: Global Test Accuracy = 0.6230
Round 134: Global Test Accuracy = 0.6230
Round 135: Global Test Accuracy = 0.6220
Round 136: Global Test Accuracy = 0.6240
Round 137: Global Test Accuracy = 0.6230
Round 138: Global Test Accuracy = 0.6210
Round 139: Global Test Accuracy = 0.6210
Round 140: Global Test Accuracy = 0.6200
Round 141: Global Test Accuracy = 0.6210
Round 142: Global Test Accuracy = 0.6260
Round 143: Global Test Accuracy = 0.6240
Round 144: Global Test Accuracy = 0.6240
Round 145: Global Test Accuracy = 0.6230
Round 146: Global Test Accuracy = 0.6230
Round 147: Global Test Accuracy = 0.6230
Round 148: Global Test Accuracy = 0.6220
Round 149: Global Test Accuracy = 0.6230
Round 150: Global Test Accuracy = 0.6230
Round 151: Global Test Accuracy = 0.6230
Round 152: Global Test Accuracy = 0.6240
Round 153: Global Test Accuracy = 0.6250
Round 154: Global Test Accuracy = 0.6260
Round 155: Global Test Accuracy = 0.6290
Round 156: Global Test Accuracy = 0.6260
Round 157: Global Test Accuracy = 0.6310
Round 158: Global Test Accuracy = 0.6330
Round 159: Global Test Accuracy = 0.6300
Round 160: Global Test Accuracy = 0.6320
Round 161: Global Test Accuracy = 0.6310
Round 162: Global Test Accuracy = 0.6270
Round 163: Global Test Accuracy = 0.6310
Round 164: Global Test Accuracy = 0.6310
Round 165: Global Test Accuracy = 0.6280
Round 166: Global Test Accuracy = 0.6290
Round 167: Global Test Accuracy = 0.6270
Round 168: Global Test Accuracy = 0.6280
Round 169: Global Test Accuracy = 0.6270
Round 170: Global Test Accuracy = 0.6280
Round 171: Global Test Accuracy = 0.6320
Round 172: Global Test Accuracy = 0.6310
Round 173: Global Test Accuracy = 0.6300
Round 174: Global Test Accuracy = 0.6330
Round 175: Global Test Accuracy = 0.6320
Round 176: Global Test Accuracy = 0.6300
Round 177: Global Test Accuracy = 0.6280
Round 178: Global Test Accuracy = 0.6280
Round 179: Global Test Accuracy = 0.6300
Round 180: Global Test Accuracy = 0.6310
Round 181: Global Test Accuracy = 0.6320
Round 182: Global Test Accuracy = 0.6320
Round 183: Global Test Accuracy = 0.6340
Round 184: Global Test Accuracy = 0.6330
Round 185: Global Test Accuracy = 0.6310
Round 186: Global Test Accuracy = 0.6290
Round 187: Global Test Accuracy = 0.6300
Round 188: Global Test Accuracy = 0.6330
Round 189: Global Test Accuracy = 0.6330
Round 190: Global Test Accuracy = 0.6290
Round 191: Global Test Accuracy = 0.6290
Round 192: Global Test Accuracy = 0.6270
Round 193: Global Test Accuracy = 0.6250
Round 194: Global Test Accuracy = 0.6240
Round 195: Global Test Accuracy = 0.6310
Round 196: Global Test Accuracy = 0.6300
Round 197: Global Test Accuracy = 0.6300
Round 198: Global Test Accuracy = 0.6290
Round 199: Global Test Accuracy = 0.6320
Round 200: Global Test Accuracy = 0.6300
//train_time: 7055.526000000001 ms//end
//Log Max memory for Large1: 3326885888.0 //end
//Log Max memory for Large2: 807137280.0 //end
//Log Max memory for Large3: 805150720.0 //end
//Log Max memory for Large4: 1243824128.0 //end
//Log Max memory for Server: 2002718720.0 //end
//Log Large1 network: 53405807.0 //end
//Log Large2 network: 49772294.0 //end
//Log Large3 network: 49756342.0 //end
//Log Large4 network: 98947440.0 //end
//Log Server network: 247487822.0 //end
//Log Total Actual Train Comm Cost: 476.24 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: citeseer, Batch Size: -1, Trainers: 5, Hops: 0, IID Beta: 10.0 => Training Time = 37.06 seconds
average_final_test_loss, 1.1534474582672118
Average test accuracy, 0.63

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge 
----------------------------------------------------------------------------------------------------
0        693.8        598      346      1.160        2.005       
1        684.2        639      441      1.071        1.552       
2        690.3        683      422      1.011        1.636       
3        690.0        651      361      1.060        1.911       
4        699.2        756      540      0.925        1.295       
====================================================================================================
Total Memory Usage: 3457.6 MB (3.38 GB)
Total Nodes: 3327, Total Edges: 2110
Average Memory per Trainer: 691.5 MB
Average Nodes per Trainer: 665.4
Average Edges per Trainer: 422.0
Max Memory: 699.2 MB (Trainer 4)
Min Memory: 684.2 MB (Trainer 1)
Overall Memory/Node Ratio: 1.039 MB/node
Overall Memory/Edge Ratio: 1.639 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 452.93 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
citeseer,10.0,-1,72.3,0.63,37.1,452.9,699.2,0.185,0.226,0
================================================================================

================================================================================
EXPERIMENT SUMMARY
================================================================================
Dataset: citeseer
Method: FedAvg
Trainers: 5
IID Beta: 10.0
Batch Size: -1
Hops: 0
Total Execution Time: 72.30 seconds
Training Time: 37.07 seconds
Pretrain Comm Cost: 0.00 MB
Training Comm Cost: 452.93 MB
================================================================================

[36m(Trainer pid=3294, ip=192.168.39.47)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 4x across cluster][0m
[36m(Trainer pid=3294, ip=192.168.39.47)[0m   return torch.load(io.BytesIO(b))[32m [repeated 4x across cluster][0m
Experiment 1/1 completed for:
  Dataset: citeseer, Trainers: 5, IID Beta: 10.0
  Method: fedgcn if 0 > 0 else FedAvg, Batch Size: -1

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: pubmed, Trainers: 5, Distribution: average, IID Beta: 10000.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'pubmed', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 5, 'num_hops': 0, 'iid_beta': 10000.0, 'distribution_type': 'average', 'gpu': False}
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x to ./data/pubmed/raw/ind.pubmed.x...
Downloaded ./data/pubmed/raw/ind.pubmed.x
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx to ./data/pubmed/raw/ind.pubmed.tx...
Downloaded ./data/pubmed/raw/ind.pubmed.tx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx to ./data/pubmed/raw/ind.pubmed.allx...
Downloaded ./data/pubmed/raw/ind.pubmed.allx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y to ./data/pubmed/raw/ind.pubmed.y...
Downloaded ./data/pubmed/raw/ind.pubmed.y
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty to ./data/pubmed/raw/ind.pubmed.ty...
Downloaded ./data/pubmed/raw/ind.pubmed.ty
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally to ./data/pubmed/raw/ind.pubmed.ally...
Downloaded ./data/pubmed/raw/ind.pubmed.ally
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph to ./data/pubmed/raw/ind.pubmed.graph...
Downloaded ./data/pubmed/raw/ind.pubmed.graph
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index to ./data/pubmed/raw/ind.pubmed.test.index...
Downloaded ./data/pubmed/raw/ind.pubmed.test.index
Initialization start: network data collected.
2025-07-29 22:42:44,471	INFO worker.py:1429 -- Using address 192.168.23.53:6379 set in the environment variable RAY_ADDRESS
2025-07-29 22:42:44,471	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.23.53:6379...
2025-07-29 22:42:44,478	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.23.53:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=3836, ip=192.168.2.152)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=3836, ip=192.168.2.152)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5155.512 ms //end
//Log Large1 init network: 633227.0 //end
//Log Large2 init network: 89465.0 //end
//Log Large3 init network: 61250.0 //end
//Log Large4 init network: 85936.0 //end
//Log Server init network: 41128509.0 //end
//Log Initialization Communication Cost (MB): 40.05 //end
Pretrain start time recorded.
//pretrain_time: 7.1209999999999996 ms//end
//Log Max memory for Large1: 3761954816.0 //end
//Log Max memory for Large2: 808464384.0 //end
//Log Max memory for Large3: 805281792.0 //end
//Log Max memory for Large4: 806092800.0 //end
//Log Max memory for Server: 2034339840.0 //end
//Log Large1 network: 3736521.0 //end
//Log Large2 network: 514708.0 //end
//Log Large3 network: 504120.0 //end
//Log Large4 network: 504303.0 //end
//Log Server network: 1289802.0 //end
//Log Total Actual Pretrain Comm Cost: 6.25 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.3260
Round 2: Global Test Accuracy = 0.2810
Round 3: Global Test Accuracy = 0.2540
Round 4: Global Test Accuracy = 0.2550
Round 5: Global Test Accuracy = 0.2410
Round 6: Global Test Accuracy = 0.2390
Round 7: Global Test Accuracy = 0.2280
Round 8: Global Test Accuracy = 0.2150
Round 9: Global Test Accuracy = 0.2110
Round 10: Global Test Accuracy = 0.2140
Round 11: Global Test Accuracy = 0.2200
Round 12: Global Test Accuracy = 0.2150
Round 13: Global Test Accuracy = 0.2100
Round 14: Global Test Accuracy = 0.2210
Round 15: Global Test Accuracy = 0.2170
Round 16: Global Test Accuracy = 0.2150
Round 17: Global Test Accuracy = 0.2120
Round 18: Global Test Accuracy = 0.2050
Round 19: Global Test Accuracy = 0.2100
Round 20: Global Test Accuracy = 0.2150
Round 21: Global Test Accuracy = 0.2110
Round 22: Global Test Accuracy = 0.2080
Round 23: Global Test Accuracy = 0.2090
Round 24: Global Test Accuracy = 0.2100
Round 25: Global Test Accuracy = 0.2110
Round 26: Global Test Accuracy = 0.2210
Round 27: Global Test Accuracy = 0.2300
Round 28: Global Test Accuracy = 0.2380
Round 29: Global Test Accuracy = 0.2520
Round 30: Global Test Accuracy = 0.2500
Round 31: Global Test Accuracy = 0.2550
Round 32: Global Test Accuracy = 0.2800
Round 33: Global Test Accuracy = 0.2660
Round 34: Global Test Accuracy = 0.2920
Round 35: Global Test Accuracy = 0.3030
Round 36: Global Test Accuracy = 0.2930
Round 37: Global Test Accuracy = 0.2910
Round 38: Global Test Accuracy = 0.2960
Round 39: Global Test Accuracy = 0.2800
Round 40: Global Test Accuracy = 0.2580
Round 41: Global Test Accuracy = 0.2490
Round 42: Global Test Accuracy = 0.2560
Round 43: Global Test Accuracy = 0.2610
Round 44: Global Test Accuracy = 0.2440
Round 45: Global Test Accuracy = 0.2340
Round 46: Global Test Accuracy = 0.2340
Round 47: Global Test Accuracy = 0.2310
Round 48: Global Test Accuracy = 0.2280
Round 49: Global Test Accuracy = 0.2440
Round 50: Global Test Accuracy = 0.2590
Round 51: Global Test Accuracy = 0.2480
Round 52: Global Test Accuracy = 0.2380
Round 53: Global Test Accuracy = 0.2410
Round 54: Global Test Accuracy = 0.2460
Round 55: Global Test Accuracy = 0.2410
Round 56: Global Test Accuracy = 0.2270
Round 57: Global Test Accuracy = 0.2250
Round 58: Global Test Accuracy = 0.2280
Round 59: Global Test Accuracy = 0.2240
Round 60: Global Test Accuracy = 0.2420
Round 61: Global Test Accuracy = 0.2350
Round 62: Global Test Accuracy = 0.2430
Round 63: Global Test Accuracy = 0.2410
Round 64: Global Test Accuracy = 0.2360
Round 65: Global Test Accuracy = 0.2220
Round 66: Global Test Accuracy = 0.2250
Round 67: Global Test Accuracy = 0.2250
Round 68: Global Test Accuracy = 0.2310
Round 69: Global Test Accuracy = 0.2270
Round 70: Global Test Accuracy = 0.2420
Round 71: Global Test Accuracy = 0.2360
Round 72: Global Test Accuracy = 0.2390
Round 73: Global Test Accuracy = 0.2430
Round 74: Global Test Accuracy = 0.2330
Round 75: Global Test Accuracy = 0.2260
Round 76: Global Test Accuracy = 0.2350
Round 77: Global Test Accuracy = 0.2450
Round 78: Global Test Accuracy = 0.2450
Round 79: Global Test Accuracy = 0.2730
Round 80: Global Test Accuracy = 0.2470
Round 81: Global Test Accuracy = 0.2380
Round 82: Global Test Accuracy = 0.2350
Round 83: Global Test Accuracy = 0.2380
Round 84: Global Test Accuracy = 0.2380
Round 85: Global Test Accuracy = 0.2390
Round 86: Global Test Accuracy = 0.2520
Round 87: Global Test Accuracy = 0.2470
Round 88: Global Test Accuracy = 0.2620
Round 89: Global Test Accuracy = 0.2710
Round 90: Global Test Accuracy = 0.2800
Round 91: Global Test Accuracy = 0.2710
Round 92: Global Test Accuracy = 0.2780
Round 93: Global Test Accuracy = 0.2480
Round 94: Global Test Accuracy = 0.2490
Round 95: Global Test Accuracy = 0.2440
Round 96: Global Test Accuracy = 0.2380
Round 97: Global Test Accuracy = 0.2400
Round 98: Global Test Accuracy = 0.2410
Round 99: Global Test Accuracy = 0.2360
Round 100: Global Test Accuracy = 0.2320
Round 101: Global Test Accuracy = 0.2320
Round 102: Global Test Accuracy = 0.2290
Round 103: Global Test Accuracy = 0.2410
Round 104: Global Test Accuracy = 0.2400
Round 105: Global Test Accuracy = 0.2240
Round 106: Global Test Accuracy = 0.2230
Round 107: Global Test Accuracy = 0.2220
Round 108: Global Test Accuracy = 0.2390
Round 109: Global Test Accuracy = 0.2300
Round 110: Global Test Accuracy = 0.2280
Round 111: Global Test Accuracy = 0.2280
Round 112: Global Test Accuracy = 0.2500
Round 113: Global Test Accuracy = 0.2430
Round 114: Global Test Accuracy = 0.2560
Round 115: Global Test Accuracy = 0.2590
Round 116: Global Test Accuracy = 0.2690
Round 117: Global Test Accuracy = 0.2770
Round 118: Global Test Accuracy = 0.2780
Round 119: Global Test Accuracy = 0.2730
Round 120: Global Test Accuracy = 0.2620
Round 121: Global Test Accuracy = 0.2470
Round 122: Global Test Accuracy = 0.2670
Round 123: Global Test Accuracy = 0.2600
Round 124: Global Test Accuracy = 0.3030
Round 125: Global Test Accuracy = 0.2930
Round 126: Global Test Accuracy = 0.2940
Round 127: Global Test Accuracy = 0.2620
Round 128: Global Test Accuracy = 0.2710
Round 129: Global Test Accuracy = 0.2920
Round 130: Global Test Accuracy = 0.3030
Round 131: Global Test Accuracy = 0.3310
Round 132: Global Test Accuracy = 0.3540
Round 133: Global Test Accuracy = 0.3640
Round 134: Global Test Accuracy = 0.3210
Round 135: Global Test Accuracy = 0.2970
Round 136: Global Test Accuracy = 0.3160
Round 137: Global Test Accuracy = 0.3390
Round 138: Global Test Accuracy = 0.3770
Round 139: Global Test Accuracy = 0.3360
Round 140: Global Test Accuracy = 0.3310
Round 141: Global Test Accuracy = 0.3260
Round 142: Global Test Accuracy = 0.2870
Round 143: Global Test Accuracy = 0.2770
Round 144: Global Test Accuracy = 0.2810
Round 145: Global Test Accuracy = 0.2940
Round 146: Global Test Accuracy = 0.3270
Round 147: Global Test Accuracy = 0.3100
Round 148: Global Test Accuracy = 0.2700
Round 149: Global Test Accuracy = 0.2700
Round 150: Global Test Accuracy = 0.2510
Round 151: Global Test Accuracy = 0.2640
Round 152: Global Test Accuracy = 0.2620
Round 153: Global Test Accuracy = 0.2620
Round 154: Global Test Accuracy = 0.2380
Round 155: Global Test Accuracy = 0.2280
Round 156: Global Test Accuracy = 0.2310
Round 157: Global Test Accuracy = 0.2610
Round 158: Global Test Accuracy = 0.2470
Round 159: Global Test Accuracy = 0.2340
Round 160: Global Test Accuracy = 0.2260
Round 161: Global Test Accuracy = 0.2420
Round 162: Global Test Accuracy = 0.2570
Round 163: Global Test Accuracy = 0.2610
Round 164: Global Test Accuracy = 0.2630
Round 165: Global Test Accuracy = 0.2630
Round 166: Global Test Accuracy = 0.2410
Round 167: Global Test Accuracy = 0.2610
Round 168: Global Test Accuracy = 0.2580
Round 169: Global Test Accuracy = 0.2540
Round 170: Global Test Accuracy = 0.2420
Round 171: Global Test Accuracy = 0.2290
Round 172: Global Test Accuracy = 0.2630
Round 173: Global Test Accuracy = 0.2770
Round 174: Global Test Accuracy = 0.2850
Round 175: Global Test Accuracy = 0.2760
Round 176: Global Test Accuracy = 0.2720
Round 177: Global Test Accuracy = 0.2890
Round 178: Global Test Accuracy = 0.2880
Round 179: Global Test Accuracy = 0.2920
Round 180: Global Test Accuracy = 0.2990
Round 181: Global Test Accuracy = 0.3280
Round 182: Global Test Accuracy = 0.3050
Round 183: Global Test Accuracy = 0.3040
Round 184: Global Test Accuracy = 0.3090
Round 185: Global Test Accuracy = 0.3180
Round 186: Global Test Accuracy = 0.3150
Round 187: Global Test Accuracy = 0.3100
Round 188: Global Test Accuracy = 0.2840
Round 189: Global Test Accuracy = 0.2910
Round 190: Global Test Accuracy = 0.3110
Round 191: Global Test Accuracy = 0.3250
Round 192: Global Test Accuracy = 0.3110
Round 193: Global Test Accuracy = 0.3330
Round 194: Global Test Accuracy = 0.3520
Round 195: Global Test Accuracy = 0.2920
Round 196: Global Test Accuracy = 0.2930
Round 197: Global Test Accuracy = 0.3150
Round 198: Global Test Accuracy = 0.3170
Round 199: Global Test Accuracy = 0.3290
Round 200: Global Test Accuracy = 0.3430
//train_time: 4020.734 ms//end
//Log Max memory for Large1: 3794452480.0 //end
//Log Max memory for Large2: 825339904.0 //end
//Log Max memory for Large3: 818200576.0 //end
//Log Max memory for Large4: 820772864.0 //end
//Log Max memory for Server: 2075832320.0 //end
//Log Large1 network: 18388135.0 //end
//Log Large2 network: 7731271.0 //end
//Log Large3 network: 7758578.0 //end
//Log Large4 network: 7771346.0 //end
//Log Server network: 38283315.0 //end
//Log Total Actual Train Comm Cost: 76.23 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: pubmed, Batch Size: -1, Trainers: 5, Hops: 0, IID Beta: 10000.0 => Training Time = 34.02 seconds
average_final_test_loss, 1.09515562748909
Average test accuracy, 0.343

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge 
----------------------------------------------------------------------------------------------------
0        681.2        3942     3680     0.173        0.185       
1        678.8        3961     3642     0.171        0.186       
2        681.4        3962     3558     0.172        0.192       
3        679.5        3945     3534     0.172        0.192       
4        681.0        3907     3535     0.174        0.193       
====================================================================================================
Total Memory Usage: 3401.9 MB (3.32 GB)
Total Nodes: 19717, Total Edges: 17949
Average Memory per Trainer: 680.4 MB
Average Nodes per Trainer: 3943.4
Average Edges per Trainer: 3589.8
Max Memory: 681.4 MB (Trainer 2)
Min Memory: 678.8 MB (Trainer 1)
Overall Memory/Node Ratio: 0.173 MB/node
Overall Memory/Edge Ratio: 0.190 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 61.55 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
pubmed,10000.0,-1,69.2,0.34,34.0,61.5,681.4,0.170,0.031,0
================================================================================

================================================================================
EXPERIMENT SUMMARY
================================================================================
Dataset: pubmed
Method: FedAvg
Trainers: 5
IID Beta: 10000.0
Batch Size: -1
Hops: 0
Total Execution Time: 69.19 seconds
Training Time: 34.03 seconds
Pretrain Comm Cost: 0.00 MB
Training Comm Cost: 61.55 MB
================================================================================

[36m(Trainer pid=3824, ip=192.168.52.140)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 4x across cluster][0m
[36m(Trainer pid=3824, ip=192.168.52.140)[0m   return torch.load(io.BytesIO(b))[32m [repeated 4x across cluster][0m
Experiment 1/1 completed for:
  Dataset: pubmed, Trainers: 5, IID Beta: 10000.0
  Method: fedgcn if 0 > 0 else FedAvg, Batch Size: -1

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: pubmed, Trainers: 5, Distribution: average, IID Beta: 100.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'pubmed', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 5, 'num_hops': 0, 'iid_beta': 100.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/pubmed/raw/ind.pubmed.x
File already exists: ./data/pubmed/raw/ind.pubmed.tx
File already exists: ./data/pubmed/raw/ind.pubmed.allx
File already exists: ./data/pubmed/raw/ind.pubmed.y
File already exists: ./data/pubmed/raw/ind.pubmed.ty
File already exists: ./data/pubmed/raw/ind.pubmed.ally
File already exists: ./data/pubmed/raw/ind.pubmed.graph
File already exists: ./data/pubmed/raw/ind.pubmed.test.index
Initialization start: network data collected.
2025-07-29 22:44:04,415	INFO worker.py:1429 -- Using address 192.168.23.53:6379 set in the environment variable RAY_ADDRESS
2025-07-29 22:44:04,415	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.23.53:6379...
2025-07-29 22:44:04,421	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.23.53:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=4250, ip=192.168.2.152)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=4250, ip=192.168.2.152)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5204.125999999999 ms //end
//Log Large1 init network: 619218.0 //end
//Log Large2 init network: 96807.0 //end
//Log Large3 init network: 111430.0 //end
//Log Large4 init network: 88127.0 //end
//Log Server init network: 41119100.0 //end
//Log Initialization Communication Cost (MB): 40.09 //end
Pretrain start time recorded.
//pretrain_time: 7.694999999999999 ms//end
//Log Max memory for Large1: 3336531968.0 //end
//Log Max memory for Large2: 816209920.0 //end
//Log Max memory for Large3: 1239924736.0 //end
//Log Max memory for Large4: 815947776.0 //end
//Log Max memory for Server: 2096508928.0 //end
//Log Large1 network: 3322026.0 //end
//Log Large2 network: 490170.0 //end
//Log Large3 network: 636675.0 //end
//Log Large4 network: 502562.0 //end
//Log Server network: 1287947.0 //end
//Log Total Actual Pretrain Comm Cost: 5.95 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.3290
Round 2: Global Test Accuracy = 0.2810
Round 3: Global Test Accuracy = 0.2370
Round 4: Global Test Accuracy = 0.2170
Round 5: Global Test Accuracy = 0.2220
Round 6: Global Test Accuracy = 0.2120
Round 7: Global Test Accuracy = 0.2020
Round 8: Global Test Accuracy = 0.1940
Round 9: Global Test Accuracy = 0.1930
Round 10: Global Test Accuracy = 0.1870
Round 11: Global Test Accuracy = 0.1860
Round 12: Global Test Accuracy = 0.1870
Round 13: Global Test Accuracy = 0.1870
Round 14: Global Test Accuracy = 0.1880
Round 15: Global Test Accuracy = 0.1810
Round 16: Global Test Accuracy = 0.1800
Round 17: Global Test Accuracy = 0.1800
Round 18: Global Test Accuracy = 0.1820
Round 19: Global Test Accuracy = 0.1880
Round 20: Global Test Accuracy = 0.1870
Round 21: Global Test Accuracy = 0.1900
Round 22: Global Test Accuracy = 0.1850
Round 23: Global Test Accuracy = 0.1820
Round 24: Global Test Accuracy = 0.1850
Round 25: Global Test Accuracy = 0.1860
Round 26: Global Test Accuracy = 0.1830
Round 27: Global Test Accuracy = 0.1830
Round 28: Global Test Accuracy = 0.1830
Round 29: Global Test Accuracy = 0.1820
Round 30: Global Test Accuracy = 0.1820
Round 31: Global Test Accuracy = 0.1820
Round 32: Global Test Accuracy = 0.1830
Round 33: Global Test Accuracy = 0.1840
Round 34: Global Test Accuracy = 0.1840
Round 35: Global Test Accuracy = 0.1830
Round 36: Global Test Accuracy = 0.1830
Round 37: Global Test Accuracy = 0.1830
Round 38: Global Test Accuracy = 0.1830
Round 39: Global Test Accuracy = 0.1820
Round 40: Global Test Accuracy = 0.1820
Round 41: Global Test Accuracy = 0.1830
Round 42: Global Test Accuracy = 0.1820
Round 43: Global Test Accuracy = 0.1810
Round 44: Global Test Accuracy = 0.1810
Round 45: Global Test Accuracy = 0.1830
Round 46: Global Test Accuracy = 0.1830
Round 47: Global Test Accuracy = 0.1820
Round 48: Global Test Accuracy = 0.1810
Round 49: Global Test Accuracy = 0.1820
Round 50: Global Test Accuracy = 0.1810
Round 51: Global Test Accuracy = 0.1820
Round 52: Global Test Accuracy = 0.1820
Round 53: Global Test Accuracy = 0.1810
Round 54: Global Test Accuracy = 0.1820
Round 55: Global Test Accuracy = 0.1820
Round 56: Global Test Accuracy = 0.1820
Round 57: Global Test Accuracy = 0.1820
Round 58: Global Test Accuracy = 0.1820
Round 59: Global Test Accuracy = 0.1820
Round 60: Global Test Accuracy = 0.1820
Round 61: Global Test Accuracy = 0.1820
Round 62: Global Test Accuracy = 0.1820
Round 63: Global Test Accuracy = 0.1820
Round 64: Global Test Accuracy = 0.1810
Round 65: Global Test Accuracy = 0.1820
Round 66: Global Test Accuracy = 0.1820
Round 67: Global Test Accuracy = 0.1820
Round 68: Global Test Accuracy = 0.1820
Round 69: Global Test Accuracy = 0.1820
Round 70: Global Test Accuracy = 0.1820
Round 71: Global Test Accuracy = 0.1820
Round 72: Global Test Accuracy = 0.1820
Round 73: Global Test Accuracy = 0.1820
Round 74: Global Test Accuracy = 0.1820
Round 75: Global Test Accuracy = 0.1820
Round 76: Global Test Accuracy = 0.1820
Round 77: Global Test Accuracy = 0.1820
Round 78: Global Test Accuracy = 0.1820
Round 79: Global Test Accuracy = 0.1830
Round 80: Global Test Accuracy = 0.1830
Round 81: Global Test Accuracy = 0.1830
Round 82: Global Test Accuracy = 0.1830
Round 83: Global Test Accuracy = 0.1830
Round 84: Global Test Accuracy = 0.1830
Round 85: Global Test Accuracy = 0.1830
Round 86: Global Test Accuracy = 0.1830
Round 87: Global Test Accuracy = 0.1830
Round 88: Global Test Accuracy = 0.1830
Round 89: Global Test Accuracy = 0.1830
Round 90: Global Test Accuracy = 0.1830
Round 91: Global Test Accuracy = 0.1830
Round 92: Global Test Accuracy = 0.1830
Round 93: Global Test Accuracy = 0.1830
Round 94: Global Test Accuracy = 0.1830
Round 95: Global Test Accuracy = 0.1830
Round 96: Global Test Accuracy = 0.1830
Round 97: Global Test Accuracy = 0.1830
Round 98: Global Test Accuracy = 0.1830
Round 99: Global Test Accuracy = 0.1830
Round 100: Global Test Accuracy = 0.1830
Round 101: Global Test Accuracy = 0.1830
Round 102: Global Test Accuracy = 0.1830
Round 103: Global Test Accuracy = 0.1840
Round 104: Global Test Accuracy = 0.1840
Round 105: Global Test Accuracy = 0.1830
Round 106: Global Test Accuracy = 0.1820
Round 107: Global Test Accuracy = 0.1830
Round 108: Global Test Accuracy = 0.1830
Round 109: Global Test Accuracy = 0.1830
Round 110: Global Test Accuracy = 0.1830
Round 111: Global Test Accuracy = 0.1830
Round 112: Global Test Accuracy = 0.1830
Round 113: Global Test Accuracy = 0.1830
Round 114: Global Test Accuracy = 0.1830
Round 115: Global Test Accuracy = 0.1830
Round 116: Global Test Accuracy = 0.1840
Round 117: Global Test Accuracy = 0.1840
Round 118: Global Test Accuracy = 0.1840
Round 119: Global Test Accuracy = 0.1840
Round 120: Global Test Accuracy = 0.1830
Round 121: Global Test Accuracy = 0.1830
Round 122: Global Test Accuracy = 0.1830
Round 123: Global Test Accuracy = 0.1840
Round 124: Global Test Accuracy = 0.1840
Round 125: Global Test Accuracy = 0.1840
Round 126: Global Test Accuracy = 0.1840
Round 127: Global Test Accuracy = 0.1840
Round 128: Global Test Accuracy = 0.1860
Round 129: Global Test Accuracy = 0.1850
Round 130: Global Test Accuracy = 0.1840
Round 131: Global Test Accuracy = 0.1840
Round 132: Global Test Accuracy = 0.1850
Round 133: Global Test Accuracy = 0.1860
Round 134: Global Test Accuracy = 0.1850
Round 135: Global Test Accuracy = 0.1840
Round 136: Global Test Accuracy = 0.1850
Round 137: Global Test Accuracy = 0.1860
Round 138: Global Test Accuracy = 0.1860
Round 139: Global Test Accuracy = 0.1860
Round 140: Global Test Accuracy = 0.1850
Round 141: Global Test Accuracy = 0.1870
Round 142: Global Test Accuracy = 0.1850
Round 143: Global Test Accuracy = 0.1880
Round 144: Global Test Accuracy = 0.1860
Round 145: Global Test Accuracy = 0.1870
Round 146: Global Test Accuracy = 0.1870
Round 147: Global Test Accuracy = 0.1880
Round 148: Global Test Accuracy = 0.1900
Round 149: Global Test Accuracy = 0.1890
Round 150: Global Test Accuracy = 0.1890
Round 151: Global Test Accuracy = 0.1900
Round 152: Global Test Accuracy = 0.1910
Round 153: Global Test Accuracy = 0.1910
Round 154: Global Test Accuracy = 0.1890
Round 155: Global Test Accuracy = 0.1900
Round 156: Global Test Accuracy = 0.1910
Round 157: Global Test Accuracy = 0.1910
Round 158: Global Test Accuracy = 0.1910
Round 159: Global Test Accuracy = 0.1900
Round 160: Global Test Accuracy = 0.1920
Round 161: Global Test Accuracy = 0.1930
Round 162: Global Test Accuracy = 0.1920
Round 163: Global Test Accuracy = 0.1900
Round 164: Global Test Accuracy = 0.1920
Round 165: Global Test Accuracy = 0.1920
Round 166: Global Test Accuracy = 0.1900
Round 167: Global Test Accuracy = 0.1910
Round 168: Global Test Accuracy = 0.1900
Round 169: Global Test Accuracy = 0.1900
Round 170: Global Test Accuracy = 0.1900
Round 171: Global Test Accuracy = 0.1890
Round 172: Global Test Accuracy = 0.1890
Round 173: Global Test Accuracy = 0.1920
Round 174: Global Test Accuracy = 0.1960
Round 175: Global Test Accuracy = 0.1950
Round 176: Global Test Accuracy = 0.1950
Round 177: Global Test Accuracy = 0.2020
Round 178: Global Test Accuracy = 0.2000
Round 179: Global Test Accuracy = 0.2000
Round 180: Global Test Accuracy = 0.2000
Round 181: Global Test Accuracy = 0.2010
Round 182: Global Test Accuracy = 0.2020
Round 183: Global Test Accuracy = 0.1970
Round 184: Global Test Accuracy = 0.2060
Round 185: Global Test Accuracy = 0.2010
Round 186: Global Test Accuracy = 0.2000
Round 187: Global Test Accuracy = 0.2010
Round 188: Global Test Accuracy = 0.2020
Round 189: Global Test Accuracy = 0.2110
Round 190: Global Test Accuracy = 0.2110
Round 191: Global Test Accuracy = 0.2110
Round 192: Global Test Accuracy = 0.2110
Round 193: Global Test Accuracy = 0.2010
Round 194: Global Test Accuracy = 0.1980
Round 195: Global Test Accuracy = 0.1960
Round 196: Global Test Accuracy = 0.1930
Round 197: Global Test Accuracy = 0.1950
Round 198: Global Test Accuracy = 0.1970
Round 199: Global Test Accuracy = 0.2000
Round 200: Global Test Accuracy = 0.1970
//train_time: 4286.401 ms//end
//Log Max memory for Large1: 3347361792.0 //end
//Log Max memory for Large2: 829579264.0 //end
//Log Max memory for Large3: 1270026240.0 //end
//Log Max memory for Large4: 827932672.0 //end
//Log Max memory for Server: 2134470656.0 //end
//Log Large1 network: 11061524.0 //end
//Log Large2 network: 7694670.0 //end
//Log Large3 network: 15020137.0 //end
//Log Large4 network: 7745356.0 //end
//Log Server network: 38303052.0 //end
//Log Total Actual Train Comm Cost: 76.13 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: pubmed, Batch Size: -1, Trainers: 5, Hops: 0, IID Beta: 100.0 => Training Time = 34.29 seconds
average_final_test_loss, 1.0999395997524262
Average test accuracy, 0.197

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge 
----------------------------------------------------------------------------------------------------
0        675.9        3962     3764     0.171        0.180       
1        676.0        3620     3112     0.187        0.217       
2        676.8        3662     3036     0.185        0.223       
3        679.3        4237     3657     0.160        0.186       
4        679.3        4236     4222     0.160        0.161       
====================================================================================================
Total Memory Usage: 3387.4 MB (3.31 GB)
Total Nodes: 19717, Total Edges: 17791
Average Memory per Trainer: 677.5 MB
Average Nodes per Trainer: 3943.4
Average Edges per Trainer: 3558.2
Max Memory: 679.3 MB (Trainer 4)
Min Memory: 675.9 MB (Trainer 0)
Overall Memory/Node Ratio: 0.172 MB/node
Overall Memory/Edge Ratio: 0.190 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 61.55 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
pubmed,100.0,-1,69.5,0.20,34.3,61.5,679.3,0.171,0.031,0
================================================================================

================================================================================
EXPERIMENT SUMMARY
================================================================================
Dataset: pubmed
Method: FedAvg
Trainers: 5
IID Beta: 100.0
Batch Size: -1
Hops: 0
Total Execution Time: 69.51 seconds
Training Time: 34.30 seconds
Pretrain Comm Cost: 0.00 MB
Training Comm Cost: 61.55 MB
================================================================================

[36m(Trainer pid=4149, ip=192.168.39.47)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 4x across cluster][0m
[36m(Trainer pid=4149, ip=192.168.39.47)[0m   return torch.load(io.BytesIO(b))[32m [repeated 4x across cluster][0m
Experiment 1/1 completed for:
  Dataset: pubmed, Trainers: 5, IID Beta: 100.0
  Method: fedgcn if 0 > 0 else FedAvg, Batch Size: -1

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: pubmed, Trainers: 5, Distribution: average, IID Beta: 10.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'pubmed', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 5, 'num_hops': 0, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/pubmed/raw/ind.pubmed.x
File already exists: ./data/pubmed/raw/ind.pubmed.tx
File already exists: ./data/pubmed/raw/ind.pubmed.allx
File already exists: ./data/pubmed/raw/ind.pubmed.y
File already exists: ./data/pubmed/raw/ind.pubmed.ty
File already exists: ./data/pubmed/raw/ind.pubmed.ally
File already exists: ./data/pubmed/raw/ind.pubmed.graph
File already exists: ./data/pubmed/raw/ind.pubmed.test.index
Initialization start: network data collected.
2025-07-29 22:45:25,019	INFO worker.py:1429 -- Using address 192.168.23.53:6379 set in the environment variable RAY_ADDRESS
2025-07-29 22:45:25,019	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.23.53:6379...
2025-07-29 22:45:25,024	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.23.53:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=4653, ip=192.168.52.140)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=4653, ip=192.168.52.140)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5133.371 ms //end
//Log Large1 init network: 596354.0 //end
//Log Large2 init network: 116763.0 //end
//Log Large3 init network: 93307.0 //end
//Log Large4 init network: 91076.0 //end
//Log Server init network: 41164494.0 //end
//Log Initialization Communication Cost (MB): 40.11 //end
Pretrain start time recorded.
//pretrain_time: 8.507000000000001 ms//end
//Log Max memory for Large1: 3337461760.0 //end
//Log Max memory for Large2: 1246085120.0 //end
//Log Max memory for Large3: 827392000.0 //end
//Log Max memory for Large4: 826585088.0 //end
//Log Max memory for Server: 2138763264.0 //end
//Log Large1 network: 3600776.0 //end
//Log Large2 network: 604153.0 //end
//Log Large3 network: 521826.0 //end
//Log Large4 network: 515617.0 //end
//Log Server network: 1385217.0 //end
//Log Total Actual Pretrain Comm Cost: 6.32 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.3390
Round 2: Global Test Accuracy = 0.3180
Round 3: Global Test Accuracy = 0.3230
Round 4: Global Test Accuracy = 0.3220
Round 5: Global Test Accuracy = 0.3380
Round 6: Global Test Accuracy = 0.3410
Round 7: Global Test Accuracy = 0.3600
Round 8: Global Test Accuracy = 0.3680
Round 9: Global Test Accuracy = 0.3750
Round 10: Global Test Accuracy = 0.3730
Round 11: Global Test Accuracy = 0.3780
Round 12: Global Test Accuracy = 0.3940
Round 13: Global Test Accuracy = 0.3940
Round 14: Global Test Accuracy = 0.3950
Round 15: Global Test Accuracy = 0.3940
Round 16: Global Test Accuracy = 0.3970
Round 17: Global Test Accuracy = 0.4020
Round 18: Global Test Accuracy = 0.4020
Round 19: Global Test Accuracy = 0.4040
Round 20: Global Test Accuracy = 0.4060
Round 21: Global Test Accuracy = 0.4060
Round 22: Global Test Accuracy = 0.4050
Round 23: Global Test Accuracy = 0.4060
Round 24: Global Test Accuracy = 0.4060
Round 25: Global Test Accuracy = 0.4060
Round 26: Global Test Accuracy = 0.4060
Round 27: Global Test Accuracy = 0.4060
Round 28: Global Test Accuracy = 0.4050
Round 29: Global Test Accuracy = 0.4060
Round 30: Global Test Accuracy = 0.4040
Round 31: Global Test Accuracy = 0.4040
Round 32: Global Test Accuracy = 0.4040
Round 33: Global Test Accuracy = 0.4040
Round 34: Global Test Accuracy = 0.4060
Round 35: Global Test Accuracy = 0.4080
Round 36: Global Test Accuracy = 0.4080
Round 37: Global Test Accuracy = 0.4080
Round 38: Global Test Accuracy = 0.4080
Round 39: Global Test Accuracy = 0.4080
Round 40: Global Test Accuracy = 0.4070
Round 41: Global Test Accuracy = 0.4070
Round 42: Global Test Accuracy = 0.4080
Round 43: Global Test Accuracy = 0.4080
Round 44: Global Test Accuracy = 0.4080
Round 45: Global Test Accuracy = 0.4080
Round 46: Global Test Accuracy = 0.4080
Round 47: Global Test Accuracy = 0.4080
Round 48: Global Test Accuracy = 0.4080
Round 49: Global Test Accuracy = 0.4080
Round 50: Global Test Accuracy = 0.4080
Round 51: Global Test Accuracy = 0.4080
Round 52: Global Test Accuracy = 0.4080
Round 53: Global Test Accuracy = 0.4070
Round 54: Global Test Accuracy = 0.4070
Round 55: Global Test Accuracy = 0.4080
Round 56: Global Test Accuracy = 0.4080
Round 57: Global Test Accuracy = 0.4070
Round 58: Global Test Accuracy = 0.4070
Round 59: Global Test Accuracy = 0.4070
Round 60: Global Test Accuracy = 0.4070
Round 61: Global Test Accuracy = 0.4070
Round 62: Global Test Accuracy = 0.4070
Round 63: Global Test Accuracy = 0.4070
Round 64: Global Test Accuracy = 0.4070
Round 65: Global Test Accuracy = 0.4070
Round 66: Global Test Accuracy = 0.4070
Round 67: Global Test Accuracy = 0.4070
Round 68: Global Test Accuracy = 0.4070
Round 69: Global Test Accuracy = 0.4070
Round 70: Global Test Accuracy = 0.4070
Round 71: Global Test Accuracy = 0.4070
Round 72: Global Test Accuracy = 0.4070
Round 73: Global Test Accuracy = 0.4070
Round 74: Global Test Accuracy = 0.4070
Round 75: Global Test Accuracy = 0.4070
Round 76: Global Test Accuracy = 0.4080
Round 77: Global Test Accuracy = 0.4080
Round 78: Global Test Accuracy = 0.4070
Round 79: Global Test Accuracy = 0.4070
Round 80: Global Test Accuracy = 0.4070
Round 81: Global Test Accuracy = 0.4070
Round 82: Global Test Accuracy = 0.4070
Round 83: Global Test Accuracy = 0.4070
Round 84: Global Test Accuracy = 0.4070
Round 85: Global Test Accuracy = 0.4070
Round 86: Global Test Accuracy = 0.4070
Round 87: Global Test Accuracy = 0.4070
Round 88: Global Test Accuracy = 0.4070
Round 89: Global Test Accuracy = 0.4070
Round 90: Global Test Accuracy = 0.4070
Round 91: Global Test Accuracy = 0.4070
Round 92: Global Test Accuracy = 0.4070
Round 93: Global Test Accuracy = 0.4070
Round 94: Global Test Accuracy = 0.4070
Round 95: Global Test Accuracy = 0.4070
Round 96: Global Test Accuracy = 0.4070
Round 97: Global Test Accuracy = 0.4070
Round 98: Global Test Accuracy = 0.4070
Round 99: Global Test Accuracy = 0.4070
Round 100: Global Test Accuracy = 0.4070
Round 101: Global Test Accuracy = 0.4070
Round 102: Global Test Accuracy = 0.4070
Round 103: Global Test Accuracy = 0.4070
Round 104: Global Test Accuracy = 0.4070
Round 105: Global Test Accuracy = 0.4070
Round 106: Global Test Accuracy = 0.4070
Round 107: Global Test Accuracy = 0.4070
Round 108: Global Test Accuracy = 0.4070
Round 109: Global Test Accuracy = 0.4070
Round 110: Global Test Accuracy = 0.4070
Round 111: Global Test Accuracy = 0.4070
Round 112: Global Test Accuracy = 0.4070
Round 113: Global Test Accuracy = 0.4070
Round 114: Global Test Accuracy = 0.4070
Round 115: Global Test Accuracy = 0.4070
Round 116: Global Test Accuracy = 0.4070
Round 117: Global Test Accuracy = 0.4070
Round 118: Global Test Accuracy = 0.4070
Round 119: Global Test Accuracy = 0.4070
Round 120: Global Test Accuracy = 0.4070
Round 121: Global Test Accuracy = 0.4070
Round 122: Global Test Accuracy = 0.4070
Round 123: Global Test Accuracy = 0.4070
Round 124: Global Test Accuracy = 0.4070
Round 125: Global Test Accuracy = 0.4070
Round 126: Global Test Accuracy = 0.4070
Round 127: Global Test Accuracy = 0.4070
Round 128: Global Test Accuracy = 0.4070
Round 129: Global Test Accuracy = 0.4070
Round 130: Global Test Accuracy = 0.4070
Round 131: Global Test Accuracy = 0.4070
Round 132: Global Test Accuracy = 0.4070
Round 133: Global Test Accuracy = 0.4070
Round 134: Global Test Accuracy = 0.4070
Round 135: Global Test Accuracy = 0.4070
Round 136: Global Test Accuracy = 0.4070
Round 137: Global Test Accuracy = 0.4070
Round 138: Global Test Accuracy = 0.4070
Round 139: Global Test Accuracy = 0.4070
Round 140: Global Test Accuracy = 0.4070
Round 141: Global Test Accuracy = 0.4070
Round 142: Global Test Accuracy = 0.4070
Round 143: Global Test Accuracy = 0.4070
Round 144: Global Test Accuracy = 0.4070
Round 145: Global Test Accuracy = 0.4070
Round 146: Global Test Accuracy = 0.4070
Round 147: Global Test Accuracy = 0.4070
Round 148: Global Test Accuracy = 0.4070
Round 149: Global Test Accuracy = 0.4070
Round 150: Global Test Accuracy = 0.4060
Round 151: Global Test Accuracy = 0.4070
Round 152: Global Test Accuracy = 0.4070
Round 153: Global Test Accuracy = 0.4080
Round 154: Global Test Accuracy = 0.4070
Round 155: Global Test Accuracy = 0.4070
Round 156: Global Test Accuracy = 0.4070
Round 157: Global Test Accuracy = 0.4060
Round 158: Global Test Accuracy = 0.4060
Round 159: Global Test Accuracy = 0.4060
Round 160: Global Test Accuracy = 0.4060
Round 161: Global Test Accuracy = 0.4060
Round 162: Global Test Accuracy = 0.4060
Round 163: Global Test Accuracy = 0.4070
Round 164: Global Test Accuracy = 0.4080
Round 165: Global Test Accuracy = 0.4090
Round 166: Global Test Accuracy = 0.4080
Round 167: Global Test Accuracy = 0.4080
Round 168: Global Test Accuracy = 0.4100
Round 169: Global Test Accuracy = 0.4090
Round 170: Global Test Accuracy = 0.4120
Round 171: Global Test Accuracy = 0.4130
Round 172: Global Test Accuracy = 0.4130
Round 173: Global Test Accuracy = 0.4120
Round 174: Global Test Accuracy = 0.4130
Round 175: Global Test Accuracy = 0.4110
Round 176: Global Test Accuracy = 0.4090
Round 177: Global Test Accuracy = 0.4090
Round 178: Global Test Accuracy = 0.4080
Round 179: Global Test Accuracy = 0.4090
Round 180: Global Test Accuracy = 0.4080
Round 181: Global Test Accuracy = 0.4090
Round 182: Global Test Accuracy = 0.4120
Round 183: Global Test Accuracy = 0.4140
Round 184: Global Test Accuracy = 0.4120
Round 185: Global Test Accuracy = 0.4120
Round 186: Global Test Accuracy = 0.4120
Round 187: Global Test Accuracy = 0.4130
Round 188: Global Test Accuracy = 0.4130
Round 189: Global Test Accuracy = 0.4120
Round 190: Global Test Accuracy = 0.4150
Round 191: Global Test Accuracy = 0.4170
Round 192: Global Test Accuracy = 0.4160
Round 193: Global Test Accuracy = 0.4170
Round 194: Global Test Accuracy = 0.4090
Round 195: Global Test Accuracy = 0.4140
Round 196: Global Test Accuracy = 0.4150
Round 197: Global Test Accuracy = 0.4210
Round 198: Global Test Accuracy = 0.4200
Round 199: Global Test Accuracy = 0.4190
Round 200: Global Test Accuracy = 0.4230
//train_time: 4444.6320000000005 ms//end
//Log Max memory for Large1: 3347836928.0 //end
//Log Max memory for Large2: 1272160256.0 //end
//Log Max memory for Large3: 844480512.0 //end
//Log Max memory for Large4: 848662528.0 //end
//Log Max memory for Server: 2172071936.0 //end
//Log Large1 network: 11046625.0 //end
//Log Large2 network: 15015889.0 //end
//Log Large3 network: 7743766.0 //end
//Log Large4 network: 7692739.0 //end
//Log Server network: 38299014.0 //end
//Log Total Actual Train Comm Cost: 76.10 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: pubmed, Batch Size: -1, Trainers: 5, Hops: 0, IID Beta: 10.0 => Training Time = 34.45 seconds
average_final_test_loss, 1.0940564894676208
Average test accuracy, 0.423

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge 
----------------------------------------------------------------------------------------------------
0        678.1        4001     3674     0.169        0.185       
1        691.9        5385     6632     0.128        0.104       
2        674.8        3553     2818     0.190        0.239       
3        683.3        4469     5289     0.153        0.129       
4        668.6        2309     1436     0.290        0.466       
====================================================================================================
Total Memory Usage: 3396.7 MB (3.32 GB)
Total Nodes: 19717, Total Edges: 19849
Average Memory per Trainer: 679.3 MB
Average Nodes per Trainer: 3943.4
Average Edges per Trainer: 3969.8
Max Memory: 691.9 MB (Trainer 1)
Min Memory: 668.6 MB (Trainer 4)
Overall Memory/Node Ratio: 0.172 MB/node
Overall Memory/Edge Ratio: 0.171 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 61.55 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
pubmed,10.0,-1,69.6,0.42,34.5,61.5,691.9,0.172,0.031,0
================================================================================

================================================================================
EXPERIMENT SUMMARY
================================================================================
Dataset: pubmed
Method: FedAvg
Trainers: 5
IID Beta: 10.0
Batch Size: -1
Hops: 0
Total Execution Time: 69.60 seconds
Training Time: 34.46 seconds
Pretrain Comm Cost: 0.00 MB
Training Comm Cost: 61.55 MB
================================================================================

[36m(Trainer pid=4639, ip=192.168.39.47)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 4x across cluster][0m
[36m(Trainer pid=4639, ip=192.168.39.47)[0m   return torch.load(io.BytesIO(b))[32m [repeated 4x across cluster][0m
Experiment 1/1 completed for:
  Dataset: pubmed, Trainers: 5, IID Beta: 10.0
  Method: fedgcn if 0 > 0 else FedAvg, Batch Size: -1

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: ogbn-arxiv, Trainers: 5, Distribution: average, IID Beta: 10000.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'ogbn-arxiv', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 5, 'num_hops': 0, 'iid_beta': 10000.0, 'distribution_type': 'average', 'gpu': False}
ogbn-arxiv has been updated.
Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip

  0%|          | 0/81 [00:00<?, ?it/s]
Downloaded 0.00 GB:   0%|          | 0/81 [00:00<?, ?it/s]
Downloaded 0.00 GB:   1%|          | 1/81 [00:00<01:15,  1.06it/s]
Downloaded 0.00 GB:   1%|          | 1/81 [00:01<01:15,  1.06it/s]
Downloaded 0.00 GB:   2%|▏         | 2/81 [00:01<00:43,  1.82it/s]
Downloaded 0.00 GB:   2%|▏         | 2/81 [00:01<00:43,  1.82it/s]
Downloaded 0.00 GB:   4%|▎         | 3/81 [00:01<00:28,  2.77it/s]
Downloaded 0.00 GB:   4%|▎         | 3/81 [00:01<00:28,  2.77it/s]
Downloaded 0.00 GB:   5%|▍         | 4/81 [00:01<00:20,  3.69it/s]
Downloaded 0.00 GB:   5%|▍         | 4/81 [00:01<00:20,  3.69it/s]
Downloaded 0.01 GB:   5%|▍         | 4/81 [00:01<00:20,  3.69it/s]
Downloaded 0.01 GB:   7%|▋         | 6/81 [00:01<00:12,  6.09it/s]
Downloaded 0.01 GB:   7%|▋         | 6/81 [00:01<00:12,  6.09it/s]
Downloaded 0.01 GB:   7%|▋         | 6/81 [00:01<00:12,  6.09it/s]
Downloaded 0.01 GB:   7%|▋         | 6/81 [00:01<00:12,  6.09it/s]
Downloaded 0.01 GB:  11%|█         | 9/81 [00:01<00:07,  9.88it/s]
Downloaded 0.01 GB:  11%|█         | 9/81 [00:01<00:07,  9.88it/s]
Downloaded 0.01 GB:  11%|█         | 9/81 [00:01<00:07,  9.88it/s]
Downloaded 0.01 GB:  11%|█         | 9/81 [00:01<00:07,  9.88it/s]
Downloaded 0.01 GB:  11%|█         | 9/81 [00:01<00:07,  9.88it/s]
Downloaded 0.01 GB:  16%|█▌        | 13/81 [00:01<00:04, 14.88it/s]
Downloaded 0.01 GB:  16%|█▌        | 13/81 [00:01<00:04, 14.88it/s]
Downloaded 0.01 GB:  16%|█▌        | 13/81 [00:01<00:04, 14.88it/s]
Downloaded 0.02 GB:  16%|█▌        | 13/81 [00:01<00:04, 14.88it/s]
Downloaded 0.02 GB:  16%|█▌        | 13/81 [00:01<00:04, 14.88it/s]
Downloaded 0.02 GB:  16%|█▌        | 13/81 [00:02<00:04, 14.88it/s]
Downloaded 0.02 GB:  22%|██▏       | 18/81 [00:02<00:03, 20.94it/s]
Downloaded 0.02 GB:  22%|██▏       | 18/81 [00:02<00:03, 20.94it/s]
Downloaded 0.02 GB:  22%|██▏       | 18/81 [00:02<00:03, 20.94it/s]
Downloaded 0.02 GB:  22%|██▏       | 18/81 [00:02<00:03, 20.94it/s]
Downloaded 0.02 GB:  22%|██▏       | 18/81 [00:02<00:03, 20.94it/s]
Downloaded 0.02 GB:  22%|██▏       | 18/81 [00:02<00:03, 20.94it/s]
Downloaded 0.02 GB:  22%|██▏       | 18/81 [00:02<00:03, 20.94it/s]
Downloaded 0.02 GB:  30%|██▉       | 24/81 [00:02<00:02, 26.88it/s]
Downloaded 0.02 GB:  30%|██▉       | 24/81 [00:02<00:02, 26.88it/s]
Downloaded 0.03 GB:  30%|██▉       | 24/81 [00:02<00:02, 26.88it/s]
Downloaded 0.03 GB:  30%|██▉       | 24/81 [00:02<00:02, 26.88it/s]
Downloaded 0.03 GB:  30%|██▉       | 24/81 [00:02<00:02, 26.88it/s]
Downloaded 0.03 GB:  30%|██▉       | 24/81 [00:02<00:02, 26.88it/s]
Downloaded 0.03 GB:  36%|███▌      | 29/81 [00:02<00:01, 29.75it/s]
Downloaded 0.03 GB:  36%|███▌      | 29/81 [00:02<00:01, 29.75it/s]
Downloaded 0.03 GB:  36%|███▌      | 29/81 [00:02<00:01, 29.75it/s]
Downloaded 0.03 GB:  36%|███▌      | 29/81 [00:02<00:01, 29.75it/s]
Downloaded 0.03 GB:  36%|███▌      | 29/81 [00:02<00:01, 29.75it/s]
Downloaded 0.03 GB:  36%|███▌      | 29/81 [00:02<00:01, 29.75it/s]
Downloaded 0.03 GB:  42%|████▏     | 34/81 [00:02<00:01, 31.88it/s]
Downloaded 0.03 GB:  42%|████▏     | 34/81 [00:02<00:01, 31.88it/s]
Downloaded 0.04 GB:  42%|████▏     | 34/81 [00:02<00:01, 31.88it/s]
Downloaded 0.04 GB:  42%|████▏     | 34/81 [00:02<00:01, 31.88it/s]
Downloaded 0.04 GB:  42%|████▏     | 34/81 [00:02<00:01, 31.88it/s]
Downloaded 0.04 GB:  42%|████▏     | 34/81 [00:02<00:01, 31.88it/s]
Downloaded 0.04 GB:  42%|████▏     | 34/81 [00:02<00:01, 31.88it/s]
Downloaded 0.04 GB:  49%|████▉     | 40/81 [00:02<00:01, 34.87it/s]
Downloaded 0.04 GB:  49%|████▉     | 40/81 [00:02<00:01, 34.87it/s]
Downloaded 0.04 GB:  49%|████▉     | 40/81 [00:02<00:01, 34.87it/s]
Downloaded 0.04 GB:  49%|████▉     | 40/81 [00:02<00:01, 34.87it/s]
Downloaded 0.04 GB:  49%|████▉     | 40/81 [00:02<00:01, 34.87it/s]
Downloaded 0.04 GB:  49%|████▉     | 40/81 [00:02<00:01, 34.87it/s]
Downloaded 0.04 GB:  56%|█████▌    | 45/81 [00:02<00:01, 35.50it/s]
Downloaded 0.04 GB:  56%|█████▌    | 45/81 [00:02<00:01, 35.50it/s]
Downloaded 0.05 GB:  56%|█████▌    | 45/81 [00:02<00:01, 35.50it/s]
Downloaded 0.05 GB:  56%|█████▌    | 45/81 [00:02<00:01, 35.50it/s]
Downloaded 0.05 GB:  56%|█████▌    | 45/81 [00:02<00:01, 35.50it/s]
Downloaded 0.05 GB:  56%|█████▌    | 45/81 [00:02<00:01, 35.50it/s]
Downloaded 0.05 GB:  62%|██████▏   | 50/81 [00:02<00:00, 35.99it/s]
Downloaded 0.05 GB:  62%|██████▏   | 50/81 [00:02<00:00, 35.99it/s]
Downloaded 0.05 GB:  62%|██████▏   | 50/81 [00:02<00:00, 35.99it/s]
Downloaded 0.05 GB:  62%|██████▏   | 50/81 [00:02<00:00, 35.99it/s]
Downloaded 0.05 GB:  62%|██████▏   | 50/81 [00:02<00:00, 35.99it/s]
Downloaded 0.05 GB:  62%|██████▏   | 50/81 [00:02<00:00, 35.99it/s]
Downloaded 0.05 GB:  68%|██████▊   | 55/81 [00:02<00:00, 36.39it/s]
Downloaded 0.05 GB:  68%|██████▊   | 55/81 [00:02<00:00, 36.39it/s]
Downloaded 0.06 GB:  68%|██████▊   | 55/81 [00:03<00:00, 36.39it/s]
Downloaded 0.06 GB:  68%|██████▊   | 55/81 [00:03<00:00, 36.39it/s]
Downloaded 0.06 GB:  68%|██████▊   | 55/81 [00:03<00:00, 36.39it/s]
Downloaded 0.06 GB:  68%|██████▊   | 55/81 [00:03<00:00, 36.39it/s]
Downloaded 0.06 GB:  74%|███████▍  | 60/81 [00:03<00:00, 36.64it/s]
Downloaded 0.06 GB:  74%|███████▍  | 60/81 [00:03<00:00, 36.64it/s]
Downloaded 0.06 GB:  74%|███████▍  | 60/81 [00:03<00:00, 36.64it/s]
Downloaded 0.06 GB:  74%|███████▍  | 60/81 [00:03<00:00, 36.64it/s]
Downloaded 0.06 GB:  74%|███████▍  | 60/81 [00:03<00:00, 36.64it/s]
Downloaded 0.06 GB:  74%|███████▍  | 60/81 [00:03<00:00, 36.64it/s]
Downloaded 0.06 GB:  80%|████████  | 65/81 [00:03<00:00, 36.79it/s]
Downloaded 0.06 GB:  80%|████████  | 65/81 [00:03<00:00, 36.79it/s]
Downloaded 0.07 GB:  80%|████████  | 65/81 [00:03<00:00, 36.79it/s]
Downloaded 0.07 GB:  80%|████████  | 65/81 [00:03<00:00, 36.79it/s]
Downloaded 0.07 GB:  80%|████████  | 65/81 [00:03<00:00, 36.79it/s]
Downloaded 0.07 GB:  80%|████████  | 65/81 [00:03<00:00, 36.79it/s]
Downloaded 0.07 GB:  80%|████████  | 65/81 [00:03<00:00, 36.79it/s]
Downloaded 0.07 GB:  88%|████████▊ | 71/81 [00:03<00:00, 38.35it/s]
Downloaded 0.07 GB:  88%|████████▊ | 71/81 [00:03<00:00, 38.35it/s]
Downloaded 0.07 GB:  88%|████████▊ | 71/81 [00:03<00:00, 38.35it/s]
Downloaded 0.07 GB:  88%|████████▊ | 71/81 [00:03<00:00, 38.35it/s]
Downloaded 0.07 GB:  88%|████████▊ | 71/81 [00:03<00:00, 38.35it/s]
Downloaded 0.07 GB:  88%|████████▊ | 71/81 [00:03<00:00, 38.35it/s]
Downloaded 0.07 GB:  94%|█████████▍| 76/81 [00:03<00:00, 38.04it/s]
Downloaded 0.08 GB:  94%|█████████▍| 76/81 [00:03<00:00, 38.04it/s]
Downloaded 0.08 GB:  94%|█████████▍| 76/81 [00:03<00:00, 38.04it/s]
Downloaded 0.08 GB:  94%|█████████▍| 76/81 [00:03<00:00, 38.04it/s]
Downloaded 0.08 GB:  94%|█████████▍| 76/81 [00:03<00:00, 38.04it/s]
Downloaded 0.08 GB:  94%|█████████▍| 76/81 [00:03<00:00, 38.04it/s]
Downloaded 0.08 GB: 100%|██████████| 81/81 [00:03<00:00, 22.43it/s]
Extracting dataset/arxiv.zip
Processing...
Loading necessary files...
This might take a while.
Processing graphs...

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 27594.11it/s]
Converting graphs into PyG objects...

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 7269.16it/s]
Saving...
Done!
/usr/local/lib/python3.11/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
Initialization start: network data collected.
2025-07-29 22:46:48,892	INFO worker.py:1429 -- Using address 192.168.23.53:6379 set in the environment variable RAY_ADDRESS
2025-07-29 22:46:48,892	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.23.53:6379...
2025-07-29 22:46:48,899	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.23.53:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=8739, ip=192.168.0.191)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=8739, ip=192.168.0.191)[0m   return torch.load(io.BytesIO(b))
/usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
[36m(Trainer pid=8739, ip=192.168.0.191)[0m Running GCN_arxiv
Running GCN_arxiv
//Log init_time: 5634.625 ms //end
//Log Large1 init network: 625639.0 //end
//Log Large2 init network: 109856.0 //end
//Log Large3 init network: 111371.0 //end
//Log Large4 init network: 154947.0 //end
//Log Server init network: 100517425.0 //end
//Log Initialization Communication Cost (MB): 96.82 //end
Pretrain start time recorded.
//pretrain_time: 8.468 ms//end
//Log Max memory for Large1: 3354046464.0 //end
//Log Max memory for Large2: 859676672.0 //end
//Log Max memory for Large3: 848289792.0 //end
//Log Max memory for Large4: 1288073216.0 //end
//Log Max memory for Server: 2410680320.0 //end
//Log Large1 network: 3830414.0 //end
//Log Large2 network: 744293.0 //end
//Log Large3 network: 753319.0 //end
//Log Large4 network: 1107777.0 //end
//Log Server network: 2122826.0 //end
//Log Total Actual Pretrain Comm Cost: 8.16 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.0928
Round 2: Global Test Accuracy = 0.0767
Round 3: Global Test Accuracy = 0.0620
Round 4: Global Test Accuracy = 0.0758
Round 5: Global Test Accuracy = 0.1645
Round 6: Global Test Accuracy = 0.2464
Round 7: Global Test Accuracy = 0.2611
Round 8: Global Test Accuracy = 0.2673
Round 9: Global Test Accuracy = 0.2736
Round 10: Global Test Accuracy = 0.2801
Round 11: Global Test Accuracy = 0.2880
Round 12: Global Test Accuracy = 0.2951
Round 13: Global Test Accuracy = 0.3044
Round 14: Global Test Accuracy = 0.3121
Round 15: Global Test Accuracy = 0.3183
Round 16: Global Test Accuracy = 0.3270
Round 17: Global Test Accuracy = 0.3339
Round 18: Global Test Accuracy = 0.3414
Round 19: Global Test Accuracy = 0.3520
Round 20: Global Test Accuracy = 0.3607
Round 21: Global Test Accuracy = 0.3673
Round 22: Global Test Accuracy = 0.3764
Round 23: Global Test Accuracy = 0.3864
Round 24: Global Test Accuracy = 0.3929
Round 25: Global Test Accuracy = 0.3971
Round 26: Global Test Accuracy = 0.4050
Round 27: Global Test Accuracy = 0.4121
Round 28: Global Test Accuracy = 0.4216
Round 29: Global Test Accuracy = 0.4258
Round 30: Global Test Accuracy = 0.4321
Round 31: Global Test Accuracy = 0.4364
Round 32: Global Test Accuracy = 0.4409
Round 33: Global Test Accuracy = 0.4460
Round 34: Global Test Accuracy = 0.4517
Round 35: Global Test Accuracy = 0.4579
Round 36: Global Test Accuracy = 0.4628
Round 37: Global Test Accuracy = 0.4691
Round 38: Global Test Accuracy = 0.4738
Round 39: Global Test Accuracy = 0.4779
Round 40: Global Test Accuracy = 0.4811
Round 41: Global Test Accuracy = 0.4849
Round 42: Global Test Accuracy = 0.4887
Round 43: Global Test Accuracy = 0.4897
Round 44: Global Test Accuracy = 0.4911
Round 45: Global Test Accuracy = 0.4951
Round 46: Global Test Accuracy = 0.4992
Round 47: Global Test Accuracy = 0.5013
Round 48: Global Test Accuracy = 0.5027
Round 49: Global Test Accuracy = 0.5047
Round 50: Global Test Accuracy = 0.5071
Round 51: Global Test Accuracy = 0.5090
Round 52: Global Test Accuracy = 0.5101
Round 53: Global Test Accuracy = 0.5137
Round 54: Global Test Accuracy = 0.5143
Round 55: Global Test Accuracy = 0.5151
Round 56: Global Test Accuracy = 0.5164
Round 57: Global Test Accuracy = 0.5183
Round 58: Global Test Accuracy = 0.5196
Round 59: Global Test Accuracy = 0.5206
Round 60: Global Test Accuracy = 0.5225
Round 61: Global Test Accuracy = 0.5237
Round 62: Global Test Accuracy = 0.5230
Round 63: Global Test Accuracy = 0.5246
Round 64: Global Test Accuracy = 0.5261
Round 65: Global Test Accuracy = 0.5258
Round 66: Global Test Accuracy = 0.5267
Round 67: Global Test Accuracy = 0.5287
Round 68: Global Test Accuracy = 0.5295
Round 69: Global Test Accuracy = 0.5307
Round 70: Global Test Accuracy = 0.5312
Round 71: Global Test Accuracy = 0.5332
Round 72: Global Test Accuracy = 0.5345
Round 73: Global Test Accuracy = 0.5352
Round 74: Global Test Accuracy = 0.5365
Round 75: Global Test Accuracy = 0.5373
Round 76: Global Test Accuracy = 0.5375
Round 77: Global Test Accuracy = 0.5386
Round 78: Global Test Accuracy = 0.5386
Round 79: Global Test Accuracy = 0.5412
Round 80: Global Test Accuracy = 0.5412
Round 81: Global Test Accuracy = 0.5425
Round 82: Global Test Accuracy = 0.5398
Round 83: Global Test Accuracy = 0.5415
Round 84: Global Test Accuracy = 0.5419
Round 85: Global Test Accuracy = 0.5435
Round 86: Global Test Accuracy = 0.5432
Round 87: Global Test Accuracy = 0.5453
Round 88: Global Test Accuracy = 0.5449
Round 89: Global Test Accuracy = 0.5464
Round 90: Global Test Accuracy = 0.5461
Round 91: Global Test Accuracy = 0.5464
Round 92: Global Test Accuracy = 0.5463
Round 93: Global Test Accuracy = 0.5460
Round 94: Global Test Accuracy = 0.5476
Round 95: Global Test Accuracy = 0.5485
Round 96: Global Test Accuracy = 0.5490
Round 97: Global Test Accuracy = 0.5505
Round 98: Global Test Accuracy = 0.5508
Round 99: Global Test Accuracy = 0.5524
Round 100: Global Test Accuracy = 0.5505
Round 101: Global Test Accuracy = 0.5520
Round 102: Global Test Accuracy = 0.5534
Round 103: Global Test Accuracy = 0.5533
Round 104: Global Test Accuracy = 0.5537
Round 105: Global Test Accuracy = 0.5542
Round 106: Global Test Accuracy = 0.5541
Round 107: Global Test Accuracy = 0.5548
Round 108: Global Test Accuracy = 0.5552
Round 109: Global Test Accuracy = 0.5553
Round 110: Global Test Accuracy = 0.5553
Round 111: Global Test Accuracy = 0.5550
Round 112: Global Test Accuracy = 0.5552
Round 113: Global Test Accuracy = 0.5557
Round 114: Global Test Accuracy = 0.5547
Round 115: Global Test Accuracy = 0.5553
Round 116: Global Test Accuracy = 0.5569
Round 117: Global Test Accuracy = 0.5579
Round 118: Global Test Accuracy = 0.5574
Round 119: Global Test Accuracy = 0.5572
Round 120: Global Test Accuracy = 0.5585
Round 121: Global Test Accuracy = 0.5605
Round 122: Global Test Accuracy = 0.5605
Round 123: Global Test Accuracy = 0.5604
Round 124: Global Test Accuracy = 0.5608
Round 125: Global Test Accuracy = 0.5612
Round 126: Global Test Accuracy = 0.5614
Round 127: Global Test Accuracy = 0.5620
Round 128: Global Test Accuracy = 0.5634
Round 129: Global Test Accuracy = 0.5625
Round 130: Global Test Accuracy = 0.5634
Round 131: Global Test Accuracy = 0.5635
Round 132: Global Test Accuracy = 0.5639
Round 133: Global Test Accuracy = 0.5631
Round 134: Global Test Accuracy = 0.5635
Round 135: Global Test Accuracy = 0.5648
Round 136: Global Test Accuracy = 0.5647
Round 137: Global Test Accuracy = 0.5649
Round 138: Global Test Accuracy = 0.5645
Round 139: Global Test Accuracy = 0.5655
Round 140: Global Test Accuracy = 0.5658
Round 141: Global Test Accuracy = 0.5655
Round 142: Global Test Accuracy = 0.5653
Round 143: Global Test Accuracy = 0.5659
Round 144: Global Test Accuracy = 0.5669
Round 145: Global Test Accuracy = 0.5667
Round 146: Global Test Accuracy = 0.5675
Round 147: Global Test Accuracy = 0.5675
Round 148: Global Test Accuracy = 0.5673
Round 149: Global Test Accuracy = 0.5669
Round 150: Global Test Accuracy = 0.5673
Round 151: Global Test Accuracy = 0.5672
Round 152: Global Test Accuracy = 0.5674
Round 153: Global Test Accuracy = 0.5675
Round 154: Global Test Accuracy = 0.5680
Round 155: Global Test Accuracy = 0.5685
Round 156: Global Test Accuracy = 0.5686
Round 157: Global Test Accuracy = 0.5695
Round 158: Global Test Accuracy = 0.5691
Round 159: Global Test Accuracy = 0.5694
Round 160: Global Test Accuracy = 0.5693
Round 161: Global Test Accuracy = 0.5692
Round 162: Global Test Accuracy = 0.5700
Round 163: Global Test Accuracy = 0.5704
Round 164: Global Test Accuracy = 0.5701
Round 165: Global Test Accuracy = 0.5699
Round 166: Global Test Accuracy = 0.5706
Round 167: Global Test Accuracy = 0.5705
Round 168: Global Test Accuracy = 0.5701
Round 169: Global Test Accuracy = 0.5713
Round 170: Global Test Accuracy = 0.5714
Round 171: Global Test Accuracy = 0.5718
Round 172: Global Test Accuracy = 0.5721
Round 173: Global Test Accuracy = 0.5713
Round 174: Global Test Accuracy = 0.5705
Round 175: Global Test Accuracy = 0.5714
Round 176: Global Test Accuracy = 0.5722
Round 177: Global Test Accuracy = 0.5720
Round 178: Global Test Accuracy = 0.5720
Round 179: Global Test Accuracy = 0.5727
Round 180: Global Test Accuracy = 0.5742
Round 181: Global Test Accuracy = 0.5728
Round 182: Global Test Accuracy = 0.5727
Round 183: Global Test Accuracy = 0.5734
Round 184: Global Test Accuracy = 0.5739
Round 185: Global Test Accuracy = 0.5742
Round 186: Global Test Accuracy = 0.5739
Round 187: Global Test Accuracy = 0.5741
Round 188: Global Test Accuracy = 0.5740
Round 189: Global Test Accuracy = 0.5745
Round 190: Global Test Accuracy = 0.5749
Round 191: Global Test Accuracy = 0.5754
Round 192: Global Test Accuracy = 0.5748
Round 193: Global Test Accuracy = 0.5754
Round 194: Global Test Accuracy = 0.5757
Round 195: Global Test Accuracy = 0.5754
Round 196: Global Test Accuracy = 0.5766
Round 197: Global Test Accuracy = 0.5762
Round 198: Global Test Accuracy = 0.5755
Round 199: Global Test Accuracy = 0.5758
Round 200: Global Test Accuracy = 0.5754
//train_time: 151113.135 ms//end
//Log Max memory for Large1: 3672326144.0 //end
//Log Max memory for Large2: 1274576896.0 //end
//Log Max memory for Large3: 1175056384.0 //end
//Log Max memory for Large4: 1789669376.0 //end
//Log Max memory for Server: 2409504768.0 //end
//Log Large1 network: 57938895.0 //end
//Log Large2 network: 40236861.0 //end
//Log Large3 network: 40232425.0 //end
//Log Large4 network: 77472405.0 //end
//Log Server network: 193418619.0 //end
//Log Total Actual Train Comm Cost: 390.34 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: ogbn-arxiv, Batch Size: -1, Trainers: 5, Hops: 0, IID Beta: 10000.0 => Training Time = 181.11 seconds
average_final_test_loss, 1.5937259240084032
Average test accuracy, 0.5754377301812645

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge 
----------------------------------------------------------------------------------------------------
0        729.2        33899    100478   0.022        0.007       
1        920.1        33832    91380    0.027        0.010       
2        728.9        33886    95274    0.022        0.008       
3        820.2        33816    84970    0.024        0.010       
4        727.8        33910    92518    0.021        0.008       
====================================================================================================
Total Memory Usage: 3926.0 MB (3.83 GB)
Total Nodes: 169343, Total Edges: 464620
Average Memory per Trainer: 785.2 MB
Average Nodes per Trainer: 33868.6
Average Edges per Trainer: 92924.0
Max Memory: 920.1 MB (Trainer 1)
Min Memory: 727.8 MB (Trainer 4)
Overall Memory/Node Ratio: 0.023 MB/node
Overall Memory/Edge Ratio: 0.008 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 334.29 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
ogbn-arxiv,10000.0,-1,217.0,0.58,181.3,334.3,920.1,0.907,0.167,0
================================================================================

================================================================================
EXPERIMENT SUMMARY
================================================================================
Dataset: ogbn-arxiv
Method: FedAvg
Trainers: 5
IID Beta: 10000.0
Batch Size: -1
Hops: 0
Total Execution Time: 216.95 seconds
Training Time: 181.32 seconds
Pretrain Comm Cost: 0.00 MB
Training Comm Cost: 334.29 MB
================================================================================

[36m(Trainer pid=5072, ip=192.168.39.47)[0m Running GCN_arxiv[32m [repeated 4x across cluster][0m
[36m(Trainer pid=5090, ip=192.168.52.140)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 4x across cluster][0m
[36m(Trainer pid=5090, ip=192.168.52.140)[0m   return torch.load(io.BytesIO(b))[32m [repeated 4x across cluster][0m
Experiment 1/1 completed for:
  Dataset: ogbn-arxiv, Trainers: 5, IID Beta: 10000.0
  Method: fedgcn if 0 > 0 else FedAvg, Batch Size: -1

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: ogbn-arxiv, Trainers: 5, Distribution: average, IID Beta: 100.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'ogbn-arxiv', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 5, 'num_hops': 0, 'iid_beta': 100.0, 'distribution_type': 'average', 'gpu': False}
/usr/local/lib/python3.11/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):
/usr/local/lib/python3.11/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):
/usr/local/lib/python3.11/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
Initialization start: network data collected.
2025-07-29 22:50:31,914	INFO worker.py:1429 -- Using address 192.168.23.53:6379 set in the environment variable RAY_ADDRESS
2025-07-29 22:50:31,914	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.23.53:6379...
2025-07-29 22:50:31,920	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.23.53:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=6194, ip=192.168.2.152)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=6194, ip=192.168.2.152)[0m   return torch.load(io.BytesIO(b))
/usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
[36m(Trainer pid=9765, ip=192.168.0.191)[0m Running GCN_arxiv
Running GCN_arxiv
//Log init_time: 5398.442 ms //end
//Log Large1 init network: 669608.0 //end
//Log Large2 init network: 112645.0 //end
//Log Large3 init network: 110875.0 //end
//Log Large4 init network: 108674.0 //end
//Log Server init network: 100520270.0 //end
//Log Initialization Communication Cost (MB): 96.82 //end
Pretrain start time recorded.
//pretrain_time: 6.87 ms//end
//Log Max memory for Large1: 3833036800.0 //end
//Log Max memory for Large2: 840318976.0 //end
//Log Max memory for Large3: 826425344.0 //end
//Log Max memory for Large4: 827265024.0 //end
//Log Max memory for Server: 2430103552.0 //end
//Log Large1 network: 3901477.0 //end
//Log Large2 network: 735898.0 //end
//Log Large3 network: 739410.0 //end
//Log Large4 network: 736204.0 //end
//Log Server network: 2070487.0 //end
//Log Total Actual Pretrain Comm Cost: 7.80 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.0931
Round 2: Global Test Accuracy = 0.0791
Round 3: Global Test Accuracy = 0.0622
Round 4: Global Test Accuracy = 0.0726
Round 5: Global Test Accuracy = 0.1529
Round 6: Global Test Accuracy = 0.2391
Round 7: Global Test Accuracy = 0.2589
Round 8: Global Test Accuracy = 0.2670
Round 9: Global Test Accuracy = 0.2734
Round 10: Global Test Accuracy = 0.2806
Round 11: Global Test Accuracy = 0.2884
Round 12: Global Test Accuracy = 0.2967
Round 13: Global Test Accuracy = 0.3049
Round 14: Global Test Accuracy = 0.3122
Round 15: Global Test Accuracy = 0.3194
Round 16: Global Test Accuracy = 0.3297
Round 17: Global Test Accuracy = 0.3371
Round 18: Global Test Accuracy = 0.3453
Round 19: Global Test Accuracy = 0.3551
Round 20: Global Test Accuracy = 0.3613
Round 21: Global Test Accuracy = 0.3696
Round 22: Global Test Accuracy = 0.3771
Round 23: Global Test Accuracy = 0.3838
Round 24: Global Test Accuracy = 0.3886
Round 25: Global Test Accuracy = 0.3973
Round 26: Global Test Accuracy = 0.4052
Round 27: Global Test Accuracy = 0.4127
Round 28: Global Test Accuracy = 0.4181
Round 29: Global Test Accuracy = 0.4260
Round 30: Global Test Accuracy = 0.4339
Round 31: Global Test Accuracy = 0.4372
Round 32: Global Test Accuracy = 0.4437
Round 33: Global Test Accuracy = 0.4489
Round 34: Global Test Accuracy = 0.4539
Round 35: Global Test Accuracy = 0.4603
Round 36: Global Test Accuracy = 0.4653
Round 37: Global Test Accuracy = 0.4708
Round 38: Global Test Accuracy = 0.4744
Round 39: Global Test Accuracy = 0.4787
Round 40: Global Test Accuracy = 0.4818
Round 41: Global Test Accuracy = 0.4868
Round 42: Global Test Accuracy = 0.4877
Round 43: Global Test Accuracy = 0.4894
Round 44: Global Test Accuracy = 0.4931
Round 45: Global Test Accuracy = 0.4968
Round 46: Global Test Accuracy = 0.4980
Round 47: Global Test Accuracy = 0.5035
Round 48: Global Test Accuracy = 0.5041
Round 49: Global Test Accuracy = 0.5069
Round 50: Global Test Accuracy = 0.5067
Round 51: Global Test Accuracy = 0.5084
Round 52: Global Test Accuracy = 0.5089
Round 53: Global Test Accuracy = 0.5111
Round 54: Global Test Accuracy = 0.5133
Round 55: Global Test Accuracy = 0.5138
Round 56: Global Test Accuracy = 0.5166
Round 57: Global Test Accuracy = 0.5175
Round 58: Global Test Accuracy = 0.5206
Round 59: Global Test Accuracy = 0.5230
Round 60: Global Test Accuracy = 0.5248
Round 61: Global Test Accuracy = 0.5264
Round 62: Global Test Accuracy = 0.5273
Round 63: Global Test Accuracy = 0.5281
Round 64: Global Test Accuracy = 0.5280
Round 65: Global Test Accuracy = 0.5294
Round 66: Global Test Accuracy = 0.5311
Round 67: Global Test Accuracy = 0.5318
Round 68: Global Test Accuracy = 0.5329
Round 69: Global Test Accuracy = 0.5337
Round 70: Global Test Accuracy = 0.5356
Round 71: Global Test Accuracy = 0.5346
Round 72: Global Test Accuracy = 0.5348
Round 73: Global Test Accuracy = 0.5368
Round 74: Global Test Accuracy = 0.5371
Round 75: Global Test Accuracy = 0.5384
Round 76: Global Test Accuracy = 0.5384
Round 77: Global Test Accuracy = 0.5403
Round 78: Global Test Accuracy = 0.5398
Round 79: Global Test Accuracy = 0.5398
Round 80: Global Test Accuracy = 0.5408
Round 81: Global Test Accuracy = 0.5421
Round 82: Global Test Accuracy = 0.5416
Round 83: Global Test Accuracy = 0.5431
Round 84: Global Test Accuracy = 0.5444
Round 85: Global Test Accuracy = 0.5445
Round 86: Global Test Accuracy = 0.5447
Round 87: Global Test Accuracy = 0.5453
Round 88: Global Test Accuracy = 0.5457
Round 89: Global Test Accuracy = 0.5471
Round 90: Global Test Accuracy = 0.5487
Round 91: Global Test Accuracy = 0.5489
Round 92: Global Test Accuracy = 0.5483
Round 93: Global Test Accuracy = 0.5481
Round 94: Global Test Accuracy = 0.5494
Round 95: Global Test Accuracy = 0.5504
Round 96: Global Test Accuracy = 0.5511
Round 97: Global Test Accuracy = 0.5509
Round 98: Global Test Accuracy = 0.5503
Round 99: Global Test Accuracy = 0.5509
Round 100: Global Test Accuracy = 0.5509
Round 101: Global Test Accuracy = 0.5517
Round 102: Global Test Accuracy = 0.5524
Round 103: Global Test Accuracy = 0.5531
Round 104: Global Test Accuracy = 0.5543
Round 105: Global Test Accuracy = 0.5543
Round 106: Global Test Accuracy = 0.5539
Round 107: Global Test Accuracy = 0.5554
Round 108: Global Test Accuracy = 0.5562
Round 109: Global Test Accuracy = 0.5566
Round 110: Global Test Accuracy = 0.5565
Round 111: Global Test Accuracy = 0.5559
Round 112: Global Test Accuracy = 0.5562
Round 113: Global Test Accuracy = 0.5564
Round 114: Global Test Accuracy = 0.5565
Round 115: Global Test Accuracy = 0.5574
Round 116: Global Test Accuracy = 0.5578
Round 117: Global Test Accuracy = 0.5583
Round 118: Global Test Accuracy = 0.5584
Round 119: Global Test Accuracy = 0.5587
Round 120: Global Test Accuracy = 0.5590
Round 121: Global Test Accuracy = 0.5602
Round 122: Global Test Accuracy = 0.5602
Round 123: Global Test Accuracy = 0.5606
Round 124: Global Test Accuracy = 0.5600
Round 125: Global Test Accuracy = 0.5600
Round 126: Global Test Accuracy = 0.5605
Round 127: Global Test Accuracy = 0.5615
Round 128: Global Test Accuracy = 0.5609
Round 129: Global Test Accuracy = 0.5611
Round 130: Global Test Accuracy = 0.5605
Round 131: Global Test Accuracy = 0.5615
Round 132: Global Test Accuracy = 0.5606
Round 133: Global Test Accuracy = 0.5609
Round 134: Global Test Accuracy = 0.5623
Round 135: Global Test Accuracy = 0.5625
Round 136: Global Test Accuracy = 0.5640
Round 137: Global Test Accuracy = 0.5643
Round 138: Global Test Accuracy = 0.5648
Round 139: Global Test Accuracy = 0.5641
Round 140: Global Test Accuracy = 0.5643
Round 141: Global Test Accuracy = 0.5642
Round 142: Global Test Accuracy = 0.5650
Round 143: Global Test Accuracy = 0.5650
Round 144: Global Test Accuracy = 0.5649
Round 145: Global Test Accuracy = 0.5646
Round 146: Global Test Accuracy = 0.5642
Round 147: Global Test Accuracy = 0.5640
Round 148: Global Test Accuracy = 0.5646
Round 149: Global Test Accuracy = 0.5647
Round 150: Global Test Accuracy = 0.5650
Round 151: Global Test Accuracy = 0.5658
Round 152: Global Test Accuracy = 0.5665
Round 153: Global Test Accuracy = 0.5658
Round 154: Global Test Accuracy = 0.5657
Round 155: Global Test Accuracy = 0.5667
Round 156: Global Test Accuracy = 0.5663
Round 157: Global Test Accuracy = 0.5668
Round 158: Global Test Accuracy = 0.5671
Round 159: Global Test Accuracy = 0.5678
Round 160: Global Test Accuracy = 0.5685
Round 161: Global Test Accuracy = 0.5688
Round 162: Global Test Accuracy = 0.5689
Round 163: Global Test Accuracy = 0.5702
Round 164: Global Test Accuracy = 0.5690
Round 165: Global Test Accuracy = 0.5693
Round 166: Global Test Accuracy = 0.5691
Round 167: Global Test Accuracy = 0.5693
Round 168: Global Test Accuracy = 0.5695
Round 169: Global Test Accuracy = 0.5695
Round 170: Global Test Accuracy = 0.5691
Round 171: Global Test Accuracy = 0.5682
Round 172: Global Test Accuracy = 0.5682
Round 173: Global Test Accuracy = 0.5694
Round 174: Global Test Accuracy = 0.5696
Round 175: Global Test Accuracy = 0.5691
Round 176: Global Test Accuracy = 0.5691
Round 177: Global Test Accuracy = 0.5700
Round 178: Global Test Accuracy = 0.5706
Round 179: Global Test Accuracy = 0.5704
Round 180: Global Test Accuracy = 0.5708
Round 181: Global Test Accuracy = 0.5707
Round 182: Global Test Accuracy = 0.5707
Round 183: Global Test Accuracy = 0.5705
Round 184: Global Test Accuracy = 0.5710
Round 185: Global Test Accuracy = 0.5708
Round 186: Global Test Accuracy = 0.5713
Round 187: Global Test Accuracy = 0.5715
Round 188: Global Test Accuracy = 0.5718
Round 189: Global Test Accuracy = 0.5719
Round 190: Global Test Accuracy = 0.5735
Round 191: Global Test Accuracy = 0.5741
Round 192: Global Test Accuracy = 0.5742
Round 193: Global Test Accuracy = 0.5745
Round 194: Global Test Accuracy = 0.5737
Round 195: Global Test Accuracy = 0.5739
Round 196: Global Test Accuracy = 0.5740
Round 197: Global Test Accuracy = 0.5746
Round 198: Global Test Accuracy = 0.5751
Round 199: Global Test Accuracy = 0.5757
Round 200: Global Test Accuracy = 0.5763
//train_time: 151412.418 ms//end
//Log Max memory for Large1: 4360847360.0 //end
//Log Max memory for Large2: 1190977536.0 //end
//Log Max memory for Large3: 1221263360.0 //end
//Log Max memory for Large4: 1100623872.0 //end
//Log Max memory for Server: 2428669952.0 //end
//Log Large1 network: 95282567.0 //end
//Log Large2 network: 40144211.0 //end
//Log Large3 network: 40185958.0 //end
//Log Large4 network: 40137260.0 //end
//Log Server network: 193073618.0 //end
//Log Total Actual Train Comm Cost: 389.88 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: ogbn-arxiv, Batch Size: -1, Trainers: 5, Hops: 0, IID Beta: 100.0 => Training Time = 181.41 seconds
average_final_test_loss, 1.5991899688418059
Average test accuracy, 0.5762812995082608

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge 
----------------------------------------------------------------------------------------------------
0        889.4        34376    108320   0.026        0.008       
1        760.1        33379    80268    0.023        0.009       
2        786.0        34123    93184    0.023        0.008       
3        742.9        33359    86320    0.022        0.009       
4        866.6        34106    98120    0.025        0.009       
====================================================================================================
Total Memory Usage: 4044.9 MB (3.95 GB)
Total Nodes: 169343, Total Edges: 466212
Average Memory per Trainer: 809.0 MB
Average Nodes per Trainer: 33868.6
Average Edges per Trainer: 93242.4
Max Memory: 889.4 MB (Trainer 0)
Min Memory: 742.9 MB (Trainer 3)
Overall Memory/Node Ratio: 0.024 MB/node
Overall Memory/Edge Ratio: 0.009 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 334.29 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
ogbn-arxiv,100.0,-1,217.0,0.58,181.6,334.3,889.4,0.908,0.167,0
================================================================================

================================================================================
EXPERIMENT SUMMARY
================================================================================
Dataset: ogbn-arxiv
Method: FedAvg
Trainers: 5
IID Beta: 100.0
Batch Size: -1
Hops: 0
Total Execution Time: 216.99 seconds
Training Time: 181.59 seconds
Pretrain Comm Cost: 0.00 MB
Training Comm Cost: 334.29 MB
================================================================================

[36m(Trainer pid=6194, ip=192.168.2.152)[0m Running GCN_arxiv[32m [repeated 4x across cluster][0m
[36m(Trainer pid=6192, ip=192.168.52.140)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 4x across cluster][0m
[36m(Trainer pid=6192, ip=192.168.52.140)[0m   return torch.load(io.BytesIO(b))[32m [repeated 4x across cluster][0m
Experiment 1/1 completed for:
  Dataset: ogbn-arxiv, Trainers: 5, IID Beta: 100.0
  Method: fedgcn if 0 > 0 else FedAvg, Batch Size: -1

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: ogbn-arxiv, Trainers: 5, Distribution: average, IID Beta: 10.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'ogbn-arxiv', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 5, 'num_hops': 0, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
/usr/local/lib/python3.11/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):
/usr/local/lib/python3.11/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):
/usr/local/lib/python3.11/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
Initialization start: network data collected.
2025-07-29 22:54:15,034	INFO worker.py:1429 -- Using address 192.168.23.53:6379 set in the environment variable RAY_ADDRESS
2025-07-29 22:54:15,034	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.23.53:6379...
2025-07-29 22:54:15,041	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.23.53:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=7211, ip=192.168.2.152)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=7211, ip=192.168.2.152)[0m   return torch.load(io.BytesIO(b))
/usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
[36m(Trainer pid=7211, ip=192.168.2.152)[0m Running GCN_arxiv
Running GCN_arxiv
//Log init_time: 5365.76 ms //end
//Log Large1 init network: 627487.0 //end
//Log Large2 init network: 111841.0 //end
//Log Large3 init network: 133318.0 //end
//Log Large4 init network: 122754.0 //end
//Log Server init network: 100871219.0 //end
//Log Initialization Communication Cost (MB): 97.15 //end
Pretrain start time recorded.
//pretrain_time: 7.204000000000001 ms//end
//Log Max memory for Large1: 3389403136.0 //end
//Log Max memory for Large2: 840232960.0 //end
//Log Max memory for Large3: 1286193152.0 //end
//Log Max memory for Large4: 835510272.0 //end
//Log Max memory for Server: 2445885440.0 //end
//Log Large1 network: 3546968.0 //end
//Log Large2 network: 735537.0 //end
//Log Large3 network: 1100039.0 //end
//Log Large4 network: 727892.0 //end
//Log Server network: 2077204.0 //end
//Log Total Actual Pretrain Comm Cost: 7.81 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.0934
Round 2: Global Test Accuracy = 0.0780
Round 3: Global Test Accuracy = 0.0618
Round 4: Global Test Accuracy = 0.0727
Round 5: Global Test Accuracy = 0.1550
Round 6: Global Test Accuracy = 0.2460
Round 7: Global Test Accuracy = 0.2614
Round 8: Global Test Accuracy = 0.2682
Round 9: Global Test Accuracy = 0.2741
Round 10: Global Test Accuracy = 0.2809
Round 11: Global Test Accuracy = 0.2884
Round 12: Global Test Accuracy = 0.2962
Round 13: Global Test Accuracy = 0.3035
Round 14: Global Test Accuracy = 0.3118
Round 15: Global Test Accuracy = 0.3198
Round 16: Global Test Accuracy = 0.3290
Round 17: Global Test Accuracy = 0.3337
Round 18: Global Test Accuracy = 0.3407
Round 19: Global Test Accuracy = 0.3501
Round 20: Global Test Accuracy = 0.3565
Round 21: Global Test Accuracy = 0.3653
Round 22: Global Test Accuracy = 0.3737
Round 23: Global Test Accuracy = 0.3814
Round 24: Global Test Accuracy = 0.3872
Round 25: Global Test Accuracy = 0.3983
Round 26: Global Test Accuracy = 0.4062
Round 27: Global Test Accuracy = 0.4138
Round 28: Global Test Accuracy = 0.4196
Round 29: Global Test Accuracy = 0.4266
Round 30: Global Test Accuracy = 0.4343
Round 31: Global Test Accuracy = 0.4388
Round 32: Global Test Accuracy = 0.4405
Round 33: Global Test Accuracy = 0.4469
Round 34: Global Test Accuracy = 0.4502
Round 35: Global Test Accuracy = 0.4594
Round 36: Global Test Accuracy = 0.4643
Round 37: Global Test Accuracy = 0.4680
Round 38: Global Test Accuracy = 0.4698
Round 39: Global Test Accuracy = 0.4762
Round 40: Global Test Accuracy = 0.4788
Round 41: Global Test Accuracy = 0.4810
Round 42: Global Test Accuracy = 0.4894
Round 43: Global Test Accuracy = 0.4930
Round 44: Global Test Accuracy = 0.4944
Round 45: Global Test Accuracy = 0.4981
Round 46: Global Test Accuracy = 0.4993
Round 47: Global Test Accuracy = 0.5020
Round 48: Global Test Accuracy = 0.5033
Round 49: Global Test Accuracy = 0.5046
Round 50: Global Test Accuracy = 0.5078
Round 51: Global Test Accuracy = 0.5081
Round 52: Global Test Accuracy = 0.5116
Round 53: Global Test Accuracy = 0.5150
Round 54: Global Test Accuracy = 0.5177
Round 55: Global Test Accuracy = 0.5194
Round 56: Global Test Accuracy = 0.5217
Round 57: Global Test Accuracy = 0.5229
Round 58: Global Test Accuracy = 0.5231
Round 59: Global Test Accuracy = 0.5248
Round 60: Global Test Accuracy = 0.5267
Round 61: Global Test Accuracy = 0.5249
Round 62: Global Test Accuracy = 0.5242
Round 63: Global Test Accuracy = 0.5280
Round 64: Global Test Accuracy = 0.5306
Round 65: Global Test Accuracy = 0.5295
Round 66: Global Test Accuracy = 0.5299
Round 67: Global Test Accuracy = 0.5315
Round 68: Global Test Accuracy = 0.5336
Round 69: Global Test Accuracy = 0.5337
Round 70: Global Test Accuracy = 0.5355
Round 71: Global Test Accuracy = 0.5367
Round 72: Global Test Accuracy = 0.5372
Round 73: Global Test Accuracy = 0.5369
Round 74: Global Test Accuracy = 0.5385
Round 75: Global Test Accuracy = 0.5393
Round 76: Global Test Accuracy = 0.5397
Round 77: Global Test Accuracy = 0.5402
Round 78: Global Test Accuracy = 0.5425
Round 79: Global Test Accuracy = 0.5443
Round 80: Global Test Accuracy = 0.5431
Round 81: Global Test Accuracy = 0.5446
Round 82: Global Test Accuracy = 0.5448
Round 83: Global Test Accuracy = 0.5453
Round 84: Global Test Accuracy = 0.5456
Round 85: Global Test Accuracy = 0.5466
Round 86: Global Test Accuracy = 0.5475
Round 87: Global Test Accuracy = 0.5481
Round 88: Global Test Accuracy = 0.5473
Round 89: Global Test Accuracy = 0.5497
Round 90: Global Test Accuracy = 0.5494
Round 91: Global Test Accuracy = 0.5506
Round 92: Global Test Accuracy = 0.5510
Round 93: Global Test Accuracy = 0.5510
Round 94: Global Test Accuracy = 0.5519
Round 95: Global Test Accuracy = 0.5509
Round 96: Global Test Accuracy = 0.5521
Round 97: Global Test Accuracy = 0.5528
Round 98: Global Test Accuracy = 0.5542
Round 99: Global Test Accuracy = 0.5544
Round 100: Global Test Accuracy = 0.5551
Round 101: Global Test Accuracy = 0.5551
Round 102: Global Test Accuracy = 0.5547
Round 103: Global Test Accuracy = 0.5545
Round 104: Global Test Accuracy = 0.5543
Round 105: Global Test Accuracy = 0.5552
Round 106: Global Test Accuracy = 0.5550
Round 107: Global Test Accuracy = 0.5555
Round 108: Global Test Accuracy = 0.5563
Round 109: Global Test Accuracy = 0.5561
Round 110: Global Test Accuracy = 0.5562
Round 111: Global Test Accuracy = 0.5582
Round 112: Global Test Accuracy = 0.5595
Round 113: Global Test Accuracy = 0.5588
Round 114: Global Test Accuracy = 0.5588
Round 115: Global Test Accuracy = 0.5600
Round 116: Global Test Accuracy = 0.5601
Round 117: Global Test Accuracy = 0.5600
Round 118: Global Test Accuracy = 0.5606
Round 119: Global Test Accuracy = 0.5596
Round 120: Global Test Accuracy = 0.5604
Round 121: Global Test Accuracy = 0.5615
Round 122: Global Test Accuracy = 0.5611
Round 123: Global Test Accuracy = 0.5635
Round 124: Global Test Accuracy = 0.5626
Round 125: Global Test Accuracy = 0.5611
Round 126: Global Test Accuracy = 0.5614
Round 127: Global Test Accuracy = 0.5622
Round 128: Global Test Accuracy = 0.5633
Round 129: Global Test Accuracy = 0.5634
Round 130: Global Test Accuracy = 0.5648
Round 131: Global Test Accuracy = 0.5653
Round 132: Global Test Accuracy = 0.5664
Round 133: Global Test Accuracy = 0.5665
Round 134: Global Test Accuracy = 0.5663
Round 135: Global Test Accuracy = 0.5667
Round 136: Global Test Accuracy = 0.5660
Round 137: Global Test Accuracy = 0.5676
Round 138: Global Test Accuracy = 0.5671
Round 139: Global Test Accuracy = 0.5674
Round 140: Global Test Accuracy = 0.5675
Round 141: Global Test Accuracy = 0.5669
Round 142: Global Test Accuracy = 0.5658
Round 143: Global Test Accuracy = 0.5672
Round 144: Global Test Accuracy = 0.5676
Round 145: Global Test Accuracy = 0.5682
Round 146: Global Test Accuracy = 0.5695
Round 147: Global Test Accuracy = 0.5689
Round 148: Global Test Accuracy = 0.5698
Round 149: Global Test Accuracy = 0.5698
Round 150: Global Test Accuracy = 0.5696
Round 151: Global Test Accuracy = 0.5689
Round 152: Global Test Accuracy = 0.5694
Round 153: Global Test Accuracy = 0.5699
Round 154: Global Test Accuracy = 0.5702
Round 155: Global Test Accuracy = 0.5699
Round 156: Global Test Accuracy = 0.5698
Round 157: Global Test Accuracy = 0.5697
Round 158: Global Test Accuracy = 0.5709
Round 159: Global Test Accuracy = 0.5711
Round 160: Global Test Accuracy = 0.5712
Round 161: Global Test Accuracy = 0.5712
Round 162: Global Test Accuracy = 0.5712
Round 163: Global Test Accuracy = 0.5715
Round 164: Global Test Accuracy = 0.5726
Round 165: Global Test Accuracy = 0.5719
Round 166: Global Test Accuracy = 0.5716
Round 167: Global Test Accuracy = 0.5708
Round 168: Global Test Accuracy = 0.5717
Round 169: Global Test Accuracy = 0.5718
Round 170: Global Test Accuracy = 0.5720
Round 171: Global Test Accuracy = 0.5722
Round 172: Global Test Accuracy = 0.5728
Round 173: Global Test Accuracy = 0.5740
Round 174: Global Test Accuracy = 0.5741
Round 175: Global Test Accuracy = 0.5743
Round 176: Global Test Accuracy = 0.5751
Round 177: Global Test Accuracy = 0.5743
Round 178: Global Test Accuracy = 0.5745
Round 179: Global Test Accuracy = 0.5747
Round 180: Global Test Accuracy = 0.5749
Round 181: Global Test Accuracy = 0.5755
Round 182: Global Test Accuracy = 0.5755
Round 183: Global Test Accuracy = 0.5760
Round 184: Global Test Accuracy = 0.5751
Round 185: Global Test Accuracy = 0.5744
Round 186: Global Test Accuracy = 0.5752
Round 187: Global Test Accuracy = 0.5756
Round 188: Global Test Accuracy = 0.5760
Round 189: Global Test Accuracy = 0.5751
Round 190: Global Test Accuracy = 0.5756
Round 191: Global Test Accuracy = 0.5766
Round 192: Global Test Accuracy = 0.5768
Round 193: Global Test Accuracy = 0.5771
Round 194: Global Test Accuracy = 0.5771
Round 195: Global Test Accuracy = 0.5779
Round 196: Global Test Accuracy = 0.5778
Round 197: Global Test Accuracy = 0.5781
Round 198: Global Test Accuracy = 0.5784
Round 199: Global Test Accuracy = 0.5783
Round 200: Global Test Accuracy = 0.5785
//train_time: 151604.364 ms//end
//Log Max memory for Large1: 3619627008.0 //end
//Log Max memory for Large2: 1204895744.0 //end
//Log Max memory for Large3: 1720172544.0 //end
//Log Max memory for Large4: 1071353856.0 //end
//Log Max memory for Server: 2446409728.0 //end
//Log Large1 network: 57830304.0 //end
//Log Large2 network: 40046001.0 //end
//Log Large3 network: 77373818.0 //end
//Log Large4 network: 40022453.0 //end
//Log Server network: 192921288.0 //end
//Log Total Actual Train Comm Cost: 389.28 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: ogbn-arxiv, Batch Size: -1, Trainers: 5, Hops: 0, IID Beta: 10.0 => Training Time = 181.61 seconds
average_final_test_loss, 1.588255337252822
Average test accuracy, 0.5785445342880069

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge 
----------------------------------------------------------------------------------------------------
0        787.3        33993    97376    0.023        0.008       
1        781.6        34295    114940   0.023        0.007       
2        771.3        33899    92310    0.023        0.008       
3        766.8        34177    90762    0.022        0.008       
4        726.9        32979    89720    0.022        0.008       
====================================================================================================
Total Memory Usage: 3834.0 MB (3.74 GB)
Total Nodes: 169343, Total Edges: 485108
Average Memory per Trainer: 766.8 MB
Average Nodes per Trainer: 33868.6
Average Edges per Trainer: 97021.6
Max Memory: 787.3 MB (Trainer 0)
Min Memory: 726.9 MB (Trainer 4)
Overall Memory/Node Ratio: 0.023 MB/node
Overall Memory/Edge Ratio: 0.008 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 334.29 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
ogbn-arxiv,10.0,-1,217.2,0.58,181.8,334.3,787.3,0.909,0.167,0
================================================================================

================================================================================
EXPERIMENT SUMMARY
================================================================================
Dataset: ogbn-arxiv
Method: FedAvg
Trainers: 5
IID Beta: 10.0
Batch Size: -1
Hops: 0
Total Execution Time: 217.17 seconds
Training Time: 181.81 seconds
Pretrain Comm Cost: 0.00 MB
Training Comm Cost: 334.29 MB
================================================================================

[36m(Trainer pid=7118, ip=192.168.39.47)[0m Running GCN_arxiv[32m [repeated 4x across cluster][0m
[36m(Trainer pid=7118, ip=192.168.39.47)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 4x across cluster][0m
[36m(Trainer pid=7118, ip=192.168.39.47)[0m   return torch.load(io.BytesIO(b))[32m [repeated 4x across cluster][0m
Experiment 1/1 completed for:
  Dataset: ogbn-arxiv, Trainers: 5, IID Beta: 10.0
  Method: fedgcn if 0 > 0 else FedAvg, Batch Size: -1
Benchmark completed.

------------------------------------------
Job 'raysubmit_Pfutn9ATPerA72Wr' succeeded
------------------------------------------

