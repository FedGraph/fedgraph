2025-07-29 16:55:21,786	INFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_e3a7f2b07c78922b.zip.
2025-07-29 16:55:21,790	INFO packaging.py:575 -- Creating a file package for local module '.'.
Job submission server address: http://localhost:8265

-------------------------------------------------------
Job 'raysubmit_dUQu6vdwYrmUzi1u' submitted successfully
-------------------------------------------------------

Next steps
  Query the logs of the job:
    ray job logs raysubmit_dUQu6vdwYrmUzi1u
  Query the status of the job:
    ray job status raysubmit_dUQu6vdwYrmUzi1u
  Request the job to be stopped:
    ray job stop raysubmit_dUQu6vdwYrmUzi1u

Tailing logs until the job exits (disable with --no-wait):

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: cora, Trainers: 20, Distribution: average, IID Beta: 10.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'cora', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 20, 'num_hops': 0, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x to ./data/cora/raw/ind.cora.x...
Downloaded ./data/cora/raw/ind.cora.x
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx to ./data/cora/raw/ind.cora.tx...
Downloaded ./data/cora/raw/ind.cora.tx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx to ./data/cora/raw/ind.cora.allx...
Downloaded ./data/cora/raw/ind.cora.allx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y to ./data/cora/raw/ind.cora.y...
Downloaded ./data/cora/raw/ind.cora.y
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty to ./data/cora/raw/ind.cora.ty...
Downloaded ./data/cora/raw/ind.cora.ty
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally to ./data/cora/raw/ind.cora.ally...
Downloaded ./data/cora/raw/ind.cora.ally
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph to ./data/cora/raw/ind.cora.graph...
Downloaded ./data/cora/raw/ind.cora.graph
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index to ./data/cora/raw/ind.cora.test.index...
Downloaded ./data/cora/raw/ind.cora.test.index
Initialization start: network data collected.
2025-07-29 23:55:31,905	INFO worker.py:1429 -- Using address 192.168.23.53:6379 set in the environment variable RAY_ADDRESS
2025-07-29 23:55:31,905	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.23.53:6379...
2025-07-29 23:55:32,011	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.23.53:8265 [39m[22m
Changing method to FedAvg
/usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
[36m(Trainer pid=23546, ip=192.168.2.152)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=23546, ip=192.168.2.152)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 8399.738 ms //end
//Log Large1 init network: 913299.0 //end
//Log Large2 init network: 280708.0 //end
//Log Large3 init network: 192082.0 //end
//Log Large4 init network: 235198.0 //end
//Log Server init network: 37665180.0 //end
//Log Initialization Communication Cost (MB): 37.47 //end
Pretrain start time recorded.
//pretrain_time: 8.824 ms//end
//Log Max memory for Large1: 4599164928.0 //end
//Log Max memory for Large2: 2460123136.0 //end
//Log Max memory for Large3: 2453790720.0 //end
//Log Max memory for Large4: 2452660224.0 //end
//Log Max memory for Server: 2007973888.0 //end
//Log Large1 network: 3141375.0 //end
//Log Large2 network: 765172.0 //end
//Log Large3 network: 857192.0 //end
//Log Large4 network: 773925.0 //end
//Log Server network: 3058361.0 //end
//Log Total Actual Pretrain Comm Cost: 8.20 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1490
Round 2: Global Test Accuracy = 0.1530
Round 3: Global Test Accuracy = 0.1540
Round 4: Global Test Accuracy = 0.1560
Round 5: Global Test Accuracy = 0.1540
Round 6: Global Test Accuracy = 0.1530
Round 7: Global Test Accuracy = 0.1510
Round 8: Global Test Accuracy = 0.1500
Round 9: Global Test Accuracy = 0.1450
Round 10: Global Test Accuracy = 0.1490
Round 11: Global Test Accuracy = 0.1490
Round 12: Global Test Accuracy = 0.1490
Round 13: Global Test Accuracy = 0.1530
Round 14: Global Test Accuracy = 0.1480
Round 15: Global Test Accuracy = 0.1540
Round 16: Global Test Accuracy = 0.1550
Round 17: Global Test Accuracy = 0.1560
Round 18: Global Test Accuracy = 0.1550
Round 19: Global Test Accuracy = 0.1510
Round 20: Global Test Accuracy = 0.1550
Round 21: Global Test Accuracy = 0.1570
Round 22: Global Test Accuracy = 0.1530
Round 23: Global Test Accuracy = 0.1540
Round 24: Global Test Accuracy = 0.1560
Round 25: Global Test Accuracy = 0.1600
Round 26: Global Test Accuracy = 0.1610
Round 27: Global Test Accuracy = 0.1610
Round 28: Global Test Accuracy = 0.1640
Round 29: Global Test Accuracy = 0.1740
Round 30: Global Test Accuracy = 0.1790
Round 31: Global Test Accuracy = 0.1800
Round 32: Global Test Accuracy = 0.1790
Round 33: Global Test Accuracy = 0.1810
Round 34: Global Test Accuracy = 0.1850
Round 35: Global Test Accuracy = 0.1900
Round 36: Global Test Accuracy = 0.1920
Round 37: Global Test Accuracy = 0.1940
Round 38: Global Test Accuracy = 0.2000
Round 39: Global Test Accuracy = 0.2080
Round 40: Global Test Accuracy = 0.2130
Round 41: Global Test Accuracy = 0.2140
Round 42: Global Test Accuracy = 0.2260
Round 43: Global Test Accuracy = 0.2300
Round 44: Global Test Accuracy = 0.2340
Round 45: Global Test Accuracy = 0.2370
Round 46: Global Test Accuracy = 0.2410
Round 47: Global Test Accuracy = 0.2410
Round 48: Global Test Accuracy = 0.2460
Round 49: Global Test Accuracy = 0.2470
Round 50: Global Test Accuracy = 0.2490
Round 51: Global Test Accuracy = 0.2570
Round 52: Global Test Accuracy = 0.2560
Round 53: Global Test Accuracy = 0.2570
Round 54: Global Test Accuracy = 0.2620
Round 55: Global Test Accuracy = 0.2610
Round 56: Global Test Accuracy = 0.2620
Round 57: Global Test Accuracy = 0.2670
Round 58: Global Test Accuracy = 0.2680
Round 59: Global Test Accuracy = 0.2670
Round 60: Global Test Accuracy = 0.2770
Round 61: Global Test Accuracy = 0.2780
Round 62: Global Test Accuracy = 0.2820
Round 63: Global Test Accuracy = 0.2880
Round 64: Global Test Accuracy = 0.2840
Round 65: Global Test Accuracy = 0.2910
Round 66: Global Test Accuracy = 0.3020
Round 67: Global Test Accuracy = 0.3000
Round 68: Global Test Accuracy = 0.3030
Round 69: Global Test Accuracy = 0.3080
Round 70: Global Test Accuracy = 0.3110
Round 71: Global Test Accuracy = 0.3150
Round 72: Global Test Accuracy = 0.3250
Round 73: Global Test Accuracy = 0.3290
Round 74: Global Test Accuracy = 0.3340
Round 75: Global Test Accuracy = 0.3350
Round 76: Global Test Accuracy = 0.3370
Round 77: Global Test Accuracy = 0.3440
Round 78: Global Test Accuracy = 0.3490
Round 79: Global Test Accuracy = 0.3490
Round 80: Global Test Accuracy = 0.3590
Round 81: Global Test Accuracy = 0.3620
Round 82: Global Test Accuracy = 0.3590
Round 83: Global Test Accuracy = 0.3610
Round 84: Global Test Accuracy = 0.3660
Round 85: Global Test Accuracy = 0.3740
Round 86: Global Test Accuracy = 0.3790
Round 87: Global Test Accuracy = 0.3820
Round 88: Global Test Accuracy = 0.3830
Round 89: Global Test Accuracy = 0.3930
Round 90: Global Test Accuracy = 0.3950
Round 91: Global Test Accuracy = 0.3950
Round 92: Global Test Accuracy = 0.4030
Round 93: Global Test Accuracy = 0.4050
Round 94: Global Test Accuracy = 0.4120
Round 95: Global Test Accuracy = 0.4110
Round 96: Global Test Accuracy = 0.4090
Round 97: Global Test Accuracy = 0.4130
Round 98: Global Test Accuracy = 0.4210
Round 99: Global Test Accuracy = 0.4270
Round 100: Global Test Accuracy = 0.4320
Round 101: Global Test Accuracy = 0.4380
Round 102: Global Test Accuracy = 0.4370
Round 103: Global Test Accuracy = 0.4450
Round 104: Global Test Accuracy = 0.4540
Round 105: Global Test Accuracy = 0.4550
Round 106: Global Test Accuracy = 0.4590
Round 107: Global Test Accuracy = 0.4630
Round 108: Global Test Accuracy = 0.4650
Round 109: Global Test Accuracy = 0.4660
Round 110: Global Test Accuracy = 0.4660
Round 111: Global Test Accuracy = 0.4650
Round 112: Global Test Accuracy = 0.4700
Round 113: Global Test Accuracy = 0.4690
Round 114: Global Test Accuracy = 0.4730
Round 115: Global Test Accuracy = 0.4750
Round 116: Global Test Accuracy = 0.4740
Round 117: Global Test Accuracy = 0.4750
Round 118: Global Test Accuracy = 0.4800
Round 119: Global Test Accuracy = 0.4870
Round 120: Global Test Accuracy = 0.4860
Round 121: Global Test Accuracy = 0.4920
Round 122: Global Test Accuracy = 0.4910
Round 123: Global Test Accuracy = 0.4960
Round 124: Global Test Accuracy = 0.5000
Round 125: Global Test Accuracy = 0.5000
Round 126: Global Test Accuracy = 0.4980
Round 127: Global Test Accuracy = 0.4990
Round 128: Global Test Accuracy = 0.5070
Round 129: Global Test Accuracy = 0.5040
Round 130: Global Test Accuracy = 0.5050
Round 131: Global Test Accuracy = 0.5090
Round 132: Global Test Accuracy = 0.5130
Round 133: Global Test Accuracy = 0.5150
Round 134: Global Test Accuracy = 0.5220
Round 135: Global Test Accuracy = 0.5250
Round 136: Global Test Accuracy = 0.5280
Round 137: Global Test Accuracy = 0.5300
Round 138: Global Test Accuracy = 0.5290
Round 139: Global Test Accuracy = 0.5310
Round 140: Global Test Accuracy = 0.5320
Round 141: Global Test Accuracy = 0.5330
Round 142: Global Test Accuracy = 0.5290
Round 143: Global Test Accuracy = 0.5310
Round 144: Global Test Accuracy = 0.5280
Round 145: Global Test Accuracy = 0.5330
Round 146: Global Test Accuracy = 0.5340
Round 147: Global Test Accuracy = 0.5330
Round 148: Global Test Accuracy = 0.5320
Round 149: Global Test Accuracy = 0.5310
Round 150: Global Test Accuracy = 0.5300
Round 151: Global Test Accuracy = 0.5350
Round 152: Global Test Accuracy = 0.5380
Round 153: Global Test Accuracy = 0.5390
Round 154: Global Test Accuracy = 0.5410
Round 155: Global Test Accuracy = 0.5430
Round 156: Global Test Accuracy = 0.5450
Round 157: Global Test Accuracy = 0.5460
Round 158: Global Test Accuracy = 0.5480
Round 159: Global Test Accuracy = 0.5460
Round 160: Global Test Accuracy = 0.5520
Round 161: Global Test Accuracy = 0.5510
Round 162: Global Test Accuracy = 0.5510
Round 163: Global Test Accuracy = 0.5520
Round 164: Global Test Accuracy = 0.5510
Round 165: Global Test Accuracy = 0.5510
Round 166: Global Test Accuracy = 0.5520
Round 167: Global Test Accuracy = 0.5530
Round 168: Global Test Accuracy = 0.5530
Round 169: Global Test Accuracy = 0.5530
Round 170: Global Test Accuracy = 0.5540
Round 171: Global Test Accuracy = 0.5540
Round 172: Global Test Accuracy = 0.5530
Round 173: Global Test Accuracy = 0.5520
Round 174: Global Test Accuracy = 0.5540
Round 175: Global Test Accuracy = 0.5520
Round 176: Global Test Accuracy = 0.5570
Round 177: Global Test Accuracy = 0.5590
Round 178: Global Test Accuracy = 0.5590
Round 179: Global Test Accuracy = 0.5630
Round 180: Global Test Accuracy = 0.5610
Round 181: Global Test Accuracy = 0.5620
Round 182: Global Test Accuracy = 0.5640
Round 183: Global Test Accuracy = 0.5660
Round 184: Global Test Accuracy = 0.5640
Round 185: Global Test Accuracy = 0.5650
Round 186: Global Test Accuracy = 0.5630
Round 187: Global Test Accuracy = 0.5630
Round 188: Global Test Accuracy = 0.5660
Round 189: Global Test Accuracy = 0.5690
Round 190: Global Test Accuracy = 0.5710
Round 191: Global Test Accuracy = 0.5700
Round 192: Global Test Accuracy = 0.5710
Round 193: Global Test Accuracy = 0.5710
Round 194: Global Test Accuracy = 0.5710
Round 195: Global Test Accuracy = 0.5730
Round 196: Global Test Accuracy = 0.5760
Round 197: Global Test Accuracy = 0.5760
Round 198: Global Test Accuracy = 0.5730
Round 199: Global Test Accuracy = 0.5730
Round 200: Global Test Accuracy = 0.5760
//train_time: 7386.4890000000005 ms//end
//Log Max memory for Large1: 4650414080.0 //end
//Log Max memory for Large2: 2513485824.0 //end
//Log Max memory for Large3: 2502492160.0 //end
//Log Max memory for Large4: 2507759616.0 //end
//Log Max memory for Server: 2200281088.0 //end
//Log Large1 network: 100093146.0 //end
//Log Large2 network: 97195090.0 //end
//Log Large3 network: 97218353.0 //end
//Log Large4 network: 97191573.0 //end
//Log Server network: 389958171.0 //end
//Log Total Actual Train Comm Cost: 745.45 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: cora, Batch Size: -1, Trainers: 20, Hops: 0, IID Beta: 10.0 => Training Time = 37.39 seconds
average_final_test_loss, 1.3389262276887894
Average test accuracy, 0.576

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge 
----------------------------------------------------------------------------------------------------
0        659.2        135      16       4.883        41.199      
1        658.0        144      20       4.569        32.899      
2        661.0        127      26       5.205        25.423      
3        658.6        145      36       4.542        18.294      
4        660.3        127      18       5.199        36.682      
5        658.5        136      42       4.842        15.680      
6        659.0        150      46       4.394        14.327      
7        659.7        122      12       5.407        54.972      
8        660.0        137      32       4.817        20.624      
9        658.4        116      22       5.676        29.926      
10       661.1        137      18       4.826        36.729      
11       658.8        141      32       4.672        20.587      
12       660.0        133      14       4.962        47.142      
13       660.6        129      20       5.121        33.029      
14       659.5        137      10       4.814        65.955      
15       658.7        144      46       4.575        14.321      
16       659.1        146      34       4.514        19.385      
17       659.2        142      78       4.642        8.451       
18       658.5        128      18       5.144        36.582      
19       659.2        132      20       4.994        32.958      
====================================================================================================
Total Memory Usage: 13187.2 MB (12.88 GB)
Total Nodes: 2708, Total Edges: 560
Average Memory per Trainer: 659.4 MB
Average Nodes per Trainer: 135.4
Average Edges per Trainer: 28.0
Max Memory: 661.1 MB (Trainer 10)
Min Memory: 658.0 MB (Trainer 1)
Overall Memory/Node Ratio: 4.870 MB/node
Overall Memory/Edge Ratio: 23.549 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 703.83 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
cora,10.0,-1,75.6,0.58,37.4,703.8,661.1,0.187,0.088,0
================================================================================

================================================================================
EXPERIMENT SUMMARY
================================================================================
Dataset: cora
Method: FedAvg
Trainers: 20
IID Beta: 10.0
Batch Size: -1
Hops: 0
Total Execution Time: 75.62 seconds
Training Time: 37.41 seconds
Pretrain Comm Cost: 0.00 MB
Training Comm Cost: 703.83 MB
================================================================================

[36m(Trainer pid=23775, ip=192.168.39.47)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 19x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(Trainer pid=23775, ip=192.168.39.47)[0m   return torch.load(io.BytesIO(b))[32m [repeated 19x across cluster][0m
Experiment 1/1 completed for:
  Dataset: cora, Trainers: 20, IID Beta: 10.0
  Method: fedgcn if 0 > 0 else FedAvg, Batch Size: -1

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: citeseer, Trainers: 20, Distribution: average, IID Beta: 10.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'citeseer', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 20, 'num_hops': 0, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x to ./data/citeseer/raw/ind.citeseer.x...
Downloaded ./data/citeseer/raw/ind.citeseer.x
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx to ./data/citeseer/raw/ind.citeseer.tx...
Downloaded ./data/citeseer/raw/ind.citeseer.tx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx to ./data/citeseer/raw/ind.citeseer.allx...
Downloaded ./data/citeseer/raw/ind.citeseer.allx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y to ./data/citeseer/raw/ind.citeseer.y...
Downloaded ./data/citeseer/raw/ind.citeseer.y
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty to ./data/citeseer/raw/ind.citeseer.ty...
Downloaded ./data/citeseer/raw/ind.citeseer.ty
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally to ./data/citeseer/raw/ind.citeseer.ally...
Downloaded ./data/citeseer/raw/ind.citeseer.ally
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph to ./data/citeseer/raw/ind.citeseer.graph...
Downloaded ./data/citeseer/raw/ind.citeseer.graph
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index to ./data/citeseer/raw/ind.citeseer.test.index...
Downloaded ./data/citeseer/raw/ind.citeseer.test.index
Initialization start: network data collected.
2025-07-29 23:56:56,615	INFO worker.py:1429 -- Using address 192.168.23.53:6379 set in the environment variable RAY_ADDRESS
2025-07-29 23:56:56,615	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.23.53:6379...
2025-07-29 23:56:56,709	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.23.53:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=24287, ip=192.168.2.152)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=24287, ip=192.168.2.152)[0m   return torch.load(io.BytesIO(b))
Error running experiment: [36mray::Trainer.get_info()[39m (pid=24439, ip=192.168.2.152, actor_id=df5219ce27e39abc5469e39014000000, repr=<fedgraph.federated_methods.Trainer object at 0x7f4d561b1c50>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/tmp/ray/session_2025-07-29_22-29-58_687072_1/runtime_resources/working_dir_files/_ray_pkg_e3a7f2b07c78922b/fedgraph/trainer_class.py", line 200, in get_info
    self.train_labels.max().item(), self.test_labels.max().item()
    ^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
Configuration: {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'citeseer', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 20, 'num_hops': 0, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
[36m(Trainer pid=24439, ip=192.168.2.152)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 11x across cluster][0m
[36m(Trainer pid=24439, ip=192.168.2.152)[0m   return torch.load(io.BytesIO(b))[32m [repeated 11x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: pubmed, Trainers: 20, Distribution: average, IID Beta: 10.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'pubmed', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 20, 'num_hops': 0, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x to ./data/pubmed/raw/ind.pubmed.x...
Downloaded ./data/pubmed/raw/ind.pubmed.x
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx to ./data/pubmed/raw/ind.pubmed.tx...
Downloaded ./data/pubmed/raw/ind.pubmed.tx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx to ./data/pubmed/raw/ind.pubmed.allx...
Downloaded ./data/pubmed/raw/ind.pubmed.allx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y to ./data/pubmed/raw/ind.pubmed.y...
Downloaded ./data/pubmed/raw/ind.pubmed.y
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty to ./data/pubmed/raw/ind.pubmed.ty...
Downloaded ./data/pubmed/raw/ind.pubmed.ty
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally to ./data/pubmed/raw/ind.pubmed.ally...
Downloaded ./data/pubmed/raw/ind.pubmed.ally
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph to ./data/pubmed/raw/ind.pubmed.graph...
Downloaded ./data/pubmed/raw/ind.pubmed.graph
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index to ./data/pubmed/raw/ind.pubmed.test.index...
Downloaded ./data/pubmed/raw/ind.pubmed.test.index
Initialization start: network data collected.
2025-07-29 23:57:23,914	INFO worker.py:1429 -- Using address 192.168.23.53:6379 set in the environment variable RAY_ADDRESS
2025-07-29 23:57:23,914	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.23.53:6379...
2025-07-29 23:57:23,921	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.23.53:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=24776, ip=192.168.2.152)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=24776, ip=192.168.2.152)[0m   return torch.load(io.BytesIO(b))
Error running experiment: [36mray::Trainer.get_info()[39m (pid=28449, ip=192.168.0.191, actor_id=661a724434de51e23c3ecc8e15000000, repr=<fedgraph.federated_methods.Trainer object at 0x7fda92a67ed0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/tmp/ray/session_2025-07-29_22-29-58_687072_1/runtime_resources/working_dir_files/_ray_pkg_e3a7f2b07c78922b/fedgraph/trainer_class.py", line 200, in get_info
    self.train_labels.max().item(), self.test_labels.max().item()
    ^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
Configuration: {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'pubmed', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 20, 'num_hops': 0, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
[36m(Trainer pid=24801, ip=192.168.39.47)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 4x across cluster][0m
[36m(Trainer pid=24801, ip=192.168.39.47)[0m   return torch.load(io.BytesIO(b))[32m [repeated 4x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: ogbn-arxiv, Trainers: 20, Distribution: average, IID Beta: 10.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'ogbn-arxiv', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 20, 'num_hops': 0, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
ogbn-arxiv has been updated.
Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip

  0%|          | 0/81 [00:00<?, ?it/s]
Downloaded 0.00 GB:   0%|          | 0/81 [00:00<?, ?it/s]
Downloaded 0.00 GB:   1%|          | 1/81 [00:00<01:09,  1.16it/s]
Downloaded 0.00 GB:   1%|          | 1/81 [00:01<01:09,  1.16it/s]
Downloaded 0.00 GB:   2%|â–         | 2/81 [00:01<00:40,  1.95it/s]
Downloaded 0.00 GB:   2%|â–         | 2/81 [00:01<00:40,  1.95it/s]
Downloaded 0.00 GB:   4%|â–Ž         | 3/81 [00:01<00:28,  2.70it/s]
Downloaded 0.00 GB:   4%|â–Ž         | 3/81 [00:01<00:28,  2.70it/s]
Downloaded 0.00 GB:   5%|â–         | 4/81 [00:01<00:21,  3.59it/s]
Downloaded 0.00 GB:   5%|â–         | 4/81 [00:01<00:21,  3.59it/s]
Downloaded 0.00 GB:   6%|â–Œ         | 5/81 [00:01<00:17,  4.42it/s]
Downloaded 0.01 GB:   6%|â–Œ         | 5/81 [00:01<00:17,  4.42it/s]
Downloaded 0.01 GB:   7%|â–‹         | 6/81 [00:01<00:14,  5.14it/s]
Downloaded 0.01 GB:   7%|â–‹         | 6/81 [00:01<00:14,  5.14it/s]
Downloaded 0.01 GB:   9%|â–Š         | 7/81 [00:01<00:12,  5.76it/s]
Downloaded 0.01 GB:   9%|â–Š         | 7/81 [00:01<00:12,  5.76it/s]
Downloaded 0.01 GB:   9%|â–Š         | 7/81 [00:02<00:12,  5.76it/s]
Downloaded 0.01 GB:  11%|â–ˆ         | 9/81 [00:02<00:08,  8.11it/s]
Downloaded 0.01 GB:  11%|â–ˆ         | 9/81 [00:02<00:08,  8.11it/s]
Downloaded 0.01 GB:  11%|â–ˆ         | 9/81 [00:02<00:08,  8.11it/s]
Downloaded 0.01 GB:  14%|â–ˆâ–Ž        | 11/81 [00:02<00:07,  9.96it/s]
Downloaded 0.01 GB:  14%|â–ˆâ–Ž        | 11/81 [00:02<00:07,  9.96it/s]
Downloaded 0.01 GB:  14%|â–ˆâ–Ž        | 11/81 [00:02<00:07,  9.96it/s]
Downloaded 0.01 GB:  16%|â–ˆâ–Œ        | 13/81 [00:02<00:05, 11.48it/s]
Downloaded 0.01 GB:  16%|â–ˆâ–Œ        | 13/81 [00:02<00:05, 11.48it/s]
Downloaded 0.01 GB:  16%|â–ˆâ–Œ        | 13/81 [00:02<00:05, 11.48it/s]
Downloaded 0.02 GB:  16%|â–ˆâ–Œ        | 13/81 [00:02<00:05, 11.48it/s]
Downloaded 0.02 GB:  20%|â–ˆâ–‰        | 16/81 [00:02<00:04, 14.78it/s]
Downloaded 0.02 GB:  20%|â–ˆâ–‰        | 16/81 [00:02<00:04, 14.78it/s]
Downloaded 0.02 GB:  20%|â–ˆâ–‰        | 16/81 [00:02<00:04, 14.78it/s]
Downloaded 0.02 GB:  20%|â–ˆâ–‰        | 16/81 [00:02<00:04, 14.78it/s]
Downloaded 0.02 GB:  20%|â–ˆâ–‰        | 16/81 [00:02<00:04, 14.78it/s]
Downloaded 0.02 GB:  20%|â–ˆâ–‰        | 16/81 [00:02<00:04, 14.78it/s]
Downloaded 0.02 GB:  26%|â–ˆâ–ˆâ–Œ       | 21/81 [00:02<00:02, 21.38it/s]
Downloaded 0.02 GB:  26%|â–ˆâ–ˆâ–Œ       | 21/81 [00:02<00:02, 21.38it/s]
Downloaded 0.02 GB:  26%|â–ˆâ–ˆâ–Œ       | 21/81 [00:02<00:02, 21.38it/s]
Downloaded 0.02 GB:  26%|â–ˆâ–ˆâ–Œ       | 21/81 [00:02<00:02, 21.38it/s]
Downloaded 0.02 GB:  26%|â–ˆâ–ˆâ–Œ       | 21/81 [00:02<00:02, 21.38it/s]
Downloaded 0.03 GB:  26%|â–ˆâ–ˆâ–Œ       | 21/81 [00:02<00:02, 21.38it/s]
Downloaded 0.03 GB:  26%|â–ˆâ–ˆâ–Œ       | 21/81 [00:02<00:02, 21.38it/s]
Downloaded 0.03 GB:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 27/81 [00:02<00:01, 27.91it/s]
Downloaded 0.03 GB:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 27/81 [00:02<00:01, 27.91it/s]
Downloaded 0.03 GB:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 27/81 [00:02<00:01, 27.91it/s]
Downloaded 0.03 GB:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 27/81 [00:02<00:01, 27.91it/s]
Downloaded 0.03 GB:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 27/81 [00:02<00:01, 27.91it/s]
Downloaded 0.03 GB:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 27/81 [00:02<00:01, 27.91it/s]
Downloaded 0.03 GB:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 32/81 [00:02<00:01, 30.78it/s]
Downloaded 0.03 GB:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 32/81 [00:02<00:01, 30.78it/s]
Downloaded 0.03 GB:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 32/81 [00:02<00:01, 30.78it/s]
Downloaded 0.03 GB:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 32/81 [00:02<00:01, 30.78it/s]
Downloaded 0.04 GB:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 32/81 [00:02<00:01, 30.78it/s]
Downloaded 0.04 GB:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 32/81 [00:02<00:01, 30.78it/s]
Downloaded 0.04 GB:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 32/81 [00:02<00:01, 30.78it/s]
Downloaded 0.04 GB:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 38/81 [00:02<00:01, 34.36it/s]
Downloaded 0.04 GB:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 38/81 [00:02<00:01, 34.36it/s]
Downloaded 0.04 GB:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 38/81 [00:03<00:01, 34.36it/s]
Downloaded 0.04 GB:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 38/81 [00:03<00:01, 34.36it/s]
Downloaded 0.04 GB:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 38/81 [00:03<00:01, 34.36it/s]
Downloaded 0.04 GB:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 38/81 [00:03<00:01, 34.36it/s]
Downloaded 0.04 GB:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 43/81 [00:03<00:01, 35.37it/s]
Downloaded 0.04 GB:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 43/81 [00:03<00:01, 35.37it/s]
Downloaded 0.04 GB:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 43/81 [00:03<00:01, 35.37it/s]
Downloaded 0.04 GB:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 43/81 [00:03<00:01, 35.37it/s]
Downloaded 0.05 GB:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 43/81 [00:03<00:01, 35.37it/s]
Downloaded 0.05 GB:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 43/81 [00:03<00:01, 35.37it/s]
Downloaded 0.05 GB:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 48/81 [00:03<00:00, 36.08it/s]
Downloaded 0.05 GB:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 48/81 [00:03<00:00, 36.08it/s]
Downloaded 0.05 GB:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 48/81 [00:03<00:00, 36.08it/s]
Downloaded 0.05 GB:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 48/81 [00:03<00:00, 36.08it/s]
Downloaded 0.05 GB:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 48/81 [00:03<00:00, 36.08it/s]
Downloaded 0.05 GB:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 48/81 [00:03<00:00, 36.08it/s]
Downloaded 0.05 GB:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 53/81 [00:03<00:00, 36.59it/s]
Downloaded 0.05 GB:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 53/81 [00:03<00:00, 36.59it/s]
Downloaded 0.05 GB:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 53/81 [00:03<00:00, 36.59it/s]
Downloaded 0.05 GB:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 53/81 [00:03<00:00, 36.59it/s]
Downloaded 0.06 GB:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 53/81 [00:03<00:00, 36.59it/s]
Downloaded 0.06 GB:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 53/81 [00:03<00:00, 36.59it/s]
Downloaded 0.06 GB:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 58/81 [00:03<00:00, 36.94it/s]
Downloaded 0.06 GB:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 58/81 [00:03<00:00, 36.94it/s]
Downloaded 0.06 GB:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 58/81 [00:03<00:00, 36.94it/s]
Downloaded 0.06 GB:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 58/81 [00:03<00:00, 36.94it/s]
Downloaded 0.06 GB:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 58/81 [00:03<00:00, 36.94it/s]
Downloaded 0.06 GB:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 58/81 [00:03<00:00, 36.94it/s]
Downloaded 0.06 GB:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 63/81 [00:03<00:00, 37.16it/s]
Downloaded 0.06 GB:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 63/81 [00:03<00:00, 37.16it/s]
Downloaded 0.06 GB:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 63/81 [00:03<00:00, 37.16it/s]
Downloaded 0.06 GB:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 63/81 [00:03<00:00, 37.16it/s]
Downloaded 0.07 GB:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 63/81 [00:03<00:00, 37.16it/s]
Downloaded 0.07 GB:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 63/81 [00:03<00:00, 37.16it/s]
Downloaded 0.07 GB:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 63/81 [00:03<00:00, 37.16it/s]
Downloaded 0.07 GB:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 69/81 [00:03<00:00, 38.75it/s]
Downloaded 0.07 GB:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 69/81 [00:03<00:00, 38.75it/s]
Downloaded 0.07 GB:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 69/81 [00:03<00:00, 38.75it/s]
Downloaded 0.07 GB:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 69/81 [00:03<00:00, 38.75it/s]
Downloaded 0.07 GB:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 69/81 [00:03<00:00, 38.75it/s]
Downloaded 0.07 GB:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 69/81 [00:03<00:00, 38.75it/s]
Downloaded 0.07 GB:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 74/81 [00:03<00:00, 38.50it/s]
Downloaded 0.07 GB:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 74/81 [00:03<00:00, 38.50it/s]
Downloaded 0.07 GB:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 74/81 [00:03<00:00, 38.50it/s]
Downloaded 0.08 GB:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 74/81 [00:03<00:00, 38.50it/s]
Downloaded 0.08 GB:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 74/81 [00:03<00:00, 38.50it/s]
Downloaded 0.08 GB:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 74/81 [00:04<00:00, 38.50it/s]
Downloaded 0.08 GB:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 79/81 [00:04<00:00, 38.25it/s]
Downloaded 0.08 GB:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 79/81 [00:04<00:00, 38.25it/s]
Downloaded 0.08 GB:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 79/81 [00:04<00:00, 38.25it/s]
Downloaded 0.08 GB: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:04<00:00, 20.17it/s]
Extracting dataset/arxiv.zip
Processing...
Loading necessary files...
This might take a while.
Processing graphs...

  0%|          | 0/1 [00:00<?, ?it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 25115.59it/s]
Converting graphs into PyG objects...

  0%|          | 0/1 [00:00<?, ?it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 6413.31it/s]
Saving...
Done!
/usr/local/lib/python3.11/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
Initialization start: network data collected.
2025-07-29 23:57:55,515	INFO worker.py:1429 -- Using address 192.168.23.53:6379 set in the environment variable RAY_ADDRESS
2025-07-29 23:57:55,516	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.23.53:6379...
2025-07-29 23:57:55,611	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.23.53:8265 [39m[22m
Changing method to FedAvg
/usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
[36m(Trainer pid=25246, ip=192.168.2.152)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=25246, ip=192.168.2.152)[0m   return torch.load(io.BytesIO(b))
[36m(Trainer pid=28906, ip=192.168.0.191)[0m Running GCN_arxiv
Running GCN_arxiv
//Log init_time: 9807.817 ms //end
//Log Large1 init network: 943831.0 //end
//Log Large2 init network: 504765.0 //end
//Log Large3 init network: 564933.0 //end
//Log Large4 init network: 512797.0 //end
//Log Server init network: 101348347.0 //end
//Log Initialization Communication Cost (MB): 99.06 //end
Pretrain start time recorded.
//pretrain_time: 7.458 ms//end
//Log Max memory for Large1: 4648632320.0 //end
//Log Max memory for Large2: 2515824640.0 //end
//Log Max memory for Large3: 2520379392.0 //end
//Log Max memory for Large4: 2518913024.0 //end
//Log Max memory for Server: 2587615232.0 //end
//Log Large1 network: 3731181.0 //end
//Log Large2 network: 1016824.0 //end
//Log Large3 network: 875382.0 //end
//Log Large4 network: 941550.0 //end
//Log Server network: 4807883.0 //end
//Log Total Actual Pretrain Comm Cost: 10.85 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.0837
Round 2: Global Test Accuracy = 0.0912
Round 3: Global Test Accuracy = 0.0771
Round 4: Global Test Accuracy = 0.0821
Round 5: Global Test Accuracy = 0.1221
Round 6: Global Test Accuracy = 0.1873
Round 7: Global Test Accuracy = 0.2316
Round 8: Global Test Accuracy = 0.2520
Round 9: Global Test Accuracy = 0.2654
Round 10: Global Test Accuracy = 0.2764
Round 11: Global Test Accuracy = 0.2849
Round 12: Global Test Accuracy = 0.2940
Round 13: Global Test Accuracy = 0.3016
Round 14: Global Test Accuracy = 0.3081
Round 15: Global Test Accuracy = 0.3148
Round 16: Global Test Accuracy = 0.3227
Round 17: Global Test Accuracy = 0.3298
Round 18: Global Test Accuracy = 0.3364
Round 19: Global Test Accuracy = 0.3425
Round 20: Global Test Accuracy = 0.3478
Round 21: Global Test Accuracy = 0.3534
Round 22: Global Test Accuracy = 0.3584
Round 23: Global Test Accuracy = 0.3648
Round 24: Global Test Accuracy = 0.3700
Round 25: Global Test Accuracy = 0.3753
Round 26: Global Test Accuracy = 0.3792
Round 27: Global Test Accuracy = 0.3841
Round 28: Global Test Accuracy = 0.3888
Round 29: Global Test Accuracy = 0.3936
Round 30: Global Test Accuracy = 0.3973
Round 31: Global Test Accuracy = 0.4008
Round 32: Global Test Accuracy = 0.4049
Round 33: Global Test Accuracy = 0.4093
Round 34: Global Test Accuracy = 0.4132
Round 35: Global Test Accuracy = 0.4163
Round 36: Global Test Accuracy = 0.4206
Round 37: Global Test Accuracy = 0.4237
Round 38: Global Test Accuracy = 0.4259
Round 39: Global Test Accuracy = 0.4281
Round 40: Global Test Accuracy = 0.4305
Round 41: Global Test Accuracy = 0.4331
Round 42: Global Test Accuracy = 0.4355
Round 43: Global Test Accuracy = 0.4380
Round 44: Global Test Accuracy = 0.4406
Round 45: Global Test Accuracy = 0.4431
Round 46: Global Test Accuracy = 0.4446
Round 47: Global Test Accuracy = 0.4477
Round 48: Global Test Accuracy = 0.4502
Round 49: Global Test Accuracy = 0.4511
Round 50: Global Test Accuracy = 0.4525
Round 51: Global Test Accuracy = 0.4539
Round 52: Global Test Accuracy = 0.4550
Round 53: Global Test Accuracy = 0.4563
Round 54: Global Test Accuracy = 0.4581
Round 55: Global Test Accuracy = 0.4597
Round 56: Global Test Accuracy = 0.4615
Round 57: Global Test Accuracy = 0.4631
Round 58: Global Test Accuracy = 0.4643
Round 59: Global Test Accuracy = 0.4658
Round 60: Global Test Accuracy = 0.4670
Round 61: Global Test Accuracy = 0.4684
Round 62: Global Test Accuracy = 0.4694
Round 63: Global Test Accuracy = 0.4709
Round 64: Global Test Accuracy = 0.4714
Round 65: Global Test Accuracy = 0.4725
Round 66: Global Test Accuracy = 0.4732
Round 67: Global Test Accuracy = 0.4747
Round 68: Global Test Accuracy = 0.4759
Round 69: Global Test Accuracy = 0.4766
Round 70: Global Test Accuracy = 0.4774
Round 71: Global Test Accuracy = 0.4782
Round 72: Global Test Accuracy = 0.4792
Round 73: Global Test Accuracy = 0.4796
Round 74: Global Test Accuracy = 0.4808
Round 75: Global Test Accuracy = 0.4817
Round 76: Global Test Accuracy = 0.4824
Round 77: Global Test Accuracy = 0.4829
Round 78: Global Test Accuracy = 0.4831
Round 79: Global Test Accuracy = 0.4839
Round 80: Global Test Accuracy = 0.4845
Round 81: Global Test Accuracy = 0.4852
Round 82: Global Test Accuracy = 0.4855
Round 83: Global Test Accuracy = 0.4859
Round 84: Global Test Accuracy = 0.4865
Round 85: Global Test Accuracy = 0.4872
Round 86: Global Test Accuracy = 0.4873
Round 87: Global Test Accuracy = 0.4876
Round 88: Global Test Accuracy = 0.4886
Round 89: Global Test Accuracy = 0.4897
Round 90: Global Test Accuracy = 0.4906
Round 91: Global Test Accuracy = 0.4915
Round 92: Global Test Accuracy = 0.4918
Round 93: Global Test Accuracy = 0.4920
Round 94: Global Test Accuracy = 0.4927
Round 95: Global Test Accuracy = 0.4930
Round 96: Global Test Accuracy = 0.4938
Round 97: Global Test Accuracy = 0.4938
Round 98: Global Test Accuracy = 0.4942
Round 99: Global Test Accuracy = 0.4952
Round 100: Global Test Accuracy = 0.4951
Round 101: Global Test Accuracy = 0.4958
Round 102: Global Test Accuracy = 0.4961
Round 103: Global Test Accuracy = 0.4964
Round 104: Global Test Accuracy = 0.4972
Round 105: Global Test Accuracy = 0.4969
Round 106: Global Test Accuracy = 0.4971
Round 107: Global Test Accuracy = 0.4974
Round 108: Global Test Accuracy = 0.4980
Round 109: Global Test Accuracy = 0.4981
Round 110: Global Test Accuracy = 0.4983
Round 111: Global Test Accuracy = 0.4986
Round 112: Global Test Accuracy = 0.4987
Round 113: Global Test Accuracy = 0.4994
Round 114: Global Test Accuracy = 0.4997
Round 115: Global Test Accuracy = 0.5003
Round 116: Global Test Accuracy = 0.5010
Round 117: Global Test Accuracy = 0.5006
Round 118: Global Test Accuracy = 0.5009
Round 119: Global Test Accuracy = 0.5012
Round 120: Global Test Accuracy = 0.5020
Round 121: Global Test Accuracy = 0.5022
Round 122: Global Test Accuracy = 0.5025
Round 123: Global Test Accuracy = 0.5029
Round 124: Global Test Accuracy = 0.5035
Round 125: Global Test Accuracy = 0.5036
Round 126: Global Test Accuracy = 0.5037
Round 127: Global Test Accuracy = 0.5038
Round 128: Global Test Accuracy = 0.5042
Round 129: Global Test Accuracy = 0.5047
Round 130: Global Test Accuracy = 0.5052
Round 131: Global Test Accuracy = 0.5054
Round 132: Global Test Accuracy = 0.5057
Round 133: Global Test Accuracy = 0.5063
Round 134: Global Test Accuracy = 0.5067
Round 135: Global Test Accuracy = 0.5071
Round 136: Global Test Accuracy = 0.5074
Round 137: Global Test Accuracy = 0.5072
Round 138: Global Test Accuracy = 0.5077
Round 139: Global Test Accuracy = 0.5075
Round 140: Global Test Accuracy = 0.5079
Round 141: Global Test Accuracy = 0.5073
Round 142: Global Test Accuracy = 0.5082
Round 143: Global Test Accuracy = 0.5083
Round 144: Global Test Accuracy = 0.5089
Round 145: Global Test Accuracy = 0.5090
Round 146: Global Test Accuracy = 0.5094
Round 147: Global Test Accuracy = 0.5096
Round 148: Global Test Accuracy = 0.5097
Round 149: Global Test Accuracy = 0.5101
Round 150: Global Test Accuracy = 0.5103
Round 151: Global Test Accuracy = 0.5104
Round 152: Global Test Accuracy = 0.5105
Round 153: Global Test Accuracy = 0.5108
Round 154: Global Test Accuracy = 0.5112
Round 155: Global Test Accuracy = 0.5112
Round 156: Global Test Accuracy = 0.5113
Round 157: Global Test Accuracy = 0.5110
Round 158: Global Test Accuracy = 0.5110
Round 159: Global Test Accuracy = 0.5111
Round 160: Global Test Accuracy = 0.5110
Round 161: Global Test Accuracy = 0.5117
Round 162: Global Test Accuracy = 0.5118
Round 163: Global Test Accuracy = 0.5120
Round 164: Global Test Accuracy = 0.5123
Round 165: Global Test Accuracy = 0.5125
Round 166: Global Test Accuracy = 0.5130
Round 167: Global Test Accuracy = 0.5132
Round 168: Global Test Accuracy = 0.5136
Round 169: Global Test Accuracy = 0.5139
Round 170: Global Test Accuracy = 0.5142
Round 171: Global Test Accuracy = 0.5145
Round 172: Global Test Accuracy = 0.5146
Round 173: Global Test Accuracy = 0.5151
Round 174: Global Test Accuracy = 0.5152
Round 175: Global Test Accuracy = 0.5154
Round 176: Global Test Accuracy = 0.5156
Round 177: Global Test Accuracy = 0.5160
Round 178: Global Test Accuracy = 0.5161
Round 179: Global Test Accuracy = 0.5159
Round 180: Global Test Accuracy = 0.5162
Round 181: Global Test Accuracy = 0.5163
Round 182: Global Test Accuracy = 0.5163
Round 183: Global Test Accuracy = 0.5167
Round 184: Global Test Accuracy = 0.5166
Round 185: Global Test Accuracy = 0.5170
Round 186: Global Test Accuracy = 0.5170
Round 187: Global Test Accuracy = 0.5173
Round 188: Global Test Accuracy = 0.5176
Round 189: Global Test Accuracy = 0.5176
Round 190: Global Test Accuracy = 0.5177
Round 191: Global Test Accuracy = 0.5178
Round 192: Global Test Accuracy = 0.5177
Round 193: Global Test Accuracy = 0.5181
Round 194: Global Test Accuracy = 0.5183
Round 195: Global Test Accuracy = 0.5186
Round 196: Global Test Accuracy = 0.5184
Round 197: Global Test Accuracy = 0.5185
Round 198: Global Test Accuracy = 0.5187
Round 199: Global Test Accuracy = 0.5188
Round 200: Global Test Accuracy = 0.5191
//train_time: 30392.7 ms//end
//Log Max memory for Large1: 5267816448.0 //end
//Log Max memory for Large2: 3218550784.0 //end
//Log Max memory for Large3: 3074699264.0 //end
//Log Max memory for Large4: 3257905152.0 //end
//Log Max memory for Server: 2589384704.0 //end
//Log Large1 network: 190454969.0 //end
//Log Large2 network: 185473213.0 //end
//Log Large3 network: 185355947.0 //end
//Log Large4 network: 185403985.0 //end
//Log Server network: 739700858.0 //end
//Log Total Actual Train Comm Cost: 1417.53 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: ogbn-arxiv, Batch Size: -1, Trainers: 20, Hops: 0, IID Beta: 10.0 => Training Time = 60.39 seconds
average_final_test_loss, 1.7499664361061458
Average test accuracy, 0.5191449087504887

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge 
----------------------------------------------------------------------------------------------------
0        778.7        8330     5942     0.093        0.131       
1        771.7        8565     5680     0.090        0.136       
2        761.9        8601     8076     0.089        0.094       
3        753.6        7928     5234     0.095        0.144       
4        834.3        8502     6380     0.098        0.131       
5        768.7        8646     6922     0.089        0.111       
6        736.9        8595     6798     0.086        0.108       
7        721.7        7841     7030     0.092        0.103       
8        816.8        8584     5918     0.095        0.138       
9        746.9        8477     5532     0.088        0.135       
10       737.2        8620     7622     0.086        0.097       
11       847.5        8522     6028     0.099        0.141       
12       755.8        8563     5986     0.088        0.126       
13       854.7        8529     6320     0.100        0.135       
14       800.2        8720     5738     0.092        0.139       
15       775.7        8245     4490     0.094        0.173       
16       781.7        8497     6132     0.092        0.127       
17       766.6        8566     6046     0.089        0.127       
18       808.6        8550     5084     0.095        0.159       
19       771.0        8462     5248     0.091        0.147       
====================================================================================================
Total Memory Usage: 15590.1 MB (15.22 GB)
Total Nodes: 169343, Total Edges: 122206
Average Memory per Trainer: 779.5 MB
Average Nodes per Trainer: 8467.1
Average Edges per Trainer: 6110.3
Max Memory: 854.7 MB (Trainer 13)
Min Memory: 721.7 MB (Trainer 7)
Overall Memory/Node Ratio: 0.092 MB/node
Overall Memory/Edge Ratio: 0.128 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 1337.16 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
ogbn-arxiv,10.0,-1,100.2,0.52,60.4,1337.2,854.7,0.302,0.167,0
================================================================================

================================================================================
EXPERIMENT SUMMARY
================================================================================
Dataset: ogbn-arxiv
Method: FedAvg
Trainers: 20
IID Beta: 10.0
Batch Size: -1
Hops: 0
Total Execution Time: 100.16 seconds
Training Time: 60.44 seconds
Pretrain Comm Cost: 0.00 MB
Training Comm Cost: 1337.16 MB
================================================================================

[36m(Trainer pid=25509, ip=192.168.2.152)[0m Running GCN_arxiv[32m [repeated 19x across cluster][0m
[36m(Trainer pid=25470, ip=192.168.39.47)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 19x across cluster][0m
[36m(Trainer pid=25470, ip=192.168.39.47)[0m   return torch.load(io.BytesIO(b))[32m [repeated 19x across cluster][0m
Experiment 1/1 completed for:
  Dataset: ogbn-arxiv, Trainers: 20, IID Beta: 10.0
  Method: fedgcn if 0 > 0 else FedAvg, Batch Size: -1
Benchmark completed.

------------------------------------------
Job 'raysubmit_dUQu6vdwYrmUzi1u' succeeded
------------------------------------------

