2025-05-14 22:44:58,839	INFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_65cfa3aee1605a02.zip.
2025-05-14 22:44:58,841	INFO packaging.py:575 -- Creating a file package for local module '.'.
Job submission server address: http://localhost:8265

-------------------------------------------------------
Job 'raysubmit_7teRC6vg2VPck5QL' submitted successfully
-------------------------------------------------------

Next steps
  Query the logs of the job:
    ray job logs raysubmit_7teRC6vg2VPck5QL
  Query the status of the job:
    ray job status raysubmit_7teRC6vg2VPck5QL
  Request the job to be stopped:
    ray job stop raysubmit_7teRC6vg2VPck5QL

Tailing logs until the job exits (disable with --no-wait):

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: cora, Trainers: 10, Distribution: average, IID Beta: 10000.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'cora', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 10000.0, 'distribution_type': 'average', 'gpu': False}
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x to ./data/cora/raw/ind.cora.x...
Downloaded ./data/cora/raw/ind.cora.x
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx to ./data/cora/raw/ind.cora.tx...
Downloaded ./data/cora/raw/ind.cora.tx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx to ./data/cora/raw/ind.cora.allx...
Downloaded ./data/cora/raw/ind.cora.allx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y to ./data/cora/raw/ind.cora.y...
Downloaded ./data/cora/raw/ind.cora.y
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty to ./data/cora/raw/ind.cora.ty...
Downloaded ./data/cora/raw/ind.cora.ty
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally to ./data/cora/raw/ind.cora.ally...
Downloaded ./data/cora/raw/ind.cora.ally
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph to ./data/cora/raw/ind.cora.graph...
Downloaded ./data/cora/raw/ind.cora.graph
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index to ./data/cora/raw/ind.cora.test.index...
Downloaded ./data/cora/raw/ind.cora.test.index
Initialization start: network data collected.
2025-05-15 02:45:09,936	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 02:45:09,936	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 02:45:09,945	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
Changing method to FedAvg
/usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
[36m(Trainer pid=114155, ip=192.168.14.62)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=114155, ip=192.168.14.62)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5752.085 ms //end
//Log Large1 init network: 151281.0 //end
//Log Large2 init network: 114717.0 //end
//Log Large3 init network: 117972.0 //end
//Log Large4 init network: 102608.0 //end
//Log Server init network: 27206734.0 //end
//Log Initialization Communication Cost (MB): 26.41 //end
Pretrain start time recorded.
//pretrain_time: 4.024 ms//end
//Log Max memory for Large1: 6471143424.0 //end
//Log Max memory for Large2: 5730590720.0 //end
//Log Max memory for Large3: 6214750208.0 //end
//Log Max memory for Large4: 6043893760.0 //end
//Log Max memory for Server: 17580843008.0 //end
//Log Large1 network: 686355.0 //end
//Log Large2 network: 686549.0 //end
//Log Large3 network: 728087.0 //end
//Log Large4 network: 608113.0 //end
//Log Server network: 1785847.0 //end
//Log Total Actual Pretrain Comm Cost: 4.29 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1570
Round 2: Global Test Accuracy = 0.1610
Round 3: Global Test Accuracy = 0.1590
Round 4: Global Test Accuracy = 0.1590
Round 5: Global Test Accuracy = 0.1670
Round 6: Global Test Accuracy = 0.1680
Round 7: Global Test Accuracy = 0.1680
Round 8: Global Test Accuracy = 0.1660
Round 9: Global Test Accuracy = 0.1680
Round 10: Global Test Accuracy = 0.1760
Round 11: Global Test Accuracy = 0.1850
Round 12: Global Test Accuracy = 0.1910
Round 13: Global Test Accuracy = 0.1940
Round 14: Global Test Accuracy = 0.1940
Round 15: Global Test Accuracy = 0.1960
Round 16: Global Test Accuracy = 0.1970
Round 17: Global Test Accuracy = 0.2040
Round 18: Global Test Accuracy = 0.2060
Round 19: Global Test Accuracy = 0.2150
Round 20: Global Test Accuracy = 0.2200
Round 21: Global Test Accuracy = 0.2240
Round 22: Global Test Accuracy = 0.2310
Round 23: Global Test Accuracy = 0.2330
Round 24: Global Test Accuracy = 0.2380
Round 25: Global Test Accuracy = 0.2380
Round 26: Global Test Accuracy = 0.2500
Round 27: Global Test Accuracy = 0.2560
Round 28: Global Test Accuracy = 0.2600
Round 29: Global Test Accuracy = 0.2650
Round 30: Global Test Accuracy = 0.2630
Round 31: Global Test Accuracy = 0.2720
Round 32: Global Test Accuracy = 0.2760
Round 33: Global Test Accuracy = 0.2830
Round 34: Global Test Accuracy = 0.2960
Round 35: Global Test Accuracy = 0.2930
Round 36: Global Test Accuracy = 0.2960
Round 37: Global Test Accuracy = 0.3020
Round 38: Global Test Accuracy = 0.3050
Round 39: Global Test Accuracy = 0.3120
Round 40: Global Test Accuracy = 0.3190
Round 41: Global Test Accuracy = 0.3190
Round 42: Global Test Accuracy = 0.3220
Round 43: Global Test Accuracy = 0.3230
Round 44: Global Test Accuracy = 0.3340
Round 45: Global Test Accuracy = 0.3390
Round 46: Global Test Accuracy = 0.3450
Round 47: Global Test Accuracy = 0.3450
Round 48: Global Test Accuracy = 0.3510
Round 49: Global Test Accuracy = 0.3540
Round 50: Global Test Accuracy = 0.3560
Round 51: Global Test Accuracy = 0.3600
Round 52: Global Test Accuracy = 0.3650
Round 53: Global Test Accuracy = 0.3680
Round 54: Global Test Accuracy = 0.3760
Round 55: Global Test Accuracy = 0.3830
Round 56: Global Test Accuracy = 0.3840
Round 57: Global Test Accuracy = 0.3910
Round 58: Global Test Accuracy = 0.3880
Round 59: Global Test Accuracy = 0.3960
Round 60: Global Test Accuracy = 0.3980
Round 61: Global Test Accuracy = 0.3990
Round 62: Global Test Accuracy = 0.4020
Round 63: Global Test Accuracy = 0.4090
Round 64: Global Test Accuracy = 0.4130
Round 65: Global Test Accuracy = 0.4140
Round 66: Global Test Accuracy = 0.4200
Round 67: Global Test Accuracy = 0.4270
Round 68: Global Test Accuracy = 0.4300
Round 69: Global Test Accuracy = 0.4300
Round 70: Global Test Accuracy = 0.4380
Round 71: Global Test Accuracy = 0.4370
Round 72: Global Test Accuracy = 0.4380
Round 73: Global Test Accuracy = 0.4420
Round 74: Global Test Accuracy = 0.4440
Round 75: Global Test Accuracy = 0.4450
Round 76: Global Test Accuracy = 0.4470
Round 77: Global Test Accuracy = 0.4470
Round 78: Global Test Accuracy = 0.4540
Round 79: Global Test Accuracy = 0.4580
Round 80: Global Test Accuracy = 0.4600
Round 81: Global Test Accuracy = 0.4620
Round 82: Global Test Accuracy = 0.4640
Round 83: Global Test Accuracy = 0.4650
Round 84: Global Test Accuracy = 0.4700
Round 85: Global Test Accuracy = 0.4690
Round 86: Global Test Accuracy = 0.4730
Round 87: Global Test Accuracy = 0.4770
Round 88: Global Test Accuracy = 0.4800
Round 89: Global Test Accuracy = 0.4820
Round 90: Global Test Accuracy = 0.4860
Round 91: Global Test Accuracy = 0.4900
Round 92: Global Test Accuracy = 0.4900
Round 93: Global Test Accuracy = 0.4970
Round 94: Global Test Accuracy = 0.4950
Round 95: Global Test Accuracy = 0.5020
Round 96: Global Test Accuracy = 0.5010
Round 97: Global Test Accuracy = 0.5000
Round 98: Global Test Accuracy = 0.5030
Round 99: Global Test Accuracy = 0.5080
Round 100: Global Test Accuracy = 0.5080
Round 101: Global Test Accuracy = 0.5060
Round 102: Global Test Accuracy = 0.5080
Round 103: Global Test Accuracy = 0.5150
Round 104: Global Test Accuracy = 0.5140
Round 105: Global Test Accuracy = 0.5170
Round 106: Global Test Accuracy = 0.5190
Round 107: Global Test Accuracy = 0.5200
Round 108: Global Test Accuracy = 0.5170
Round 109: Global Test Accuracy = 0.5240
Round 110: Global Test Accuracy = 0.5260
Round 111: Global Test Accuracy = 0.5290
Round 112: Global Test Accuracy = 0.5300
Round 113: Global Test Accuracy = 0.5310
Round 114: Global Test Accuracy = 0.5310
Round 115: Global Test Accuracy = 0.5320
Round 116: Global Test Accuracy = 0.5340
Round 117: Global Test Accuracy = 0.5330
Round 118: Global Test Accuracy = 0.5370
Round 119: Global Test Accuracy = 0.5350
Round 120: Global Test Accuracy = 0.5340
Round 121: Global Test Accuracy = 0.5350
Round 122: Global Test Accuracy = 0.5350
Round 123: Global Test Accuracy = 0.5360
Round 124: Global Test Accuracy = 0.5400
Round 125: Global Test Accuracy = 0.5390
Round 126: Global Test Accuracy = 0.5410
Round 127: Global Test Accuracy = 0.5420
Round 128: Global Test Accuracy = 0.5420
Round 129: Global Test Accuracy = 0.5420
Round 130: Global Test Accuracy = 0.5460
Round 131: Global Test Accuracy = 0.5450
Round 132: Global Test Accuracy = 0.5460
Round 133: Global Test Accuracy = 0.5450
Round 134: Global Test Accuracy = 0.5470
Round 135: Global Test Accuracy = 0.5480
Round 136: Global Test Accuracy = 0.5470
Round 137: Global Test Accuracy = 0.5530
Round 138: Global Test Accuracy = 0.5510
Round 139: Global Test Accuracy = 0.5530
Round 140: Global Test Accuracy = 0.5510
Round 141: Global Test Accuracy = 0.5500
Round 142: Global Test Accuracy = 0.5500
Round 143: Global Test Accuracy = 0.5510
Round 144: Global Test Accuracy = 0.5500
Round 145: Global Test Accuracy = 0.5510
Round 146: Global Test Accuracy = 0.5510
Round 147: Global Test Accuracy = 0.5560
Round 148: Global Test Accuracy = 0.5570
Round 149: Global Test Accuracy = 0.5520
Round 150: Global Test Accuracy = 0.5530
Round 151: Global Test Accuracy = 0.5500
Round 152: Global Test Accuracy = 0.5500
Round 153: Global Test Accuracy = 0.5540
Round 154: Global Test Accuracy = 0.5540
Round 155: Global Test Accuracy = 0.5580
Round 156: Global Test Accuracy = 0.5530
Round 157: Global Test Accuracy = 0.5570
Round 158: Global Test Accuracy = 0.5560
Round 159: Global Test Accuracy = 0.5580
Round 160: Global Test Accuracy = 0.5560
Round 161: Global Test Accuracy = 0.5590
Round 162: Global Test Accuracy = 0.5600
Round 163: Global Test Accuracy = 0.5580
Round 164: Global Test Accuracy = 0.5610
Round 165: Global Test Accuracy = 0.5630
Round 166: Global Test Accuracy = 0.5640
Round 167: Global Test Accuracy = 0.5640
Round 168: Global Test Accuracy = 0.5640
Round 169: Global Test Accuracy = 0.5660
Round 170: Global Test Accuracy = 0.5650
Round 171: Global Test Accuracy = 0.5660
Round 172: Global Test Accuracy = 0.5650
Round 173: Global Test Accuracy = 0.5660
Round 174: Global Test Accuracy = 0.5670
Round 175: Global Test Accuracy = 0.5670
Round 176: Global Test Accuracy = 0.5710
Round 177: Global Test Accuracy = 0.5690
Round 178: Global Test Accuracy = 0.5700
Round 179: Global Test Accuracy = 0.5710
Round 180: Global Test Accuracy = 0.5700
Round 181: Global Test Accuracy = 0.5700
Round 182: Global Test Accuracy = 0.5720
Round 183: Global Test Accuracy = 0.5710
Round 184: Global Test Accuracy = 0.5740
Round 185: Global Test Accuracy = 0.5740
Round 186: Global Test Accuracy = 0.5770
Round 187: Global Test Accuracy = 0.5770
Round 188: Global Test Accuracy = 0.5770
Round 189: Global Test Accuracy = 0.5790
Round 190: Global Test Accuracy = 0.5810
Round 191: Global Test Accuracy = 0.5780
Round 192: Global Test Accuracy = 0.5780
Round 193: Global Test Accuracy = 0.5810
Round 194: Global Test Accuracy = 0.5810
Round 195: Global Test Accuracy = 0.5800
Round 196: Global Test Accuracy = 0.5810
Round 197: Global Test Accuracy = 0.5780
Round 198: Global Test Accuracy = 0.5780
Round 199: Global Test Accuracy = 0.5780
Round 200: Global Test Accuracy = 0.5790
//train_time: 4662.484 ms//end
//Log Max memory for Large1: 6502973440.0 //end
//Log Max memory for Large2: 5753737216.0 //end
//Log Max memory for Large3: 6244192256.0 //end
//Log Max memory for Large4: 6066032640.0 //end
//Log Max memory for Server: 17696579584.0 //end
//Log Large1 network: 58508085.0 //end
//Log Large2 network: 39196242.0 //end
//Log Large3 network: 58462016.0 //end
//Log Large4 network: 39098812.0 //end
//Log Server network: 195227752.0 //end
//Log Total Actual Train Comm Cost: 372.40 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 1.2931648693084716
Average test accuracy, 0.579
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 351.91 MB //end
[36m(Trainer pid=110020, ip=192.168.39.156)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(Trainer pid=110020, ip=192.168.39.156)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: cora, Trainers: 10, Distribution: average, IID Beta: 100.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'cora', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 100.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/cora/raw/ind.cora.x
File already exists: ./data/cora/raw/ind.cora.tx
File already exists: ./data/cora/raw/ind.cora.allx
File already exists: ./data/cora/raw/ind.cora.y
File already exists: ./data/cora/raw/ind.cora.ty
File already exists: ./data/cora/raw/ind.cora.ally
File already exists: ./data/cora/raw/ind.cora.graph
File already exists: ./data/cora/raw/ind.cora.test.index
Initialization start: network data collected.
2025-05-15 02:46:25,812	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 02:46:25,812	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 02:46:25,819	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=110644, ip=192.168.42.57)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=110644, ip=192.168.42.57)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5917.447 ms //end
//Log Large1 init network: 141923.0 //end
//Log Large2 init network: 123714.0 //end
//Log Large3 init network: 103134.0 //end
//Log Large4 init network: 111136.0 //end
//Log Server init network: 16167719.0 //end
//Log Initialization Communication Cost (MB): 15.88 //end
Pretrain start time recorded.
//pretrain_time: 5.138 ms//end
//Log Max memory for Large1: 6054424576.0 //end
//Log Max memory for Large2: 6106898432.0 //end
//Log Max memory for Large3: 5769154560.0 //end
//Log Max memory for Large4: 6462738432.0 //end
//Log Max memory for Server: 17728499712.0 //end
//Log Large1 network: 599680.0 //end
//Log Large2 network: 819004.0 //end
//Log Large3 network: 652556.0 //end
//Log Large4 network: 766411.0 //end
//Log Server network: 1879612.0 //end
//Log Total Actual Pretrain Comm Cost: 4.50 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1520
Round 2: Global Test Accuracy = 0.1480
Round 3: Global Test Accuracy = 0.1500
Round 4: Global Test Accuracy = 0.1520
Round 5: Global Test Accuracy = 0.1500
Round 6: Global Test Accuracy = 0.1550
Round 7: Global Test Accuracy = 0.1560
Round 8: Global Test Accuracy = 0.1560
Round 9: Global Test Accuracy = 0.1570
Round 10: Global Test Accuracy = 0.1560
Round 11: Global Test Accuracy = 0.1650
Round 12: Global Test Accuracy = 0.1700
Round 13: Global Test Accuracy = 0.1740
Round 14: Global Test Accuracy = 0.1780
Round 15: Global Test Accuracy = 0.1770
Round 16: Global Test Accuracy = 0.1830
Round 17: Global Test Accuracy = 0.1880
Round 18: Global Test Accuracy = 0.1900
Round 19: Global Test Accuracy = 0.1980
Round 20: Global Test Accuracy = 0.1990
Round 21: Global Test Accuracy = 0.2030
Round 22: Global Test Accuracy = 0.2080
Round 23: Global Test Accuracy = 0.2180
Round 24: Global Test Accuracy = 0.2190
Round 25: Global Test Accuracy = 0.2260
Round 26: Global Test Accuracy = 0.2350
Round 27: Global Test Accuracy = 0.2420
Round 28: Global Test Accuracy = 0.2480
Round 29: Global Test Accuracy = 0.2530
Round 30: Global Test Accuracy = 0.2550
Round 31: Global Test Accuracy = 0.2590
Round 32: Global Test Accuracy = 0.2620
Round 33: Global Test Accuracy = 0.2660
Round 34: Global Test Accuracy = 0.2670
Round 35: Global Test Accuracy = 0.2770
Round 36: Global Test Accuracy = 0.2860
Round 37: Global Test Accuracy = 0.2890
Round 38: Global Test Accuracy = 0.3000
Round 39: Global Test Accuracy = 0.3020
Round 40: Global Test Accuracy = 0.3110
Round 41: Global Test Accuracy = 0.3150
Round 42: Global Test Accuracy = 0.3120
Round 43: Global Test Accuracy = 0.3170
Round 44: Global Test Accuracy = 0.3240
Round 45: Global Test Accuracy = 0.3250
Round 46: Global Test Accuracy = 0.3310
Round 47: Global Test Accuracy = 0.3320
Round 48: Global Test Accuracy = 0.3370
Round 49: Global Test Accuracy = 0.3420
Round 50: Global Test Accuracy = 0.3450
Round 51: Global Test Accuracy = 0.3540
Round 52: Global Test Accuracy = 0.3570
Round 53: Global Test Accuracy = 0.3580
Round 54: Global Test Accuracy = 0.3620
Round 55: Global Test Accuracy = 0.3640
Round 56: Global Test Accuracy = 0.3660
Round 57: Global Test Accuracy = 0.3670
Round 58: Global Test Accuracy = 0.3710
Round 59: Global Test Accuracy = 0.3730
Round 60: Global Test Accuracy = 0.3780
Round 61: Global Test Accuracy = 0.3810
Round 62: Global Test Accuracy = 0.3870
Round 63: Global Test Accuracy = 0.3910
Round 64: Global Test Accuracy = 0.3910
Round 65: Global Test Accuracy = 0.3890
Round 66: Global Test Accuracy = 0.3950
Round 67: Global Test Accuracy = 0.3980
Round 68: Global Test Accuracy = 0.4020
Round 69: Global Test Accuracy = 0.4060
Round 70: Global Test Accuracy = 0.4110
Round 71: Global Test Accuracy = 0.4200
Round 72: Global Test Accuracy = 0.4230
Round 73: Global Test Accuracy = 0.4270
Round 74: Global Test Accuracy = 0.4280
Round 75: Global Test Accuracy = 0.4320
Round 76: Global Test Accuracy = 0.4320
Round 77: Global Test Accuracy = 0.4360
Round 78: Global Test Accuracy = 0.4460
Round 79: Global Test Accuracy = 0.4420
Round 80: Global Test Accuracy = 0.4460
Round 81: Global Test Accuracy = 0.4430
Round 82: Global Test Accuracy = 0.4470
Round 83: Global Test Accuracy = 0.4530
Round 84: Global Test Accuracy = 0.4530
Round 85: Global Test Accuracy = 0.4570
Round 86: Global Test Accuracy = 0.4680
Round 87: Global Test Accuracy = 0.4680
Round 88: Global Test Accuracy = 0.4720
Round 89: Global Test Accuracy = 0.4740
Round 90: Global Test Accuracy = 0.4830
Round 91: Global Test Accuracy = 0.4820
Round 92: Global Test Accuracy = 0.4880
Round 93: Global Test Accuracy = 0.4900
Round 94: Global Test Accuracy = 0.4930
Round 95: Global Test Accuracy = 0.4950
Round 96: Global Test Accuracy = 0.5020
Round 97: Global Test Accuracy = 0.5080
Round 98: Global Test Accuracy = 0.5110
Round 99: Global Test Accuracy = 0.5150
Round 100: Global Test Accuracy = 0.5140
Round 101: Global Test Accuracy = 0.5150
Round 102: Global Test Accuracy = 0.5180
Round 103: Global Test Accuracy = 0.5220
Round 104: Global Test Accuracy = 0.5240
Round 105: Global Test Accuracy = 0.5290
Round 106: Global Test Accuracy = 0.5310
Round 107: Global Test Accuracy = 0.5340
Round 108: Global Test Accuracy = 0.5340
Round 109: Global Test Accuracy = 0.5340
Round 110: Global Test Accuracy = 0.5380
Round 111: Global Test Accuracy = 0.5370
Round 112: Global Test Accuracy = 0.5400
Round 113: Global Test Accuracy = 0.5410
Round 114: Global Test Accuracy = 0.5410
Round 115: Global Test Accuracy = 0.5420
Round 116: Global Test Accuracy = 0.5440
Round 117: Global Test Accuracy = 0.5440
Round 118: Global Test Accuracy = 0.5450
Round 119: Global Test Accuracy = 0.5490
Round 120: Global Test Accuracy = 0.5520
Round 121: Global Test Accuracy = 0.5540
Round 122: Global Test Accuracy = 0.5560
Round 123: Global Test Accuracy = 0.5580
Round 124: Global Test Accuracy = 0.5580
Round 125: Global Test Accuracy = 0.5580
Round 126: Global Test Accuracy = 0.5600
Round 127: Global Test Accuracy = 0.5610
Round 128: Global Test Accuracy = 0.5610
Round 129: Global Test Accuracy = 0.5640
Round 130: Global Test Accuracy = 0.5630
Round 131: Global Test Accuracy = 0.5630
Round 132: Global Test Accuracy = 0.5640
Round 133: Global Test Accuracy = 0.5640
Round 134: Global Test Accuracy = 0.5650
Round 135: Global Test Accuracy = 0.5640
Round 136: Global Test Accuracy = 0.5650
Round 137: Global Test Accuracy = 0.5660
Round 138: Global Test Accuracy = 0.5670
Round 139: Global Test Accuracy = 0.5690
Round 140: Global Test Accuracy = 0.5710
Round 141: Global Test Accuracy = 0.5640
Round 142: Global Test Accuracy = 0.5670
Round 143: Global Test Accuracy = 0.5700
Round 144: Global Test Accuracy = 0.5660
Round 145: Global Test Accuracy = 0.5680
Round 146: Global Test Accuracy = 0.5670
Round 147: Global Test Accuracy = 0.5730
Round 148: Global Test Accuracy = 0.5740
Round 149: Global Test Accuracy = 0.5700
Round 150: Global Test Accuracy = 0.5680
Round 151: Global Test Accuracy = 0.5700
Round 152: Global Test Accuracy = 0.5730
Round 153: Global Test Accuracy = 0.5720
Round 154: Global Test Accuracy = 0.5710
Round 155: Global Test Accuracy = 0.5730
Round 156: Global Test Accuracy = 0.5740
Round 157: Global Test Accuracy = 0.5750
Round 158: Global Test Accuracy = 0.5750
Round 159: Global Test Accuracy = 0.5780
Round 160: Global Test Accuracy = 0.5770
Round 161: Global Test Accuracy = 0.5760
Round 162: Global Test Accuracy = 0.5750
Round 163: Global Test Accuracy = 0.5770
Round 164: Global Test Accuracy = 0.5790
Round 165: Global Test Accuracy = 0.5820
Round 166: Global Test Accuracy = 0.5850
Round 167: Global Test Accuracy = 0.5840
Round 168: Global Test Accuracy = 0.5840
Round 169: Global Test Accuracy = 0.5840
Round 170: Global Test Accuracy = 0.5840
Round 171: Global Test Accuracy = 0.5840
Round 172: Global Test Accuracy = 0.5870
Round 173: Global Test Accuracy = 0.5870
Round 174: Global Test Accuracy = 0.5860
Round 175: Global Test Accuracy = 0.5840
Round 176: Global Test Accuracy = 0.5820
Round 177: Global Test Accuracy = 0.5840
Round 178: Global Test Accuracy = 0.5850
Round 179: Global Test Accuracy = 0.5820
Round 180: Global Test Accuracy = 0.5830
Round 181: Global Test Accuracy = 0.5860
Round 182: Global Test Accuracy = 0.5870
Round 183: Global Test Accuracy = 0.5880
Round 184: Global Test Accuracy = 0.5890
Round 185: Global Test Accuracy = 0.5900
Round 186: Global Test Accuracy = 0.5920
Round 187: Global Test Accuracy = 0.5930
Round 188: Global Test Accuracy = 0.5890
Round 189: Global Test Accuracy = 0.5910
Round 190: Global Test Accuracy = 0.5950
Round 191: Global Test Accuracy = 0.5930
Round 192: Global Test Accuracy = 0.5910
Round 193: Global Test Accuracy = 0.5930
Round 194: Global Test Accuracy = 0.5910
Round 195: Global Test Accuracy = 0.5920
Round 196: Global Test Accuracy = 0.5910
Round 197: Global Test Accuracy = 0.5930
Round 198: Global Test Accuracy = 0.5930
Round 199: Global Test Accuracy = 0.5920
Round 200: Global Test Accuracy = 0.5910
//train_time: 4532.789 ms//end
//Log Max memory for Large1: 6077161472.0 //end
//Log Max memory for Large2: 6139133952.0 //end
//Log Max memory for Large3: 5790351360.0 //end
//Log Max memory for Large4: 6499872768.0 //end
//Log Max memory for Server: 17780457472.0 //end
//Log Large1 network: 39153350.0 //end
//Log Large2 network: 58572772.0 //end
//Log Large3 network: 39147243.0 //end
//Log Large4 network: 58494614.0 //end
//Log Server network: 195345241.0 //end
//Log Total Actual Train Comm Cost: 372.61 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 1.2607940629720689
Average test accuracy, 0.591
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 351.91 MB //end
[36m(Trainer pid=114643, ip=192.168.14.62)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=114643, ip=192.168.14.62)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: cora, Trainers: 10, Distribution: average, IID Beta: 10.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'cora', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/cora/raw/ind.cora.x
File already exists: ./data/cora/raw/ind.cora.tx
File already exists: ./data/cora/raw/ind.cora.allx
File already exists: ./data/cora/raw/ind.cora.y
File already exists: ./data/cora/raw/ind.cora.ty
File already exists: ./data/cora/raw/ind.cora.ally
File already exists: ./data/cora/raw/ind.cora.graph
File already exists: ./data/cora/raw/ind.cora.test.index
Initialization start: network data collected.
2025-05-15 02:47:41,672	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 02:47:41,672	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 02:47:41,680	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=111200, ip=192.168.42.57)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=111200, ip=192.168.42.57)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5584.669 ms //end
//Log Large1 init network: 125138.0 //end
//Log Large2 init network: 111974.0 //end
//Log Large3 init network: 115770.0 //end
//Log Large4 init network: 100221.0 //end
//Log Server init network: 16322839.0 //end
//Log Initialization Communication Cost (MB): 16.00 //end
Pretrain start time recorded.
//pretrain_time: 5.694 ms//end
//Log Max memory for Large1: 6476361728.0 //end
//Log Max memory for Large2: 5694398464.0 //end
//Log Max memory for Large3: 6191325184.0 //end
//Log Max memory for Large4: 6049112064.0 //end
//Log Max memory for Server: 17792983040.0 //end
//Log Large1 network: 744101.0 //end
//Log Large2 network: 722236.0 //end
//Log Large3 network: 793107.0 //end
//Log Large4 network: 653750.0 //end
//Log Server network: 1886406.0 //end
//Log Total Actual Pretrain Comm Cost: 4.58 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1600
Round 2: Global Test Accuracy = 0.1610
Round 3: Global Test Accuracy = 0.1590
Round 4: Global Test Accuracy = 0.1630
Round 5: Global Test Accuracy = 0.1670
Round 6: Global Test Accuracy = 0.1690
Round 7: Global Test Accuracy = 0.1780
Round 8: Global Test Accuracy = 0.1850
Round 9: Global Test Accuracy = 0.1900
Round 10: Global Test Accuracy = 0.2000
Round 11: Global Test Accuracy = 0.2020
Round 12: Global Test Accuracy = 0.2030
Round 13: Global Test Accuracy = 0.2130
Round 14: Global Test Accuracy = 0.2170
Round 15: Global Test Accuracy = 0.2250
Round 16: Global Test Accuracy = 0.2270
Round 17: Global Test Accuracy = 0.2340
Round 18: Global Test Accuracy = 0.2350
Round 19: Global Test Accuracy = 0.2510
Round 20: Global Test Accuracy = 0.2560
Round 21: Global Test Accuracy = 0.2590
Round 22: Global Test Accuracy = 0.2690
Round 23: Global Test Accuracy = 0.2730
Round 24: Global Test Accuracy = 0.2830
Round 25: Global Test Accuracy = 0.2940
Round 26: Global Test Accuracy = 0.2960
Round 27: Global Test Accuracy = 0.3060
Round 28: Global Test Accuracy = 0.3120
Round 29: Global Test Accuracy = 0.3220
Round 30: Global Test Accuracy = 0.3260
Round 31: Global Test Accuracy = 0.3260
Round 32: Global Test Accuracy = 0.3330
Round 33: Global Test Accuracy = 0.3370
Round 34: Global Test Accuracy = 0.3420
Round 35: Global Test Accuracy = 0.3450
Round 36: Global Test Accuracy = 0.3470
Round 37: Global Test Accuracy = 0.3520
Round 38: Global Test Accuracy = 0.3610
Round 39: Global Test Accuracy = 0.3650
Round 40: Global Test Accuracy = 0.3760
Round 41: Global Test Accuracy = 0.3830
Round 42: Global Test Accuracy = 0.3830
Round 43: Global Test Accuracy = 0.3860
Round 44: Global Test Accuracy = 0.3870
Round 45: Global Test Accuracy = 0.3890
Round 46: Global Test Accuracy = 0.3940
Round 47: Global Test Accuracy = 0.3940
Round 48: Global Test Accuracy = 0.4030
Round 49: Global Test Accuracy = 0.4130
Round 50: Global Test Accuracy = 0.4110
Round 51: Global Test Accuracy = 0.4160
Round 52: Global Test Accuracy = 0.4210
Round 53: Global Test Accuracy = 0.4200
Round 54: Global Test Accuracy = 0.4270
Round 55: Global Test Accuracy = 0.4270
Round 56: Global Test Accuracy = 0.4300
Round 57: Global Test Accuracy = 0.4310
Round 58: Global Test Accuracy = 0.4340
Round 59: Global Test Accuracy = 0.4360
Round 60: Global Test Accuracy = 0.4410
Round 61: Global Test Accuracy = 0.4430
Round 62: Global Test Accuracy = 0.4490
Round 63: Global Test Accuracy = 0.4470
Round 64: Global Test Accuracy = 0.4560
Round 65: Global Test Accuracy = 0.4570
Round 66: Global Test Accuracy = 0.4630
Round 67: Global Test Accuracy = 0.4680
Round 68: Global Test Accuracy = 0.4720
Round 69: Global Test Accuracy = 0.4730
Round 70: Global Test Accuracy = 0.4800
Round 71: Global Test Accuracy = 0.4780
Round 72: Global Test Accuracy = 0.4830
Round 73: Global Test Accuracy = 0.4840
Round 74: Global Test Accuracy = 0.4870
Round 75: Global Test Accuracy = 0.4890
Round 76: Global Test Accuracy = 0.4940
Round 77: Global Test Accuracy = 0.4980
Round 78: Global Test Accuracy = 0.5010
Round 79: Global Test Accuracy = 0.5060
Round 80: Global Test Accuracy = 0.5110
Round 81: Global Test Accuracy = 0.5140
Round 82: Global Test Accuracy = 0.5200
Round 83: Global Test Accuracy = 0.5180
Round 84: Global Test Accuracy = 0.5240
Round 85: Global Test Accuracy = 0.5240
Round 86: Global Test Accuracy = 0.5300
Round 87: Global Test Accuracy = 0.5320
Round 88: Global Test Accuracy = 0.5350
Round 89: Global Test Accuracy = 0.5360
Round 90: Global Test Accuracy = 0.5390
Round 91: Global Test Accuracy = 0.5400
Round 92: Global Test Accuracy = 0.5420
Round 93: Global Test Accuracy = 0.5450
Round 94: Global Test Accuracy = 0.5430
Round 95: Global Test Accuracy = 0.5480
Round 96: Global Test Accuracy = 0.5460
Round 97: Global Test Accuracy = 0.5480
Round 98: Global Test Accuracy = 0.5510
Round 99: Global Test Accuracy = 0.5600
Round 100: Global Test Accuracy = 0.5580
Round 101: Global Test Accuracy = 0.5590
Round 102: Global Test Accuracy = 0.5640
Round 103: Global Test Accuracy = 0.5630
Round 104: Global Test Accuracy = 0.5630
Round 105: Global Test Accuracy = 0.5680
Round 106: Global Test Accuracy = 0.5690
Round 107: Global Test Accuracy = 0.5700
Round 108: Global Test Accuracy = 0.5720
Round 109: Global Test Accuracy = 0.5710
Round 110: Global Test Accuracy = 0.5710
Round 111: Global Test Accuracy = 0.5760
Round 112: Global Test Accuracy = 0.5750
Round 113: Global Test Accuracy = 0.5780
Round 114: Global Test Accuracy = 0.5790
Round 115: Global Test Accuracy = 0.5770
Round 116: Global Test Accuracy = 0.5780
Round 117: Global Test Accuracy = 0.5830
Round 118: Global Test Accuracy = 0.5800
Round 119: Global Test Accuracy = 0.5830
Round 120: Global Test Accuracy = 0.5830
Round 121: Global Test Accuracy = 0.5840
Round 122: Global Test Accuracy = 0.5860
Round 123: Global Test Accuracy = 0.5830
Round 124: Global Test Accuracy = 0.5840
Round 125: Global Test Accuracy = 0.5850
Round 126: Global Test Accuracy = 0.5870
Round 127: Global Test Accuracy = 0.5890
Round 128: Global Test Accuracy = 0.5890
Round 129: Global Test Accuracy = 0.5890
Round 130: Global Test Accuracy = 0.5900
Round 131: Global Test Accuracy = 0.5950
Round 132: Global Test Accuracy = 0.5920
Round 133: Global Test Accuracy = 0.5960
Round 134: Global Test Accuracy = 0.5970
Round 135: Global Test Accuracy = 0.5970
Round 136: Global Test Accuracy = 0.5980
Round 137: Global Test Accuracy = 0.6000
Round 138: Global Test Accuracy = 0.5990
Round 139: Global Test Accuracy = 0.6010
Round 140: Global Test Accuracy = 0.6020
Round 141: Global Test Accuracy = 0.6060
Round 142: Global Test Accuracy = 0.6060
Round 143: Global Test Accuracy = 0.6060
Round 144: Global Test Accuracy = 0.6070
Round 145: Global Test Accuracy = 0.6070
Round 146: Global Test Accuracy = 0.6100
Round 147: Global Test Accuracy = 0.6100
Round 148: Global Test Accuracy = 0.6050
Round 149: Global Test Accuracy = 0.6080
Round 150: Global Test Accuracy = 0.6080
Round 151: Global Test Accuracy = 0.6080
Round 152: Global Test Accuracy = 0.6070
Round 153: Global Test Accuracy = 0.6110
Round 154: Global Test Accuracy = 0.6120
Round 155: Global Test Accuracy = 0.6130
Round 156: Global Test Accuracy = 0.6120
Round 157: Global Test Accuracy = 0.6120
Round 158: Global Test Accuracy = 0.6120
Round 159: Global Test Accuracy = 0.6130
Round 160: Global Test Accuracy = 0.6120
Round 161: Global Test Accuracy = 0.6130
Round 162: Global Test Accuracy = 0.6120
Round 163: Global Test Accuracy = 0.6130
Round 164: Global Test Accuracy = 0.6140
Round 165: Global Test Accuracy = 0.6140
Round 166: Global Test Accuracy = 0.6160
Round 167: Global Test Accuracy = 0.6150
Round 168: Global Test Accuracy = 0.6140
Round 169: Global Test Accuracy = 0.6140
Round 170: Global Test Accuracy = 0.6130
Round 171: Global Test Accuracy = 0.6140
Round 172: Global Test Accuracy = 0.6110
Round 173: Global Test Accuracy = 0.6130
Round 174: Global Test Accuracy = 0.6130
Round 175: Global Test Accuracy = 0.6130
Round 176: Global Test Accuracy = 0.6130
Round 177: Global Test Accuracy = 0.6170
Round 178: Global Test Accuracy = 0.6150
Round 179: Global Test Accuracy = 0.6140
Round 180: Global Test Accuracy = 0.6200
Round 181: Global Test Accuracy = 0.6180
Round 182: Global Test Accuracy = 0.6190
Round 183: Global Test Accuracy = 0.6200
Round 184: Global Test Accuracy = 0.6160
Round 185: Global Test Accuracy = 0.6170
Round 186: Global Test Accuracy = 0.6180
Round 187: Global Test Accuracy = 0.6170
Round 188: Global Test Accuracy = 0.6180
Round 189: Global Test Accuracy = 0.6190
Round 190: Global Test Accuracy = 0.6190
Round 191: Global Test Accuracy = 0.6160
Round 192: Global Test Accuracy = 0.6170
Round 193: Global Test Accuracy = 0.6140
Round 194: Global Test Accuracy = 0.6140
Round 195: Global Test Accuracy = 0.6140
Round 196: Global Test Accuracy = 0.6130
Round 197: Global Test Accuracy = 0.6130
Round 198: Global Test Accuracy = 0.6150
Round 199: Global Test Accuracy = 0.6170
Round 200: Global Test Accuracy = 0.6170
//train_time: 4686.413 ms//end
//Log Max memory for Large1: 6506020864.0 //end
//Log Max memory for Large2: 5714579456.0 //end
//Log Max memory for Large3: 6220259328.0 //end
//Log Max memory for Large4: 6069448704.0 //end
//Log Max memory for Server: 17845166080.0 //end
//Log Large1 network: 58505015.0 //end
//Log Large2 network: 39230156.0 //end
//Log Large3 network: 58515741.0 //end
//Log Large4 network: 39138029.0 //end
//Log Server network: 195244322.0 //end
//Log Total Actual Train Comm Cost: 372.54 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 1.2268775664567948
Average test accuracy, 0.617
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 351.91 MB //end
[36m(Trainer pid=111066, ip=192.168.39.156)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=111066, ip=192.168.39.156)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: cora, Trainers: 10, Distribution: average, IID Beta: 10000.0, Hops: 1, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'cora', 'method': 'fedgcn', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 1, 'iid_beta': 10000.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/cora/raw/ind.cora.x
File already exists: ./data/cora/raw/ind.cora.tx
File already exists: ./data/cora/raw/ind.cora.allx
File already exists: ./data/cora/raw/ind.cora.y
File already exists: ./data/cora/raw/ind.cora.ty
File already exists: ./data/cora/raw/ind.cora.ally
File already exists: ./data/cora/raw/ind.cora.graph
File already exists: ./data/cora/raw/ind.cora.test.index
Initialization start: network data collected.
2025-05-15 02:48:57,332	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 02:48:57,332	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 02:48:57,338	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
[36m(Trainer pid=111679, ip=192.168.42.57)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=111679, ip=192.168.42.57)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5740.216 ms //end
//Log Large1 init network: 109065.0 //end
//Log Large2 init network: 129774.0 //end
//Log Large3 init network: 106827.0 //end
//Log Large4 init network: 127658.0 //end
//Log Server init network: 17529156.0 //end
//Log Initialization Communication Cost (MB): 17.17 //end
Pretrain start time recorded.
server aggregates all local neighbor feature sums
clients received feature aggregation from server
//pretrain_time: 407.955 ms//end
//Log Max memory for Large1: 6117376000.0 //end
//Log Max memory for Large2: 6243991552.0 //end
//Log Max memory for Large3: 5864734720.0 //end
//Log Max memory for Large4: 6590029824.0 //end
//Log Max memory for Server: 18005278720.0 //end
//Log Large1 network: 32474546.0 //end
//Log Large2 network: 48113082.0 //end
//Log Large3 network: 32496269.0 //end
//Log Large4 network: 48616487.0 //end
//Log Server network: 59565734.0 //end
//Log Total Actual Pretrain Comm Cost: 211.02 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.4640
Round 2: Global Test Accuracy = 0.6240
Round 3: Global Test Accuracy = 0.6540
Round 4: Global Test Accuracy = 0.7290
Round 5: Global Test Accuracy = 0.7530
Round 6: Global Test Accuracy = 0.7860
Round 7: Global Test Accuracy = 0.7420
Round 8: Global Test Accuracy = 0.7860
Round 9: Global Test Accuracy = 0.7700
Round 10: Global Test Accuracy = 0.7770
Round 11: Global Test Accuracy = 0.7830
Round 12: Global Test Accuracy = 0.7850
Round 13: Global Test Accuracy = 0.7860
Round 14: Global Test Accuracy = 0.7850
Round 15: Global Test Accuracy = 0.7930
Round 16: Global Test Accuracy = 0.7870
Round 17: Global Test Accuracy = 0.7960
Round 18: Global Test Accuracy = 0.7810
Round 19: Global Test Accuracy = 0.7850
Round 20: Global Test Accuracy = 0.7900
Round 21: Global Test Accuracy = 0.7850
Round 22: Global Test Accuracy = 0.7860
Round 23: Global Test Accuracy = 0.7800
Round 24: Global Test Accuracy = 0.7810
Round 25: Global Test Accuracy = 0.7840
Round 26: Global Test Accuracy = 0.7870
Round 27: Global Test Accuracy = 0.7890
Round 28: Global Test Accuracy = 0.7890
Round 29: Global Test Accuracy = 0.7920
Round 30: Global Test Accuracy = 0.7910
Round 31: Global Test Accuracy = 0.7830
Round 32: Global Test Accuracy = 0.7870
Round 33: Global Test Accuracy = 0.7970
Round 34: Global Test Accuracy = 0.7930
Round 35: Global Test Accuracy = 0.8000
Round 36: Global Test Accuracy = 0.7930
Round 37: Global Test Accuracy = 0.7890
Round 38: Global Test Accuracy = 0.7870
Round 39: Global Test Accuracy = 0.7960
Round 40: Global Test Accuracy = 0.7870
Round 41: Global Test Accuracy = 0.7980
Round 42: Global Test Accuracy = 0.7950
Round 43: Global Test Accuracy = 0.8070
Round 44: Global Test Accuracy = 0.7940
Round 45: Global Test Accuracy = 0.7950
Round 46: Global Test Accuracy = 0.7970
Round 47: Global Test Accuracy = 0.7980
Round 48: Global Test Accuracy = 0.7850
Round 49: Global Test Accuracy = 0.7950
Round 50: Global Test Accuracy = 0.7910
Round 51: Global Test Accuracy = 0.7880
Round 52: Global Test Accuracy = 0.7960
Round 53: Global Test Accuracy = 0.7960
Round 54: Global Test Accuracy = 0.7950
Round 55: Global Test Accuracy = 0.7900
Round 56: Global Test Accuracy = 0.7950
Round 57: Global Test Accuracy = 0.7910
Round 58: Global Test Accuracy = 0.7940
Round 59: Global Test Accuracy = 0.7920
Round 60: Global Test Accuracy = 0.7850
Round 61: Global Test Accuracy = 0.7970
Round 62: Global Test Accuracy = 0.7980
Round 63: Global Test Accuracy = 0.8050
Round 64: Global Test Accuracy = 0.8040
Round 65: Global Test Accuracy = 0.8010
Round 66: Global Test Accuracy = 0.8000
Round 67: Global Test Accuracy = 0.7970
Round 68: Global Test Accuracy = 0.7930
Round 69: Global Test Accuracy = 0.7980
Round 70: Global Test Accuracy = 0.7920
Round 71: Global Test Accuracy = 0.7890
Round 72: Global Test Accuracy = 0.7920
Round 73: Global Test Accuracy = 0.7970
Round 74: Global Test Accuracy = 0.7940
Round 75: Global Test Accuracy = 0.7950
Round 76: Global Test Accuracy = 0.7970
Round 77: Global Test Accuracy = 0.7930
Round 78: Global Test Accuracy = 0.7940
Round 79: Global Test Accuracy = 0.7940
Round 80: Global Test Accuracy = 0.7960
Round 81: Global Test Accuracy = 0.7950
Round 82: Global Test Accuracy = 0.7960
Round 83: Global Test Accuracy = 0.8000
Round 84: Global Test Accuracy = 0.7900
Round 85: Global Test Accuracy = 0.7940
Round 86: Global Test Accuracy = 0.7940
Round 87: Global Test Accuracy = 0.7900
Round 88: Global Test Accuracy = 0.7890
Round 89: Global Test Accuracy = 0.7900
Round 90: Global Test Accuracy = 0.7940
Round 91: Global Test Accuracy = 0.7900
Round 92: Global Test Accuracy = 0.7910
Round 93: Global Test Accuracy = 0.7940
Round 94: Global Test Accuracy = 0.7930
Round 95: Global Test Accuracy = 0.7940
Round 96: Global Test Accuracy = 0.7960
Round 97: Global Test Accuracy = 0.7950
Round 98: Global Test Accuracy = 0.7940
Round 99: Global Test Accuracy = 0.7930
Round 100: Global Test Accuracy = 0.7980
Round 101: Global Test Accuracy = 0.7890
Round 102: Global Test Accuracy = 0.7930
Round 103: Global Test Accuracy = 0.7920
Round 104: Global Test Accuracy = 0.7900
Round 105: Global Test Accuracy = 0.7930
Round 106: Global Test Accuracy = 0.7860
Round 107: Global Test Accuracy = 0.7850
Round 108: Global Test Accuracy = 0.7880
Round 109: Global Test Accuracy = 0.7900
Round 110: Global Test Accuracy = 0.7890
Round 111: Global Test Accuracy = 0.7920
Round 112: Global Test Accuracy = 0.7890
Round 113: Global Test Accuracy = 0.7870
Round 114: Global Test Accuracy = 0.7900
Round 115: Global Test Accuracy = 0.7890
Round 116: Global Test Accuracy = 0.7900
Round 117: Global Test Accuracy = 0.7910
Round 118: Global Test Accuracy = 0.7900
Round 119: Global Test Accuracy = 0.7940
Round 120: Global Test Accuracy = 0.7900
Round 121: Global Test Accuracy = 0.7890
Round 122: Global Test Accuracy = 0.7880
Round 123: Global Test Accuracy = 0.7910
Round 124: Global Test Accuracy = 0.7910
Round 125: Global Test Accuracy = 0.7930
Round 126: Global Test Accuracy = 0.7940
Round 127: Global Test Accuracy = 0.7920
Round 128: Global Test Accuracy = 0.7910
Round 129: Global Test Accuracy = 0.7930
Round 130: Global Test Accuracy = 0.7920
Round 131: Global Test Accuracy = 0.7920
Round 132: Global Test Accuracy = 0.7910
Round 133: Global Test Accuracy = 0.7910
Round 134: Global Test Accuracy = 0.7890
Round 135: Global Test Accuracy = 0.7900
Round 136: Global Test Accuracy = 0.7910
Round 137: Global Test Accuracy = 0.7890
Round 138: Global Test Accuracy = 0.7910
Round 139: Global Test Accuracy = 0.7910
Round 140: Global Test Accuracy = 0.7920
Round 141: Global Test Accuracy = 0.7890
Round 142: Global Test Accuracy = 0.7900
Round 143: Global Test Accuracy = 0.7900
Round 144: Global Test Accuracy = 0.7930
Round 145: Global Test Accuracy = 0.7930
Round 146: Global Test Accuracy = 0.7920
Round 147: Global Test Accuracy = 0.7910
Round 148: Global Test Accuracy = 0.7910
Round 149: Global Test Accuracy = 0.7890
Round 150: Global Test Accuracy = 0.7910
Round 151: Global Test Accuracy = 0.7930
Round 152: Global Test Accuracy = 0.7910
Round 153: Global Test Accuracy = 0.7920
Round 154: Global Test Accuracy = 0.7930
Round 155: Global Test Accuracy = 0.7940
Round 156: Global Test Accuracy = 0.7910
Round 157: Global Test Accuracy = 0.7940
Round 158: Global Test Accuracy = 0.7910
Round 159: Global Test Accuracy = 0.7890
Round 160: Global Test Accuracy = 0.7900
Round 161: Global Test Accuracy = 0.7900
Round 162: Global Test Accuracy = 0.7910
Round 163: Global Test Accuracy = 0.7910
Round 164: Global Test Accuracy = 0.7890
Round 165: Global Test Accuracy = 0.7910
Round 166: Global Test Accuracy = 0.7890
Round 167: Global Test Accuracy = 0.7890
Round 168: Global Test Accuracy = 0.7900
Round 169: Global Test Accuracy = 0.7900
Round 170: Global Test Accuracy = 0.7910
Round 171: Global Test Accuracy = 0.7920
Round 172: Global Test Accuracy = 0.7910
Round 173: Global Test Accuracy = 0.7920
Round 174: Global Test Accuracy = 0.7910
Round 175: Global Test Accuracy = 0.7920
Round 176: Global Test Accuracy = 0.7920
Round 177: Global Test Accuracy = 0.7890
Round 178: Global Test Accuracy = 0.7920
Round 179: Global Test Accuracy = 0.7900
Round 180: Global Test Accuracy = 0.7910
Round 181: Global Test Accuracy = 0.7920
Round 182: Global Test Accuracy = 0.7920
Round 183: Global Test Accuracy = 0.7920
Round 184: Global Test Accuracy = 0.7920
Round 185: Global Test Accuracy = 0.7910
Round 186: Global Test Accuracy = 0.7920
Round 187: Global Test Accuracy = 0.7910
Round 188: Global Test Accuracy = 0.7900
Round 189: Global Test Accuracy = 0.7910
Round 190: Global Test Accuracy = 0.7930
Round 191: Global Test Accuracy = 0.7910
Round 192: Global Test Accuracy = 0.7890
Round 193: Global Test Accuracy = 0.7880
Round 194: Global Test Accuracy = 0.7870
Round 195: Global Test Accuracy = 0.7900
Round 196: Global Test Accuracy = 0.7910
Round 197: Global Test Accuracy = 0.7900
Round 198: Global Test Accuracy = 0.7920
Round 199: Global Test Accuracy = 0.7920
Round 200: Global Test Accuracy = 0.7920
//train_time: 4689.082 ms//end
//Log Max memory for Large1: 6134677504.0 //end
//Log Max memory for Large2: 6270926848.0 //end
//Log Max memory for Large3: 5880496128.0 //end
//Log Max memory for Large4: 6618066944.0 //end
//Log Max memory for Server: 18110423040.0 //end
//Log Large1 network: 39179477.0 //end
//Log Large2 network: 58670936.0 //end
//Log Large3 network: 39207068.0 //end
//Log Large4 network: 58478309.0 //end
//Log Server network: 195201398.0 //end
//Log Total Actual Train Comm Cost: 372.64 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 0.7594072321653366
Average test accuracy, 0.792
//Log Theoretical Pretrain Comm Cost: 202.69 MB //end
//Log Theoretical Train Comm Cost: 351.91 MB //end
[36m(Trainer pid=115676, ip=192.168.14.62)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=115676, ip=192.168.14.62)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: cora, Trainers: 10, Distribution: average, IID Beta: 100.0, Hops: 1, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'cora', 'method': 'fedgcn', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 1, 'iid_beta': 100.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/cora/raw/ind.cora.x
File already exists: ./data/cora/raw/ind.cora.tx
File already exists: ./data/cora/raw/ind.cora.allx
File already exists: ./data/cora/raw/ind.cora.y
File already exists: ./data/cora/raw/ind.cora.ty
File already exists: ./data/cora/raw/ind.cora.ally
File already exists: ./data/cora/raw/ind.cora.graph
File already exists: ./data/cora/raw/ind.cora.test.index
Initialization start: network data collected.
2025-05-15 02:50:13,644	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 02:50:13,644	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 02:50:13,650	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
[36m(Trainer pid=112268, ip=192.168.14.54)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=112268, ip=192.168.14.54)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5622.355 ms //end
//Log Large1 init network: 127863.0 //end
//Log Large2 init network: 114726.0 //end
//Log Large3 init network: 111135.0 //end
//Log Large4 init network: 150874.0 //end
//Log Server init network: 17481101.0 //end
//Log Initialization Communication Cost (MB): 17.15 //end
Pretrain start time recorded.
server aggregates all local neighbor feature sums
clients received feature aggregation from server
//pretrain_time: 452.424 ms//end
//Log Max memory for Large1: 6557663232.0 //end
//Log Max memory for Large2: 5814267904.0 //end
//Log Max memory for Large3: 6326706176.0 //end
//Log Max memory for Large4: 6168219648.0 //end
//Log Max memory for Server: 18127458304.0 //end
//Log Large1 network: 48795609.0 //end
//Log Large2 network: 32611350.0 //end
//Log Large3 network: 47945077.0 //end
//Log Large4 network: 32753765.0 //end
//Log Server network: 60026956.0 //end
//Log Total Actual Pretrain Comm Cost: 211.84 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.5010
Round 2: Global Test Accuracy = 0.5840
Round 3: Global Test Accuracy = 0.7060
Round 4: Global Test Accuracy = 0.7070
Round 5: Global Test Accuracy = 0.7510
Round 6: Global Test Accuracy = 0.7590
Round 7: Global Test Accuracy = 0.7840
Round 8: Global Test Accuracy = 0.7730
Round 9: Global Test Accuracy = 0.7850
Round 10: Global Test Accuracy = 0.7830
Round 11: Global Test Accuracy = 0.7910
Round 12: Global Test Accuracy = 0.7870
Round 13: Global Test Accuracy = 0.8020
Round 14: Global Test Accuracy = 0.7910
Round 15: Global Test Accuracy = 0.7910
Round 16: Global Test Accuracy = 0.7840
Round 17: Global Test Accuracy = 0.7960
Round 18: Global Test Accuracy = 0.7790
Round 19: Global Test Accuracy = 0.8010
Round 20: Global Test Accuracy = 0.7930
Round 21: Global Test Accuracy = 0.7900
Round 22: Global Test Accuracy = 0.7900
Round 23: Global Test Accuracy = 0.7970
Round 24: Global Test Accuracy = 0.7960
Round 25: Global Test Accuracy = 0.7890
Round 26: Global Test Accuracy = 0.7910
Round 27: Global Test Accuracy = 0.7900
Round 28: Global Test Accuracy = 0.7940
Round 29: Global Test Accuracy = 0.7930
Round 30: Global Test Accuracy = 0.7900
Round 31: Global Test Accuracy = 0.7920
Round 32: Global Test Accuracy = 0.7960
Round 33: Global Test Accuracy = 0.7910
Round 34: Global Test Accuracy = 0.7880
Round 35: Global Test Accuracy = 0.7910
Round 36: Global Test Accuracy = 0.7920
Round 37: Global Test Accuracy = 0.7970
Round 38: Global Test Accuracy = 0.7930
Round 39: Global Test Accuracy = 0.7930
Round 40: Global Test Accuracy = 0.7940
Round 41: Global Test Accuracy = 0.7940
Round 42: Global Test Accuracy = 0.7950
Round 43: Global Test Accuracy = 0.7940
Round 44: Global Test Accuracy = 0.7950
Round 45: Global Test Accuracy = 0.7900
Round 46: Global Test Accuracy = 0.7910
Round 47: Global Test Accuracy = 0.7960
Round 48: Global Test Accuracy = 0.7980
Round 49: Global Test Accuracy = 0.7930
Round 50: Global Test Accuracy = 0.7930
Round 51: Global Test Accuracy = 0.7930
Round 52: Global Test Accuracy = 0.7850
Round 53: Global Test Accuracy = 0.7930
Round 54: Global Test Accuracy = 0.7950
Round 55: Global Test Accuracy = 0.7930
Round 56: Global Test Accuracy = 0.7930
Round 57: Global Test Accuracy = 0.7950
Round 58: Global Test Accuracy = 0.7900
Round 59: Global Test Accuracy = 0.7950
Round 60: Global Test Accuracy = 0.7950
Round 61: Global Test Accuracy = 0.7950
Round 62: Global Test Accuracy = 0.7960
Round 63: Global Test Accuracy = 0.7960
Round 64: Global Test Accuracy = 0.7970
Round 65: Global Test Accuracy = 0.7920
Round 66: Global Test Accuracy = 0.7930
Round 67: Global Test Accuracy = 0.7950
Round 68: Global Test Accuracy = 0.7940
Round 69: Global Test Accuracy = 0.7950
Round 70: Global Test Accuracy = 0.7950
Round 71: Global Test Accuracy = 0.7950
Round 72: Global Test Accuracy = 0.7940
Round 73: Global Test Accuracy = 0.7930
Round 74: Global Test Accuracy = 0.7950
Round 75: Global Test Accuracy = 0.7920
Round 76: Global Test Accuracy = 0.7910
Round 77: Global Test Accuracy = 0.7930
Round 78: Global Test Accuracy = 0.7920
Round 79: Global Test Accuracy = 0.7930
Round 80: Global Test Accuracy = 0.7910
Round 81: Global Test Accuracy = 0.7920
Round 82: Global Test Accuracy = 0.7940
Round 83: Global Test Accuracy = 0.7970
Round 84: Global Test Accuracy = 0.7960
Round 85: Global Test Accuracy = 0.7960
Round 86: Global Test Accuracy = 0.7950
Round 87: Global Test Accuracy = 0.7960
Round 88: Global Test Accuracy = 0.7940
Round 89: Global Test Accuracy = 0.7950
Round 90: Global Test Accuracy = 0.7940
Round 91: Global Test Accuracy = 0.7940
Round 92: Global Test Accuracy = 0.7920
Round 93: Global Test Accuracy = 0.7970
Round 94: Global Test Accuracy = 0.7930
Round 95: Global Test Accuracy = 0.7910
Round 96: Global Test Accuracy = 0.7890
Round 97: Global Test Accuracy = 0.7940
Round 98: Global Test Accuracy = 0.7910
Round 99: Global Test Accuracy = 0.7890
Round 100: Global Test Accuracy = 0.7940
Round 101: Global Test Accuracy = 0.7920
Round 102: Global Test Accuracy = 0.7880
Round 103: Global Test Accuracy = 0.7880
Round 104: Global Test Accuracy = 0.7900
Round 105: Global Test Accuracy = 0.7870
Round 106: Global Test Accuracy = 0.7890
Round 107: Global Test Accuracy = 0.7870
Round 108: Global Test Accuracy = 0.7900
Round 109: Global Test Accuracy = 0.7940
Round 110: Global Test Accuracy = 0.7930
Round 111: Global Test Accuracy = 0.7910
Round 112: Global Test Accuracy = 0.7950
Round 113: Global Test Accuracy = 0.7890
Round 114: Global Test Accuracy = 0.7900
Round 115: Global Test Accuracy = 0.7900
Round 116: Global Test Accuracy = 0.7930
Round 117: Global Test Accuracy = 0.7910
Round 118: Global Test Accuracy = 0.7940
Round 119: Global Test Accuracy = 0.7930
Round 120: Global Test Accuracy = 0.7910
Round 121: Global Test Accuracy = 0.7900
Round 122: Global Test Accuracy = 0.7930
Round 123: Global Test Accuracy = 0.7930
Round 124: Global Test Accuracy = 0.7920
Round 125: Global Test Accuracy = 0.7920
Round 126: Global Test Accuracy = 0.7900
Round 127: Global Test Accuracy = 0.7910
Round 128: Global Test Accuracy = 0.7910
Round 129: Global Test Accuracy = 0.7920
Round 130: Global Test Accuracy = 0.7910
Round 131: Global Test Accuracy = 0.7880
Round 132: Global Test Accuracy = 0.7910
Round 133: Global Test Accuracy = 0.7920
Round 134: Global Test Accuracy = 0.7900
Round 135: Global Test Accuracy = 0.7910
Round 136: Global Test Accuracy = 0.7910
Round 137: Global Test Accuracy = 0.7910
Round 138: Global Test Accuracy = 0.7910
Round 139: Global Test Accuracy = 0.7900
Round 140: Global Test Accuracy = 0.7900
Round 141: Global Test Accuracy = 0.7890
Round 142: Global Test Accuracy = 0.7890
Round 143: Global Test Accuracy = 0.7870
Round 144: Global Test Accuracy = 0.7900
Round 145: Global Test Accuracy = 0.7880
Round 146: Global Test Accuracy = 0.7900
Round 147: Global Test Accuracy = 0.7890
Round 148: Global Test Accuracy = 0.7870
Round 149: Global Test Accuracy = 0.7890
Round 150: Global Test Accuracy = 0.7880
Round 151: Global Test Accuracy = 0.7860
Round 152: Global Test Accuracy = 0.7880
Round 153: Global Test Accuracy = 0.7880
Round 154: Global Test Accuracy = 0.7870
Round 155: Global Test Accuracy = 0.7890
Round 156: Global Test Accuracy = 0.7900
Round 157: Global Test Accuracy = 0.7910
Round 158: Global Test Accuracy = 0.7900
Round 159: Global Test Accuracy = 0.7890
Round 160: Global Test Accuracy = 0.7910
Round 161: Global Test Accuracy = 0.7920
Round 162: Global Test Accuracy = 0.7920
Round 163: Global Test Accuracy = 0.7920
Round 164: Global Test Accuracy = 0.7930
Round 165: Global Test Accuracy = 0.7930
Round 166: Global Test Accuracy = 0.7920
Round 167: Global Test Accuracy = 0.7920
Round 168: Global Test Accuracy = 0.7910
Round 169: Global Test Accuracy = 0.7930
Round 170: Global Test Accuracy = 0.7890
Round 171: Global Test Accuracy = 0.7900
Round 172: Global Test Accuracy = 0.7910
Round 173: Global Test Accuracy = 0.7870
Round 174: Global Test Accuracy = 0.7870
Round 175: Global Test Accuracy = 0.7870
Round 176: Global Test Accuracy = 0.7910
Round 177: Global Test Accuracy = 0.7890
Round 178: Global Test Accuracy = 0.7860
Round 179: Global Test Accuracy = 0.7870
Round 180: Global Test Accuracy = 0.7860
Round 181: Global Test Accuracy = 0.7880
Round 182: Global Test Accuracy = 0.7880
Round 183: Global Test Accuracy = 0.7880
Round 184: Global Test Accuracy = 0.7880
Round 185: Global Test Accuracy = 0.7880
Round 186: Global Test Accuracy = 0.7900
Round 187: Global Test Accuracy = 0.7910
Round 188: Global Test Accuracy = 0.7880
Round 189: Global Test Accuracy = 0.7880
Round 190: Global Test Accuracy = 0.7860
Round 191: Global Test Accuracy = 0.7870
Round 192: Global Test Accuracy = 0.7860
Round 193: Global Test Accuracy = 0.7860
Round 194: Global Test Accuracy = 0.7890
Round 195: Global Test Accuracy = 0.7880
Round 196: Global Test Accuracy = 0.7860
Round 197: Global Test Accuracy = 0.7880
Round 198: Global Test Accuracy = 0.7880
Round 199: Global Test Accuracy = 0.7900
Round 200: Global Test Accuracy = 0.7860
//train_time: 4812.53 ms//end
//Log Max memory for Large1: 6582407168.0 //end
//Log Max memory for Large2: 5833809920.0 //end
//Log Max memory for Large3: 6351224832.0 //end
//Log Max memory for Large4: 6187368448.0 //end
//Log Max memory for Server: 18116923392.0 //end
//Log Large1 network: 58522998.0 //end
//Log Large2 network: 39244645.0 //end
//Log Large3 network: 58528352.0 //end
//Log Large4 network: 39216770.0 //end
//Log Server network: 195313289.0 //end
//Log Total Actual Train Comm Cost: 372.72 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 0.8047314708828927
Average test accuracy, 0.786
//Log Theoretical Pretrain Comm Cost: 203.04 MB //end
//Log Theoretical Train Comm Cost: 351.91 MB //end
[36m(Trainer pid=112109, ip=192.168.39.156)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=112109, ip=192.168.39.156)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: cora, Trainers: 10, Distribution: average, IID Beta: 10.0, Hops: 1, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'cora', 'method': 'fedgcn', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 1, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/cora/raw/ind.cora.x
File already exists: ./data/cora/raw/ind.cora.tx
File already exists: ./data/cora/raw/ind.cora.allx
File already exists: ./data/cora/raw/ind.cora.y
File already exists: ./data/cora/raw/ind.cora.ty
File already exists: ./data/cora/raw/ind.cora.ally
File already exists: ./data/cora/raw/ind.cora.graph
File already exists: ./data/cora/raw/ind.cora.test.index
Initialization start: network data collected.
2025-05-15 02:51:29,899	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 02:51:29,899	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 02:51:29,906	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
[36m(Trainer pid=112825, ip=192.168.14.54)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=112825, ip=192.168.14.54)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 6155.298 ms //end
//Log Large1 init network: 118414.0 //end
//Log Large2 init network: 122414.0 //end
//Log Large3 init network: 150627.0 //end
//Log Large4 init network: 128168.0 //end
//Log Server init network: 17541090.0 //end
//Log Initialization Communication Cost (MB): 17.22 //end
Pretrain start time recorded.
server aggregates all local neighbor feature sums
clients received feature aggregation from server
//pretrain_time: 395.02700000000004 ms//end
//Log Max memory for Large1: 6121291776.0 //end
//Log Max memory for Large2: 6254641152.0 //end
//Log Max memory for Large3: 5907210240.0 //end
//Log Max memory for Large4: 6614339584.0 //end
//Log Max memory for Server: 18118955008.0 //end
//Log Large1 network: 32415039.0 //end
//Log Large2 network: 47934671.0 //end
//Log Large3 network: 32466353.0 //end
//Log Large4 network: 48606441.0 //end
//Log Server network: 57932237.0 //end
//Log Total Actual Pretrain Comm Cost: 209.19 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.5160
Round 2: Global Test Accuracy = 0.5770
Round 3: Global Test Accuracy = 0.6520
Round 4: Global Test Accuracy = 0.6680
Round 5: Global Test Accuracy = 0.7310
Round 6: Global Test Accuracy = 0.7310
Round 7: Global Test Accuracy = 0.7720
Round 8: Global Test Accuracy = 0.7560
Round 9: Global Test Accuracy = 0.7750
Round 10: Global Test Accuracy = 0.7800
Round 11: Global Test Accuracy = 0.7710
Round 12: Global Test Accuracy = 0.7790
Round 13: Global Test Accuracy = 0.7730
Round 14: Global Test Accuracy = 0.7910
Round 15: Global Test Accuracy = 0.7810
Round 16: Global Test Accuracy = 0.7890
Round 17: Global Test Accuracy = 0.7970
Round 18: Global Test Accuracy = 0.7930
Round 19: Global Test Accuracy = 0.7870
Round 20: Global Test Accuracy = 0.7970
Round 21: Global Test Accuracy = 0.7810
Round 22: Global Test Accuracy = 0.7900
Round 23: Global Test Accuracy = 0.7910
Round 24: Global Test Accuracy = 0.7870
Round 25: Global Test Accuracy = 0.7880
Round 26: Global Test Accuracy = 0.7890
Round 27: Global Test Accuracy = 0.7830
Round 28: Global Test Accuracy = 0.7850
Round 29: Global Test Accuracy = 0.7840
Round 30: Global Test Accuracy = 0.7870
Round 31: Global Test Accuracy = 0.7780
Round 32: Global Test Accuracy = 0.7810
Round 33: Global Test Accuracy = 0.7860
Round 34: Global Test Accuracy = 0.7790
Round 35: Global Test Accuracy = 0.7940
Round 36: Global Test Accuracy = 0.7870
Round 37: Global Test Accuracy = 0.7940
Round 38: Global Test Accuracy = 0.7850
Round 39: Global Test Accuracy = 0.7930
Round 40: Global Test Accuracy = 0.7860
Round 41: Global Test Accuracy = 0.7840
Round 42: Global Test Accuracy = 0.7880
Round 43: Global Test Accuracy = 0.7790
Round 44: Global Test Accuracy = 0.7790
Round 45: Global Test Accuracy = 0.7830
Round 46: Global Test Accuracy = 0.7760
Round 47: Global Test Accuracy = 0.7980
Round 48: Global Test Accuracy = 0.7820
Round 49: Global Test Accuracy = 0.7930
Round 50: Global Test Accuracy = 0.7840
Round 51: Global Test Accuracy = 0.7870
Round 52: Global Test Accuracy = 0.7810
Round 53: Global Test Accuracy = 0.7850
Round 54: Global Test Accuracy = 0.7850
Round 55: Global Test Accuracy = 0.7850
Round 56: Global Test Accuracy = 0.7910
Round 57: Global Test Accuracy = 0.7940
Round 58: Global Test Accuracy = 0.7870
Round 59: Global Test Accuracy = 0.7830
Round 60: Global Test Accuracy = 0.7900
Round 61: Global Test Accuracy = 0.7890
Round 62: Global Test Accuracy = 0.7890
Round 63: Global Test Accuracy = 0.7890
Round 64: Global Test Accuracy = 0.7850
Round 65: Global Test Accuracy = 0.7880
Round 66: Global Test Accuracy = 0.7870
Round 67: Global Test Accuracy = 0.7880
Round 68: Global Test Accuracy = 0.7900
Round 69: Global Test Accuracy = 0.7860
Round 70: Global Test Accuracy = 0.7930
Round 71: Global Test Accuracy = 0.7840
Round 72: Global Test Accuracy = 0.7780
Round 73: Global Test Accuracy = 0.7840
Round 74: Global Test Accuracy = 0.7870
Round 75: Global Test Accuracy = 0.7900
Round 76: Global Test Accuracy = 0.7850
Round 77: Global Test Accuracy = 0.7860
Round 78: Global Test Accuracy = 0.7820
Round 79: Global Test Accuracy = 0.7790
Round 80: Global Test Accuracy = 0.7860
Round 81: Global Test Accuracy = 0.7890
Round 82: Global Test Accuracy = 0.7870
Round 83: Global Test Accuracy = 0.7890
Round 84: Global Test Accuracy = 0.7910
Round 85: Global Test Accuracy = 0.7930
Round 86: Global Test Accuracy = 0.7840
Round 87: Global Test Accuracy = 0.7850
Round 88: Global Test Accuracy = 0.7810
Round 89: Global Test Accuracy = 0.7810
Round 90: Global Test Accuracy = 0.7780
Round 91: Global Test Accuracy = 0.7830
Round 92: Global Test Accuracy = 0.7850
Round 93: Global Test Accuracy = 0.7850
Round 94: Global Test Accuracy = 0.7800
Round 95: Global Test Accuracy = 0.7840
Round 96: Global Test Accuracy = 0.7840
Round 97: Global Test Accuracy = 0.7820
Round 98: Global Test Accuracy = 0.7830
Round 99: Global Test Accuracy = 0.7840
Round 100: Global Test Accuracy = 0.7840
Round 101: Global Test Accuracy = 0.7840
Round 102: Global Test Accuracy = 0.7820
Round 103: Global Test Accuracy = 0.7860
Round 104: Global Test Accuracy = 0.7890
Round 105: Global Test Accuracy = 0.7880
Round 106: Global Test Accuracy = 0.7900
Round 107: Global Test Accuracy = 0.7870
Round 108: Global Test Accuracy = 0.7890
Round 109: Global Test Accuracy = 0.7860
Round 110: Global Test Accuracy = 0.7880
Round 111: Global Test Accuracy = 0.7860
Round 112: Global Test Accuracy = 0.7850
Round 113: Global Test Accuracy = 0.7870
Round 114: Global Test Accuracy = 0.7840
Round 115: Global Test Accuracy = 0.7840
Round 116: Global Test Accuracy = 0.7870
Round 117: Global Test Accuracy = 0.7870
Round 118: Global Test Accuracy = 0.7820
Round 119: Global Test Accuracy = 0.7860
Round 120: Global Test Accuracy = 0.7860
Round 121: Global Test Accuracy = 0.7860
Round 122: Global Test Accuracy = 0.7830
Round 123: Global Test Accuracy = 0.7790
Round 124: Global Test Accuracy = 0.7800
Round 125: Global Test Accuracy = 0.7810
Round 126: Global Test Accuracy = 0.7840
Round 127: Global Test Accuracy = 0.7870
Round 128: Global Test Accuracy = 0.7860
Round 129: Global Test Accuracy = 0.7880
Round 130: Global Test Accuracy = 0.7870
Round 131: Global Test Accuracy = 0.7850
Round 132: Global Test Accuracy = 0.7840
Round 133: Global Test Accuracy = 0.7830
Round 134: Global Test Accuracy = 0.7850
Round 135: Global Test Accuracy = 0.7850
Round 136: Global Test Accuracy = 0.7840
Round 137: Global Test Accuracy = 0.7840
Round 138: Global Test Accuracy = 0.7830
Round 139: Global Test Accuracy = 0.7850
Round 140: Global Test Accuracy = 0.7860
Round 141: Global Test Accuracy = 0.7830
Round 142: Global Test Accuracy = 0.7850
Round 143: Global Test Accuracy = 0.7850
Round 144: Global Test Accuracy = 0.7820
Round 145: Global Test Accuracy = 0.7860
Round 146: Global Test Accuracy = 0.7870
Round 147: Global Test Accuracy = 0.7850
Round 148: Global Test Accuracy = 0.7840
Round 149: Global Test Accuracy = 0.7850
Round 150: Global Test Accuracy = 0.7840
Round 151: Global Test Accuracy = 0.7860
Round 152: Global Test Accuracy = 0.7850
Round 153: Global Test Accuracy = 0.7860
Round 154: Global Test Accuracy = 0.7870
Round 155: Global Test Accuracy = 0.7850
Round 156: Global Test Accuracy = 0.7880
Round 157: Global Test Accuracy = 0.7870
Round 158: Global Test Accuracy = 0.7840
Round 159: Global Test Accuracy = 0.7870
Round 160: Global Test Accuracy = 0.7870
Round 161: Global Test Accuracy = 0.7850
Round 162: Global Test Accuracy = 0.7790
Round 163: Global Test Accuracy = 0.7790
Round 164: Global Test Accuracy = 0.7830
Round 165: Global Test Accuracy = 0.7830
Round 166: Global Test Accuracy = 0.7820
Round 167: Global Test Accuracy = 0.7850
Round 168: Global Test Accuracy = 0.7840
Round 169: Global Test Accuracy = 0.7850
Round 170: Global Test Accuracy = 0.7830
Round 171: Global Test Accuracy = 0.7810
Round 172: Global Test Accuracy = 0.7850
Round 173: Global Test Accuracy = 0.7820
Round 174: Global Test Accuracy = 0.7820
Round 175: Global Test Accuracy = 0.7840
Round 176: Global Test Accuracy = 0.7840
Round 177: Global Test Accuracy = 0.7810
Round 178: Global Test Accuracy = 0.7840
Round 179: Global Test Accuracy = 0.7820
Round 180: Global Test Accuracy = 0.7830
Round 181: Global Test Accuracy = 0.7810
Round 182: Global Test Accuracy = 0.7800
Round 183: Global Test Accuracy = 0.7800
Round 184: Global Test Accuracy = 0.7820
Round 185: Global Test Accuracy = 0.7800
Round 186: Global Test Accuracy = 0.7810
Round 187: Global Test Accuracy = 0.7820
Round 188: Global Test Accuracy = 0.7810
Round 189: Global Test Accuracy = 0.7820
Round 190: Global Test Accuracy = 0.7810
Round 191: Global Test Accuracy = 0.7820
Round 192: Global Test Accuracy = 0.7810
Round 193: Global Test Accuracy = 0.7820
Round 194: Global Test Accuracy = 0.7830
Round 195: Global Test Accuracy = 0.7830
Round 196: Global Test Accuracy = 0.7830
Round 197: Global Test Accuracy = 0.7820
Round 198: Global Test Accuracy = 0.7810
Round 199: Global Test Accuracy = 0.7800
Round 200: Global Test Accuracy = 0.7790
//train_time: 4658.375 ms//end
//Log Max memory for Large1: 6135218176.0 //end
//Log Max memory for Large2: 6282272768.0 //end
//Log Max memory for Large3: 5926735872.0 //end
//Log Max memory for Large4: 6642380800.0 //end
//Log Max memory for Server: 18127630336.0 //end
//Log Large1 network: 39208715.0 //end
//Log Large2 network: 58558291.0 //end
//Log Large3 network: 39156742.0 //end
//Log Large4 network: 58493569.0 //end
//Log Server network: 195308116.0 //end
//Log Total Actual Train Comm Cost: 372.62 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 0.823733934879303
Average test accuracy, 0.779
//Log Theoretical Pretrain Comm Cost: 201.03 MB //end
//Log Theoretical Train Comm Cost: 351.91 MB //end
[36m(Trainer pid=116718, ip=192.168.14.62)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=116718, ip=192.168.14.62)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: citeseer, Trainers: 10, Distribution: average, IID Beta: 10000.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'citeseer', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 10000.0, 'distribution_type': 'average', 'gpu': False}
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x to ./data/citeseer/raw/ind.citeseer.x...
Downloaded ./data/citeseer/raw/ind.citeseer.x
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx to ./data/citeseer/raw/ind.citeseer.tx...
Downloaded ./data/citeseer/raw/ind.citeseer.tx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx to ./data/citeseer/raw/ind.citeseer.allx...
Downloaded ./data/citeseer/raw/ind.citeseer.allx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y to ./data/citeseer/raw/ind.citeseer.y...
Downloaded ./data/citeseer/raw/ind.citeseer.y
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty to ./data/citeseer/raw/ind.citeseer.ty...
Downloaded ./data/citeseer/raw/ind.citeseer.ty
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally to ./data/citeseer/raw/ind.citeseer.ally...
Downloaded ./data/citeseer/raw/ind.citeseer.ally
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph to ./data/citeseer/raw/ind.citeseer.graph...
Downloaded ./data/citeseer/raw/ind.citeseer.graph
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index to ./data/citeseer/raw/ind.citeseer.test.index...
Downloaded ./data/citeseer/raw/ind.citeseer.test.index
Initialization start: network data collected.
2025-05-15 02:52:48,134	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 02:52:48,134	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 02:52:48,142	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=113321, ip=192.168.14.54)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=113321, ip=192.168.14.54)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5756.894 ms //end
//Log Large1 init network: 136409.0 //end
//Log Large2 init network: 133298.0 //end
//Log Large3 init network: 175883.0 //end
//Log Large4 init network: 164420.0 //end
//Log Server init network: 50151806.0 //end
//Log Initialization Communication Cost (MB): 48.41 //end
Pretrain start time recorded.
//pretrain_time: 4.36 ms//end
//Log Max memory for Large1: 6536224768.0 //end
//Log Max memory for Large2: 5802364928.0 //end
//Log Max memory for Large3: 6296158208.0 //end
//Log Max memory for Large4: 6165278720.0 //end
//Log Max memory for Server: 18176086016.0 //end
//Log Large1 network: 766388.0 //end
//Log Large2 network: 752103.0 //end
//Log Large3 network: 766097.0 //end
//Log Large4 network: 649420.0 //end
//Log Server network: 3564348.0 //end
//Log Total Actual Pretrain Comm Cost: 6.20 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1620
Round 2: Global Test Accuracy = 0.1730
Round 3: Global Test Accuracy = 0.1760
Round 4: Global Test Accuracy = 0.1770
Round 5: Global Test Accuracy = 0.1770
Round 6: Global Test Accuracy = 0.1840
Round 7: Global Test Accuracy = 0.1880
Round 8: Global Test Accuracy = 0.1970
Round 9: Global Test Accuracy = 0.1930
Round 10: Global Test Accuracy = 0.2000
Round 11: Global Test Accuracy = 0.2060
Round 12: Global Test Accuracy = 0.2160
Round 13: Global Test Accuracy = 0.2190
Round 14: Global Test Accuracy = 0.2250
Round 15: Global Test Accuracy = 0.2300
Round 16: Global Test Accuracy = 0.2320
Round 17: Global Test Accuracy = 0.2380
Round 18: Global Test Accuracy = 0.2470
Round 19: Global Test Accuracy = 0.2590
Round 20: Global Test Accuracy = 0.2650
Round 21: Global Test Accuracy = 0.2750
Round 22: Global Test Accuracy = 0.2870
Round 23: Global Test Accuracy = 0.2950
Round 24: Global Test Accuracy = 0.2960
Round 25: Global Test Accuracy = 0.3030
Round 26: Global Test Accuracy = 0.3170
Round 27: Global Test Accuracy = 0.3130
Round 28: Global Test Accuracy = 0.3170
Round 29: Global Test Accuracy = 0.3230
Round 30: Global Test Accuracy = 0.3250
Round 31: Global Test Accuracy = 0.3380
Round 32: Global Test Accuracy = 0.3540
Round 33: Global Test Accuracy = 0.3530
Round 34: Global Test Accuracy = 0.3500
Round 35: Global Test Accuracy = 0.3710
Round 36: Global Test Accuracy = 0.3710
Round 37: Global Test Accuracy = 0.3820
Round 38: Global Test Accuracy = 0.3850
Round 39: Global Test Accuracy = 0.3950
Round 40: Global Test Accuracy = 0.3990
Round 41: Global Test Accuracy = 0.4020
Round 42: Global Test Accuracy = 0.4100
Round 43: Global Test Accuracy = 0.4150
Round 44: Global Test Accuracy = 0.4230
Round 45: Global Test Accuracy = 0.4270
Round 46: Global Test Accuracy = 0.4300
Round 47: Global Test Accuracy = 0.4400
Round 48: Global Test Accuracy = 0.4400
Round 49: Global Test Accuracy = 0.4490
Round 50: Global Test Accuracy = 0.4460
Round 51: Global Test Accuracy = 0.4580
Round 52: Global Test Accuracy = 0.4580
Round 53: Global Test Accuracy = 0.4660
Round 54: Global Test Accuracy = 0.4660
Round 55: Global Test Accuracy = 0.4650
Round 56: Global Test Accuracy = 0.4710
Round 57: Global Test Accuracy = 0.4720
Round 58: Global Test Accuracy = 0.4780
Round 59: Global Test Accuracy = 0.4780
Round 60: Global Test Accuracy = 0.4810
Round 61: Global Test Accuracy = 0.4880
Round 62: Global Test Accuracy = 0.4830
Round 63: Global Test Accuracy = 0.4880
Round 64: Global Test Accuracy = 0.4900
Round 65: Global Test Accuracy = 0.4900
Round 66: Global Test Accuracy = 0.4900
Round 67: Global Test Accuracy = 0.4900
Round 68: Global Test Accuracy = 0.4890
Round 69: Global Test Accuracy = 0.4950
Round 70: Global Test Accuracy = 0.4960
Round 71: Global Test Accuracy = 0.5000
Round 72: Global Test Accuracy = 0.4960
Round 73: Global Test Accuracy = 0.4960
Round 74: Global Test Accuracy = 0.4970
Round 75: Global Test Accuracy = 0.4970
Round 76: Global Test Accuracy = 0.5000
Round 77: Global Test Accuracy = 0.5020
Round 78: Global Test Accuracy = 0.5010
Round 79: Global Test Accuracy = 0.5060
Round 80: Global Test Accuracy = 0.5080
Round 81: Global Test Accuracy = 0.5090
Round 82: Global Test Accuracy = 0.5060
Round 83: Global Test Accuracy = 0.5030
Round 84: Global Test Accuracy = 0.5030
Round 85: Global Test Accuracy = 0.5060
Round 86: Global Test Accuracy = 0.5000
Round 87: Global Test Accuracy = 0.4990
Round 88: Global Test Accuracy = 0.5120
Round 89: Global Test Accuracy = 0.5060
Round 90: Global Test Accuracy = 0.5080
Round 91: Global Test Accuracy = 0.5030
Round 92: Global Test Accuracy = 0.5110
Round 93: Global Test Accuracy = 0.5080
Round 94: Global Test Accuracy = 0.5130
Round 95: Global Test Accuracy = 0.5180
Round 96: Global Test Accuracy = 0.5140
Round 97: Global Test Accuracy = 0.5180
Round 98: Global Test Accuracy = 0.5160
Round 99: Global Test Accuracy = 0.5190
Round 100: Global Test Accuracy = 0.5160
Round 101: Global Test Accuracy = 0.5190
Round 102: Global Test Accuracy = 0.5170
Round 103: Global Test Accuracy = 0.5180
Round 104: Global Test Accuracy = 0.5180
Round 105: Global Test Accuracy = 0.5220
Round 106: Global Test Accuracy = 0.5190
Round 107: Global Test Accuracy = 0.5200
Round 108: Global Test Accuracy = 0.5230
Round 109: Global Test Accuracy = 0.5200
Round 110: Global Test Accuracy = 0.5260
Round 111: Global Test Accuracy = 0.5250
Round 112: Global Test Accuracy = 0.5260
Round 113: Global Test Accuracy = 0.5240
Round 114: Global Test Accuracy = 0.5240
Round 115: Global Test Accuracy = 0.5210
Round 116: Global Test Accuracy = 0.5220
Round 117: Global Test Accuracy = 0.5280
Round 118: Global Test Accuracy = 0.5260
Round 119: Global Test Accuracy = 0.5230
Round 120: Global Test Accuracy = 0.5240
Round 121: Global Test Accuracy = 0.5280
Round 122: Global Test Accuracy = 0.5220
Round 123: Global Test Accuracy = 0.5310
Round 124: Global Test Accuracy = 0.5320
Round 125: Global Test Accuracy = 0.5260
Round 126: Global Test Accuracy = 0.5290
Round 127: Global Test Accuracy = 0.5300
Round 128: Global Test Accuracy = 0.5340
Round 129: Global Test Accuracy = 0.5350
Round 130: Global Test Accuracy = 0.5360
Round 131: Global Test Accuracy = 0.5350
Round 132: Global Test Accuracy = 0.5360
Round 133: Global Test Accuracy = 0.5340
Round 134: Global Test Accuracy = 0.5330
Round 135: Global Test Accuracy = 0.5310
Round 136: Global Test Accuracy = 0.5340
Round 137: Global Test Accuracy = 0.5360
Round 138: Global Test Accuracy = 0.5370
Round 139: Global Test Accuracy = 0.5360
Round 140: Global Test Accuracy = 0.5330
Round 141: Global Test Accuracy = 0.5340
Round 142: Global Test Accuracy = 0.5410
Round 143: Global Test Accuracy = 0.5400
Round 144: Global Test Accuracy = 0.5410
Round 145: Global Test Accuracy = 0.5420
Round 146: Global Test Accuracy = 0.5380
Round 147: Global Test Accuracy = 0.5420
Round 148: Global Test Accuracy = 0.5430
Round 149: Global Test Accuracy = 0.5410
Round 150: Global Test Accuracy = 0.5430
Round 151: Global Test Accuracy = 0.5380
Round 152: Global Test Accuracy = 0.5410
Round 153: Global Test Accuracy = 0.5420
Round 154: Global Test Accuracy = 0.5400
Round 155: Global Test Accuracy = 0.5420
Round 156: Global Test Accuracy = 0.5450
Round 157: Global Test Accuracy = 0.5470
Round 158: Global Test Accuracy = 0.5470
Round 159: Global Test Accuracy = 0.5440
Round 160: Global Test Accuracy = 0.5480
Round 161: Global Test Accuracy = 0.5480
Round 162: Global Test Accuracy = 0.5470
Round 163: Global Test Accuracy = 0.5470
Round 164: Global Test Accuracy = 0.5470
Round 165: Global Test Accuracy = 0.5450
Round 166: Global Test Accuracy = 0.5450
Round 167: Global Test Accuracy = 0.5450
Round 168: Global Test Accuracy = 0.5440
Round 169: Global Test Accuracy = 0.5450
Round 170: Global Test Accuracy = 0.5470
Round 171: Global Test Accuracy = 0.5460
Round 172: Global Test Accuracy = 0.5450
Round 173: Global Test Accuracy = 0.5440
Round 174: Global Test Accuracy = 0.5420
Round 175: Global Test Accuracy = 0.5450
Round 176: Global Test Accuracy = 0.5450
Round 177: Global Test Accuracy = 0.5410
Round 178: Global Test Accuracy = 0.5470
Round 179: Global Test Accuracy = 0.5460
Round 180: Global Test Accuracy = 0.5480
Round 181: Global Test Accuracy = 0.5460
Round 182: Global Test Accuracy = 0.5480
Round 183: Global Test Accuracy = 0.5470
Round 184: Global Test Accuracy = 0.5490
Round 185: Global Test Accuracy = 0.5470
Round 186: Global Test Accuracy = 0.5480
Round 187: Global Test Accuracy = 0.5460
Round 188: Global Test Accuracy = 0.5460
Round 189: Global Test Accuracy = 0.5500
Round 190: Global Test Accuracy = 0.5460
Round 191: Global Test Accuracy = 0.5450
Round 192: Global Test Accuracy = 0.5470
Round 193: Global Test Accuracy = 0.5500
Round 194: Global Test Accuracy = 0.5490
Round 195: Global Test Accuracy = 0.5490
Round 196: Global Test Accuracy = 0.5470
Round 197: Global Test Accuracy = 0.5530
Round 198: Global Test Accuracy = 0.5480
Round 199: Global Test Accuracy = 0.5530
Round 200: Global Test Accuracy = 0.5530
//train_time: 12797.582 ms//end
//Log Max memory for Large1: 6451494912.0 //end
//Log Max memory for Large2: 5728403456.0 //end
//Log Max memory for Large3: 6212448256.0 //end
//Log Max memory for Large4: 6108147712.0 //end
//Log Max memory for Server: 18074124288.0 //end
//Log Large1 network: 148399125.0 //end
//Log Large2 network: 99344078.0 //end
//Log Large3 network: 148364391.0 //end
//Log Large4 network: 99209640.0 //end
//Log Server network: 493771927.0 //end
//Log Total Actual Train Comm Cost: 943.27 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 1.2488017357587815
Average test accuracy, 0.553
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 905.85 MB //end
[36m(Trainer pid=113160, ip=192.168.39.156)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=113160, ip=192.168.39.156)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: citeseer, Trainers: 10, Distribution: average, IID Beta: 100.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'citeseer', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 100.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/citeseer/raw/ind.citeseer.x
File already exists: ./data/citeseer/raw/ind.citeseer.tx
File already exists: ./data/citeseer/raw/ind.citeseer.allx
File already exists: ./data/citeseer/raw/ind.citeseer.y
File already exists: ./data/citeseer/raw/ind.citeseer.ty
File already exists: ./data/citeseer/raw/ind.citeseer.ally
File already exists: ./data/citeseer/raw/ind.citeseer.graph
File already exists: ./data/citeseer/raw/ind.citeseer.test.index
Initialization start: network data collected.
2025-05-15 02:54:12,779	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 02:54:12,779	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 02:54:12,786	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=113813, ip=192.168.42.57)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=113813, ip=192.168.42.57)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 6092.191 ms //end
//Log Large1 init network: 157810.0 //end
//Log Large2 init network: 193898.0 //end
//Log Large3 init network: 112890.0 //end
//Log Large4 init network: 148141.0 //end
//Log Server init network: 50131945.0 //end
//Log Initialization Communication Cost (MB): 48.39 //end
Pretrain start time recorded.
//pretrain_time: 4.872999999999999 ms//end
//Log Max memory for Large1: 6023237632.0 //end
//Log Max memory for Large2: 6153871360.0 //end
//Log Max memory for Large3: 5772029952.0 //end
//Log Max memory for Large4: 6525435904.0 //end
//Log Max memory for Server: 18089254912.0 //end
//Log Large1 network: 628496.0 //end
//Log Large2 network: 808604.0 //end
//Log Large3 network: 677046.0 //end
//Log Large4 network: 817148.0 //end
//Log Server network: 3406557.0 //end
//Log Total Actual Pretrain Comm Cost: 6.04 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1640
Round 2: Global Test Accuracy = 0.1710
Round 3: Global Test Accuracy = 0.1800
Round 4: Global Test Accuracy = 0.1870
Round 5: Global Test Accuracy = 0.1990
Round 6: Global Test Accuracy = 0.2010
Round 7: Global Test Accuracy = 0.2090
Round 8: Global Test Accuracy = 0.2160
Round 9: Global Test Accuracy = 0.2180
Round 10: Global Test Accuracy = 0.2200
Round 11: Global Test Accuracy = 0.2330
Round 12: Global Test Accuracy = 0.2410
Round 13: Global Test Accuracy = 0.2380
Round 14: Global Test Accuracy = 0.2460
Round 15: Global Test Accuracy = 0.2460
Round 16: Global Test Accuracy = 0.2520
Round 17: Global Test Accuracy = 0.2520
Round 18: Global Test Accuracy = 0.2600
Round 19: Global Test Accuracy = 0.2640
Round 20: Global Test Accuracy = 0.2720
Round 21: Global Test Accuracy = 0.2780
Round 22: Global Test Accuracy = 0.2830
Round 23: Global Test Accuracy = 0.2850
Round 24: Global Test Accuracy = 0.2960
Round 25: Global Test Accuracy = 0.3010
Round 26: Global Test Accuracy = 0.3080
Round 27: Global Test Accuracy = 0.3190
Round 28: Global Test Accuracy = 0.3250
Round 29: Global Test Accuracy = 0.3310
Round 30: Global Test Accuracy = 0.3380
Round 31: Global Test Accuracy = 0.3400
Round 32: Global Test Accuracy = 0.3550
Round 33: Global Test Accuracy = 0.3610
Round 34: Global Test Accuracy = 0.3660
Round 35: Global Test Accuracy = 0.3660
Round 36: Global Test Accuracy = 0.3750
Round 37: Global Test Accuracy = 0.3820
Round 38: Global Test Accuracy = 0.3890
Round 39: Global Test Accuracy = 0.3910
Round 40: Global Test Accuracy = 0.3990
Round 41: Global Test Accuracy = 0.4040
Round 42: Global Test Accuracy = 0.4110
Round 43: Global Test Accuracy = 0.4220
Round 44: Global Test Accuracy = 0.4330
Round 45: Global Test Accuracy = 0.4410
Round 46: Global Test Accuracy = 0.4430
Round 47: Global Test Accuracy = 0.4550
Round 48: Global Test Accuracy = 0.4560
Round 49: Global Test Accuracy = 0.4610
Round 50: Global Test Accuracy = 0.4630
Round 51: Global Test Accuracy = 0.4670
Round 52: Global Test Accuracy = 0.4690
Round 53: Global Test Accuracy = 0.4770
Round 54: Global Test Accuracy = 0.4860
Round 55: Global Test Accuracy = 0.4830
Round 56: Global Test Accuracy = 0.4870
Round 57: Global Test Accuracy = 0.4990
Round 58: Global Test Accuracy = 0.5040
Round 59: Global Test Accuracy = 0.5030
Round 60: Global Test Accuracy = 0.5050
Round 61: Global Test Accuracy = 0.5090
Round 62: Global Test Accuracy = 0.5180
Round 63: Global Test Accuracy = 0.5160
Round 64: Global Test Accuracy = 0.5170
Round 65: Global Test Accuracy = 0.5210
Round 66: Global Test Accuracy = 0.5190
Round 67: Global Test Accuracy = 0.5250
Round 68: Global Test Accuracy = 0.5270
Round 69: Global Test Accuracy = 0.5290
Round 70: Global Test Accuracy = 0.5290
Round 71: Global Test Accuracy = 0.5330
Round 72: Global Test Accuracy = 0.5340
Round 73: Global Test Accuracy = 0.5330
Round 74: Global Test Accuracy = 0.5340
Round 75: Global Test Accuracy = 0.5350
Round 76: Global Test Accuracy = 0.5370
Round 77: Global Test Accuracy = 0.5340
Round 78: Global Test Accuracy = 0.5350
Round 79: Global Test Accuracy = 0.5340
Round 80: Global Test Accuracy = 0.5400
Round 81: Global Test Accuracy = 0.5430
Round 82: Global Test Accuracy = 0.5440
Round 83: Global Test Accuracy = 0.5490
Round 84: Global Test Accuracy = 0.5480
Round 85: Global Test Accuracy = 0.5490
Round 86: Global Test Accuracy = 0.5480
Round 87: Global Test Accuracy = 0.5490
Round 88: Global Test Accuracy = 0.5520
Round 89: Global Test Accuracy = 0.5530
Round 90: Global Test Accuracy = 0.5530
Round 91: Global Test Accuracy = 0.5530
Round 92: Global Test Accuracy = 0.5500
Round 93: Global Test Accuracy = 0.5550
Round 94: Global Test Accuracy = 0.5580
Round 95: Global Test Accuracy = 0.5600
Round 96: Global Test Accuracy = 0.5600
Round 97: Global Test Accuracy = 0.5620
Round 98: Global Test Accuracy = 0.5660
Round 99: Global Test Accuracy = 0.5630
Round 100: Global Test Accuracy = 0.5670
Round 101: Global Test Accuracy = 0.5640
Round 102: Global Test Accuracy = 0.5650
Round 103: Global Test Accuracy = 0.5690
Round 104: Global Test Accuracy = 0.5680
Round 105: Global Test Accuracy = 0.5650
Round 106: Global Test Accuracy = 0.5660
Round 107: Global Test Accuracy = 0.5670
Round 108: Global Test Accuracy = 0.5680
Round 109: Global Test Accuracy = 0.5680
Round 110: Global Test Accuracy = 0.5670
Round 111: Global Test Accuracy = 0.5660
Round 112: Global Test Accuracy = 0.5690
Round 113: Global Test Accuracy = 0.5670
Round 114: Global Test Accuracy = 0.5680
Round 115: Global Test Accuracy = 0.5660
Round 116: Global Test Accuracy = 0.5670
Round 117: Global Test Accuracy = 0.5690
Round 118: Global Test Accuracy = 0.5670
Round 119: Global Test Accuracy = 0.5660
Round 120: Global Test Accuracy = 0.5670
Round 121: Global Test Accuracy = 0.5700
Round 122: Global Test Accuracy = 0.5700
Round 123: Global Test Accuracy = 0.5760
Round 124: Global Test Accuracy = 0.5780
Round 125: Global Test Accuracy = 0.5810
Round 126: Global Test Accuracy = 0.5770
Round 127: Global Test Accuracy = 0.5770
Round 128: Global Test Accuracy = 0.5790
Round 129: Global Test Accuracy = 0.5790
Round 130: Global Test Accuracy = 0.5780
Round 131: Global Test Accuracy = 0.5770
Round 132: Global Test Accuracy = 0.5790
Round 133: Global Test Accuracy = 0.5820
Round 134: Global Test Accuracy = 0.5780
Round 135: Global Test Accuracy = 0.5750
Round 136: Global Test Accuracy = 0.5800
Round 137: Global Test Accuracy = 0.5780
Round 138: Global Test Accuracy = 0.5810
Round 139: Global Test Accuracy = 0.5830
Round 140: Global Test Accuracy = 0.5800
Round 141: Global Test Accuracy = 0.5790
Round 142: Global Test Accuracy = 0.5820
Round 143: Global Test Accuracy = 0.5830
Round 144: Global Test Accuracy = 0.5840
Round 145: Global Test Accuracy = 0.5840
Round 146: Global Test Accuracy = 0.5830
Round 147: Global Test Accuracy = 0.5810
Round 148: Global Test Accuracy = 0.5840
Round 149: Global Test Accuracy = 0.5830
Round 150: Global Test Accuracy = 0.5820
Round 151: Global Test Accuracy = 0.5840
Round 152: Global Test Accuracy = 0.5830
Round 153: Global Test Accuracy = 0.5830
Round 154: Global Test Accuracy = 0.5810
Round 155: Global Test Accuracy = 0.5870
Round 156: Global Test Accuracy = 0.5880
Round 157: Global Test Accuracy = 0.5870
Round 158: Global Test Accuracy = 0.5840
Round 159: Global Test Accuracy = 0.5830
Round 160: Global Test Accuracy = 0.5820
Round 161: Global Test Accuracy = 0.5820
Round 162: Global Test Accuracy = 0.5800
Round 163: Global Test Accuracy = 0.5820
Round 164: Global Test Accuracy = 0.5800
Round 165: Global Test Accuracy = 0.5800
Round 166: Global Test Accuracy = 0.5820
Round 167: Global Test Accuracy = 0.5810
Round 168: Global Test Accuracy = 0.5820
Round 169: Global Test Accuracy = 0.5810
Round 170: Global Test Accuracy = 0.5850
Round 171: Global Test Accuracy = 0.5860
Round 172: Global Test Accuracy = 0.5850
Round 173: Global Test Accuracy = 0.5830
Round 174: Global Test Accuracy = 0.5820
Round 175: Global Test Accuracy = 0.5820
Round 176: Global Test Accuracy = 0.5820
Round 177: Global Test Accuracy = 0.5840
Round 178: Global Test Accuracy = 0.5860
Round 179: Global Test Accuracy = 0.5830
Round 180: Global Test Accuracy = 0.5850
Round 181: Global Test Accuracy = 0.5850
Round 182: Global Test Accuracy = 0.5840
Round 183: Global Test Accuracy = 0.5840
Round 184: Global Test Accuracy = 0.5840
Round 185: Global Test Accuracy = 0.5850
Round 186: Global Test Accuracy = 0.5850
Round 187: Global Test Accuracy = 0.5890
Round 188: Global Test Accuracy = 0.5850
Round 189: Global Test Accuracy = 0.5870
Round 190: Global Test Accuracy = 0.5870
Round 191: Global Test Accuracy = 0.5850
Round 192: Global Test Accuracy = 0.5890
Round 193: Global Test Accuracy = 0.5880
Round 194: Global Test Accuracy = 0.5890
Round 195: Global Test Accuracy = 0.5880
Round 196: Global Test Accuracy = 0.5880
Round 197: Global Test Accuracy = 0.5900
Round 198: Global Test Accuracy = 0.5900
Round 199: Global Test Accuracy = 0.5880
Round 200: Global Test Accuracy = 0.5870
//train_time: 12779.083999999999 ms//end
//Log Max memory for Large1: 6021914624.0 //end
//Log Max memory for Large2: 6128939008.0 //end
//Log Max memory for Large3: 5751709696.0 //end
//Log Max memory for Large4: 6492676096.0 //end
//Log Max memory for Server: 18089279488.0 //end
//Log Large1 network: 99146752.0 //end
//Log Large2 network: 148469103.0 //end
//Log Large3 network: 99184818.0 //end
//Log Large4 network: 148381833.0 //end
//Log Server network: 493902409.0 //end
//Log Total Actual Train Comm Cost: 943.26 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 1.2034620969295502
Average test accuracy, 0.587
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 905.85 MB //end
[36m(Trainer pid=117804, ip=192.168.14.62)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=117804, ip=192.168.14.62)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: citeseer, Trainers: 10, Distribution: average, IID Beta: 10.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'citeseer', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/citeseer/raw/ind.citeseer.x
File already exists: ./data/citeseer/raw/ind.citeseer.tx
File already exists: ./data/citeseer/raw/ind.citeseer.allx
File already exists: ./data/citeseer/raw/ind.citeseer.y
File already exists: ./data/citeseer/raw/ind.citeseer.ty
File already exists: ./data/citeseer/raw/ind.citeseer.ally
File already exists: ./data/citeseer/raw/ind.citeseer.graph
File already exists: ./data/citeseer/raw/ind.citeseer.test.index
Initialization start: network data collected.
2025-05-15 02:55:37,668	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 02:55:37,668	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 02:55:37,674	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=114266, ip=192.168.39.156)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=114266, ip=192.168.39.156)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5601.242 ms //end
//Log Large1 init network: 144948.0 //end
//Log Large2 init network: 133911.0 //end
//Log Large3 init network: 119681.0 //end
//Log Large4 init network: 122734.0 //end
//Log Server init network: 50107245.0 //end
//Log Initialization Communication Cost (MB): 48.28 //end
Pretrain start time recorded.
//pretrain_time: 4.8180000000000005 ms//end
//Log Max memory for Large1: 6447927296.0 //end
//Log Max memory for Large2: 5699276800.0 //end
//Log Max memory for Large3: 6172200960.0 //end
//Log Max memory for Large4: 6057152512.0 //end
//Log Max memory for Server: 18112942080.0 //end
//Log Large1 network: 758892.0 //end
//Log Large2 network: 739078.0 //end
//Log Large3 network: 813829.0 //end
//Log Large4 network: 628194.0 //end
//Log Server network: 3447192.0 //end
//Log Total Actual Pretrain Comm Cost: 6.09 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1690
Round 2: Global Test Accuracy = 0.1800
Round 3: Global Test Accuracy = 0.1780
Round 4: Global Test Accuracy = 0.1870
Round 5: Global Test Accuracy = 0.1920
Round 6: Global Test Accuracy = 0.2110
Round 7: Global Test Accuracy = 0.2200
Round 8: Global Test Accuracy = 0.2260
Round 9: Global Test Accuracy = 0.2350
Round 10: Global Test Accuracy = 0.2470
Round 11: Global Test Accuracy = 0.2550
Round 12: Global Test Accuracy = 0.2550
Round 13: Global Test Accuracy = 0.2670
Round 14: Global Test Accuracy = 0.2720
Round 15: Global Test Accuracy = 0.2770
Round 16: Global Test Accuracy = 0.2750
Round 17: Global Test Accuracy = 0.2860
Round 18: Global Test Accuracy = 0.2780
Round 19: Global Test Accuracy = 0.2880
Round 20: Global Test Accuracy = 0.2940
Round 21: Global Test Accuracy = 0.3070
Round 22: Global Test Accuracy = 0.3170
Round 23: Global Test Accuracy = 0.3210
Round 24: Global Test Accuracy = 0.3400
Round 25: Global Test Accuracy = 0.3440
Round 26: Global Test Accuracy = 0.3560
Round 27: Global Test Accuracy = 0.3570
Round 28: Global Test Accuracy = 0.3690
Round 29: Global Test Accuracy = 0.3740
Round 30: Global Test Accuracy = 0.3880
Round 31: Global Test Accuracy = 0.4010
Round 32: Global Test Accuracy = 0.4000
Round 33: Global Test Accuracy = 0.4180
Round 34: Global Test Accuracy = 0.4240
Round 35: Global Test Accuracy = 0.4300
Round 36: Global Test Accuracy = 0.4350
Round 37: Global Test Accuracy = 0.4340
Round 38: Global Test Accuracy = 0.4350
Round 39: Global Test Accuracy = 0.4440
Round 40: Global Test Accuracy = 0.4490
Round 41: Global Test Accuracy = 0.4750
Round 42: Global Test Accuracy = 0.4680
Round 43: Global Test Accuracy = 0.4720
Round 44: Global Test Accuracy = 0.4770
Round 45: Global Test Accuracy = 0.4810
Round 46: Global Test Accuracy = 0.4860
Round 47: Global Test Accuracy = 0.4960
Round 48: Global Test Accuracy = 0.4970
Round 49: Global Test Accuracy = 0.5030
Round 50: Global Test Accuracy = 0.5040
Round 51: Global Test Accuracy = 0.5080
Round 52: Global Test Accuracy = 0.5090
Round 53: Global Test Accuracy = 0.5160
Round 54: Global Test Accuracy = 0.5210
Round 55: Global Test Accuracy = 0.5250
Round 56: Global Test Accuracy = 0.5310
Round 57: Global Test Accuracy = 0.5330
Round 58: Global Test Accuracy = 0.5390
Round 59: Global Test Accuracy = 0.5390
Round 60: Global Test Accuracy = 0.5450
Round 61: Global Test Accuracy = 0.5530
Round 62: Global Test Accuracy = 0.5550
Round 63: Global Test Accuracy = 0.5570
Round 64: Global Test Accuracy = 0.5610
Round 65: Global Test Accuracy = 0.5590
Round 66: Global Test Accuracy = 0.5550
Round 67: Global Test Accuracy = 0.5610
Round 68: Global Test Accuracy = 0.5570
Round 69: Global Test Accuracy = 0.5650
Round 70: Global Test Accuracy = 0.5680
Round 71: Global Test Accuracy = 0.5680
Round 72: Global Test Accuracy = 0.5680
Round 73: Global Test Accuracy = 0.5770
Round 74: Global Test Accuracy = 0.5710
Round 75: Global Test Accuracy = 0.5680
Round 76: Global Test Accuracy = 0.5730
Round 77: Global Test Accuracy = 0.5670
Round 78: Global Test Accuracy = 0.5650
Round 79: Global Test Accuracy = 0.5710
Round 80: Global Test Accuracy = 0.5720
Round 81: Global Test Accuracy = 0.5710
Round 82: Global Test Accuracy = 0.5720
Round 83: Global Test Accuracy = 0.5720
Round 84: Global Test Accuracy = 0.5750
Round 85: Global Test Accuracy = 0.5810
Round 86: Global Test Accuracy = 0.5770
Round 87: Global Test Accuracy = 0.5790
Round 88: Global Test Accuracy = 0.5840
Round 89: Global Test Accuracy = 0.5830
Round 90: Global Test Accuracy = 0.5820
Round 91: Global Test Accuracy = 0.5840
Round 92: Global Test Accuracy = 0.5830
Round 93: Global Test Accuracy = 0.5900
Round 94: Global Test Accuracy = 0.5870
Round 95: Global Test Accuracy = 0.5890
Round 96: Global Test Accuracy = 0.5900
Round 97: Global Test Accuracy = 0.5940
Round 98: Global Test Accuracy = 0.5910
Round 99: Global Test Accuracy = 0.5940
Round 100: Global Test Accuracy = 0.5920
Round 101: Global Test Accuracy = 0.5960
Round 102: Global Test Accuracy = 0.5960
Round 103: Global Test Accuracy = 0.5900
Round 104: Global Test Accuracy = 0.5920
Round 105: Global Test Accuracy = 0.5950
Round 106: Global Test Accuracy = 0.5930
Round 107: Global Test Accuracy = 0.5920
Round 108: Global Test Accuracy = 0.5950
Round 109: Global Test Accuracy = 0.5950
Round 110: Global Test Accuracy = 0.5950
Round 111: Global Test Accuracy = 0.5960
Round 112: Global Test Accuracy = 0.5980
Round 113: Global Test Accuracy = 0.5980
Round 114: Global Test Accuracy = 0.5970
Round 115: Global Test Accuracy = 0.5950
Round 116: Global Test Accuracy = 0.5970
Round 117: Global Test Accuracy = 0.5980
Round 118: Global Test Accuracy = 0.5950
Round 119: Global Test Accuracy = 0.5920
Round 120: Global Test Accuracy = 0.5930
Round 121: Global Test Accuracy = 0.5950
Round 122: Global Test Accuracy = 0.5930
Round 123: Global Test Accuracy = 0.5920
Round 124: Global Test Accuracy = 0.5930
Round 125: Global Test Accuracy = 0.5950
Round 126: Global Test Accuracy = 0.5870
Round 127: Global Test Accuracy = 0.5890
Round 128: Global Test Accuracy = 0.5940
Round 129: Global Test Accuracy = 0.5940
Round 130: Global Test Accuracy = 0.5930
Round 131: Global Test Accuracy = 0.5900
Round 132: Global Test Accuracy = 0.5890
Round 133: Global Test Accuracy = 0.5850
Round 134: Global Test Accuracy = 0.5890
Round 135: Global Test Accuracy = 0.5890
Round 136: Global Test Accuracy = 0.5890
Round 137: Global Test Accuracy = 0.5910
Round 138: Global Test Accuracy = 0.5910
Round 139: Global Test Accuracy = 0.5900
Round 140: Global Test Accuracy = 0.5870
Round 141: Global Test Accuracy = 0.5880
Round 142: Global Test Accuracy = 0.5910
Round 143: Global Test Accuracy = 0.5870
Round 144: Global Test Accuracy = 0.5870
Round 145: Global Test Accuracy = 0.5880
Round 146: Global Test Accuracy = 0.5910
Round 147: Global Test Accuracy = 0.5880
Round 148: Global Test Accuracy = 0.5870
Round 149: Global Test Accuracy = 0.5900
Round 150: Global Test Accuracy = 0.5850
Round 151: Global Test Accuracy = 0.5870
Round 152: Global Test Accuracy = 0.5890
Round 153: Global Test Accuracy = 0.5900
Round 154: Global Test Accuracy = 0.5890
Round 155: Global Test Accuracy = 0.5870
Round 156: Global Test Accuracy = 0.5890
Round 157: Global Test Accuracy = 0.5880
Round 158: Global Test Accuracy = 0.5880
Round 159: Global Test Accuracy = 0.5880
Round 160: Global Test Accuracy = 0.5900
Round 161: Global Test Accuracy = 0.5880
Round 162: Global Test Accuracy = 0.5890
Round 163: Global Test Accuracy = 0.5890
Round 164: Global Test Accuracy = 0.5860
Round 165: Global Test Accuracy = 0.5850
Round 166: Global Test Accuracy = 0.5860
Round 167: Global Test Accuracy = 0.5880
Round 168: Global Test Accuracy = 0.5870
Round 169: Global Test Accuracy = 0.5880
Round 170: Global Test Accuracy = 0.5880
Round 171: Global Test Accuracy = 0.5900
Round 172: Global Test Accuracy = 0.5880
Round 173: Global Test Accuracy = 0.5880
Round 174: Global Test Accuracy = 0.5850
Round 175: Global Test Accuracy = 0.5850
Round 176: Global Test Accuracy = 0.5820
Round 177: Global Test Accuracy = 0.5810
Round 178: Global Test Accuracy = 0.5820
Round 179: Global Test Accuracy = 0.5810
Round 180: Global Test Accuracy = 0.5830
Round 181: Global Test Accuracy = 0.5850
Round 182: Global Test Accuracy = 0.5850
Round 183: Global Test Accuracy = 0.5860
Round 184: Global Test Accuracy = 0.5820
Round 185: Global Test Accuracy = 0.5810
Round 186: Global Test Accuracy = 0.5820
Round 187: Global Test Accuracy = 0.5830
Round 188: Global Test Accuracy = 0.5830
Round 189: Global Test Accuracy = 0.5810
Round 190: Global Test Accuracy = 0.5790
Round 191: Global Test Accuracy = 0.5800
Round 192: Global Test Accuracy = 0.5810
Round 193: Global Test Accuracy = 0.5810
Round 194: Global Test Accuracy = 0.5810
Round 195: Global Test Accuracy = 0.5810
Round 196: Global Test Accuracy = 0.5830
Round 197: Global Test Accuracy = 0.5810
Round 198: Global Test Accuracy = 0.5790
Round 199: Global Test Accuracy = 0.5790
Round 200: Global Test Accuracy = 0.5790
//train_time: 13088.526 ms//end
//Log Max memory for Large1: 6436614144.0 //end
//Log Max memory for Large2: 5704790016.0 //end
//Log Max memory for Large3: 6173179904.0 //end
//Log Max memory for Large4: 6049464320.0 //end
//Log Max memory for Server: 18103574528.0 //end
//Log Large1 network: 148332302.0 //end
//Log Large2 network: 99276767.0 //end
//Log Large3 network: 148361109.0 //end
//Log Large4 network: 99278328.0 //end
//Log Server network: 494045975.0 //end
//Log Total Actual Train Comm Cost: 943.46 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 1.2060833884477615
Average test accuracy, 0.579
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 905.85 MB //end
[36m(Trainer pid=114268, ip=192.168.39.156)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=114268, ip=192.168.39.156)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: citeseer, Trainers: 10, Distribution: average, IID Beta: 10000.0, Hops: 1, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'citeseer', 'method': 'fedgcn', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 1, 'iid_beta': 10000.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/citeseer/raw/ind.citeseer.x
File already exists: ./data/citeseer/raw/ind.citeseer.tx
File already exists: ./data/citeseer/raw/ind.citeseer.allx
File already exists: ./data/citeseer/raw/ind.citeseer.y
File already exists: ./data/citeseer/raw/ind.citeseer.ty
File already exists: ./data/citeseer/raw/ind.citeseer.ally
File already exists: ./data/citeseer/raw/ind.citeseer.graph
File already exists: ./data/citeseer/raw/ind.citeseer.test.index
Initialization start: network data collected.
2025-05-15 02:57:02,586	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 02:57:02,586	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 02:57:02,593	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
[36m(Trainer pid=118923, ip=192.168.14.62)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=118923, ip=192.168.14.62)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5915.863 ms //end
//Log Large1 init network: 114085.0 //end
//Log Large2 init network: 219266.0 //end
//Log Large3 init network: 123714.0 //end
//Log Large4 init network: 137811.0 //end
//Log Server init network: 50098287.0 //end
//Log Initialization Communication Cost (MB): 48.34 //end
Pretrain start time recorded.
server aggregates all local neighbor feature sums
clients received feature aggregation from server
//pretrain_time: 1486.018 ms//end
//Log Max memory for Large1: 6189178880.0 //end
//Log Max memory for Large2: 6340952064.0 //end
//Log Max memory for Large3: 5927485440.0 //end
//Log Max memory for Large4: 6660448256.0 //end
//Log Max memory for Server: 18355163136.0 //end
//Log Large1 network: 100771829.0 //end
//Log Large2 network: 149770319.0 //end
//Log Large3 network: 100693720.0 //end
//Log Large4 network: 150864031.0 //end
//Log Server network: 153204938.0 //end
//Log Total Actual Pretrain Comm Cost: 624.95 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.2450
Round 2: Global Test Accuracy = 0.3180
Round 3: Global Test Accuracy = 0.3980
Round 4: Global Test Accuracy = 0.4510
Round 5: Global Test Accuracy = 0.5200
Round 6: Global Test Accuracy = 0.5370
Round 7: Global Test Accuracy = 0.5770
Round 8: Global Test Accuracy = 0.5980
Round 9: Global Test Accuracy = 0.6260
Round 10: Global Test Accuracy = 0.6230
Round 11: Global Test Accuracy = 0.6390
Round 12: Global Test Accuracy = 0.6400
Round 13: Global Test Accuracy = 0.6520
Round 14: Global Test Accuracy = 0.6400
Round 15: Global Test Accuracy = 0.6720
Round 16: Global Test Accuracy = 0.6440
Round 17: Global Test Accuracy = 0.6380
Round 18: Global Test Accuracy = 0.6590
Round 19: Global Test Accuracy = 0.6480
Round 20: Global Test Accuracy = 0.6690
Round 21: Global Test Accuracy = 0.6610
Round 22: Global Test Accuracy = 0.6700
Round 23: Global Test Accuracy = 0.6570
Round 24: Global Test Accuracy = 0.6700
Round 25: Global Test Accuracy = 0.6590
Round 26: Global Test Accuracy = 0.6570
Round 27: Global Test Accuracy = 0.6750
Round 28: Global Test Accuracy = 0.6800
Round 29: Global Test Accuracy = 0.6690
Round 30: Global Test Accuracy = 0.6820
Round 31: Global Test Accuracy = 0.6690
Round 32: Global Test Accuracy = 0.6680
Round 33: Global Test Accuracy = 0.6730
Round 34: Global Test Accuracy = 0.6840
Round 35: Global Test Accuracy = 0.6650
Round 36: Global Test Accuracy = 0.6700
Round 37: Global Test Accuracy = 0.6710
Round 38: Global Test Accuracy = 0.6690
Round 39: Global Test Accuracy = 0.6740
Round 40: Global Test Accuracy = 0.6800
Round 41: Global Test Accuracy = 0.6800
Round 42: Global Test Accuracy = 0.6820
Round 43: Global Test Accuracy = 0.6790
Round 44: Global Test Accuracy = 0.6780
Round 45: Global Test Accuracy = 0.6850
Round 46: Global Test Accuracy = 0.6780
Round 47: Global Test Accuracy = 0.6900
Round 48: Global Test Accuracy = 0.6870
Round 49: Global Test Accuracy = 0.6900
Round 50: Global Test Accuracy = 0.6870
Round 51: Global Test Accuracy = 0.6770
Round 52: Global Test Accuracy = 0.6820
Round 53: Global Test Accuracy = 0.6770
Round 54: Global Test Accuracy = 0.6690
Round 55: Global Test Accuracy = 0.6700
Round 56: Global Test Accuracy = 0.6790
Round 57: Global Test Accuracy = 0.6820
Round 58: Global Test Accuracy = 0.6720
Round 59: Global Test Accuracy = 0.6720
Round 60: Global Test Accuracy = 0.6870
Round 61: Global Test Accuracy = 0.6850
Round 62: Global Test Accuracy = 0.6790
Round 63: Global Test Accuracy = 0.6720
Round 64: Global Test Accuracy = 0.6730
Round 65: Global Test Accuracy = 0.6860
Round 66: Global Test Accuracy = 0.6780
Round 67: Global Test Accuracy = 0.6730
Round 68: Global Test Accuracy = 0.6750
Round 69: Global Test Accuracy = 0.6820
Round 70: Global Test Accuracy = 0.6820
Round 71: Global Test Accuracy = 0.6820
Round 72: Global Test Accuracy = 0.6770
Round 73: Global Test Accuracy = 0.6790
Round 74: Global Test Accuracy = 0.6790
Round 75: Global Test Accuracy = 0.6770
Round 76: Global Test Accuracy = 0.6830
Round 77: Global Test Accuracy = 0.6850
Round 78: Global Test Accuracy = 0.6780
Round 79: Global Test Accuracy = 0.6690
Round 80: Global Test Accuracy = 0.6840
Round 81: Global Test Accuracy = 0.6860
Round 82: Global Test Accuracy = 0.6860
Round 83: Global Test Accuracy = 0.6830
Round 84: Global Test Accuracy = 0.6850
Round 85: Global Test Accuracy = 0.6870
Round 86: Global Test Accuracy = 0.6830
Round 87: Global Test Accuracy = 0.6780
Round 88: Global Test Accuracy = 0.6800
Round 89: Global Test Accuracy = 0.6790
Round 90: Global Test Accuracy = 0.6780
Round 91: Global Test Accuracy = 0.6890
Round 92: Global Test Accuracy = 0.6920
Round 93: Global Test Accuracy = 0.6890
Round 94: Global Test Accuracy = 0.6840
Round 95: Global Test Accuracy = 0.6860
Round 96: Global Test Accuracy = 0.6860
Round 97: Global Test Accuracy = 0.6880
Round 98: Global Test Accuracy = 0.6860
Round 99: Global Test Accuracy = 0.6860
Round 100: Global Test Accuracy = 0.6860
Round 101: Global Test Accuracy = 0.6940
Round 102: Global Test Accuracy = 0.6910
Round 103: Global Test Accuracy = 0.6870
Round 104: Global Test Accuracy = 0.6870
Round 105: Global Test Accuracy = 0.6870
Round 106: Global Test Accuracy = 0.6870
Round 107: Global Test Accuracy = 0.6870
Round 108: Global Test Accuracy = 0.6840
Round 109: Global Test Accuracy = 0.6890
Round 110: Global Test Accuracy = 0.6890
Round 111: Global Test Accuracy = 0.6870
Round 112: Global Test Accuracy = 0.6860
Round 113: Global Test Accuracy = 0.6820
Round 114: Global Test Accuracy = 0.6860
Round 115: Global Test Accuracy = 0.6850
Round 116: Global Test Accuracy = 0.6850
Round 117: Global Test Accuracy = 0.6880
Round 118: Global Test Accuracy = 0.6880
Round 119: Global Test Accuracy = 0.6880
Round 120: Global Test Accuracy = 0.6900
Round 121: Global Test Accuracy = 0.6870
Round 122: Global Test Accuracy = 0.6890
Round 123: Global Test Accuracy = 0.6840
Round 124: Global Test Accuracy = 0.6870
Round 125: Global Test Accuracy = 0.6760
Round 126: Global Test Accuracy = 0.6860
Round 127: Global Test Accuracy = 0.6900
Round 128: Global Test Accuracy = 0.6910
Round 129: Global Test Accuracy = 0.6780
Round 130: Global Test Accuracy = 0.6830
Round 131: Global Test Accuracy = 0.6830
Round 132: Global Test Accuracy = 0.6870
Round 133: Global Test Accuracy = 0.6840
Round 134: Global Test Accuracy = 0.6890
Round 135: Global Test Accuracy = 0.6920
Round 136: Global Test Accuracy = 0.6910
Round 137: Global Test Accuracy = 0.6900
Round 138: Global Test Accuracy = 0.6890
Round 139: Global Test Accuracy = 0.6880
Round 140: Global Test Accuracy = 0.6880
Round 141: Global Test Accuracy = 0.6900
Round 142: Global Test Accuracy = 0.6890
Round 143: Global Test Accuracy = 0.6900
Round 144: Global Test Accuracy = 0.6900
Round 145: Global Test Accuracy = 0.6920
Round 146: Global Test Accuracy = 0.6910
Round 147: Global Test Accuracy = 0.6910
Round 148: Global Test Accuracy = 0.6830
Round 149: Global Test Accuracy = 0.6870
Round 150: Global Test Accuracy = 0.6860
Round 151: Global Test Accuracy = 0.6850
Round 152: Global Test Accuracy = 0.6910
Round 153: Global Test Accuracy = 0.6900
Round 154: Global Test Accuracy = 0.6890
Round 155: Global Test Accuracy = 0.6900
Round 156: Global Test Accuracy = 0.6920
Round 157: Global Test Accuracy = 0.6910
Round 158: Global Test Accuracy = 0.6890
Round 159: Global Test Accuracy = 0.6870
Round 160: Global Test Accuracy = 0.6890
Round 161: Global Test Accuracy = 0.6890
Round 162: Global Test Accuracy = 0.6890
Round 163: Global Test Accuracy = 0.6890
Round 164: Global Test Accuracy = 0.6910
Round 165: Global Test Accuracy = 0.6910
Round 166: Global Test Accuracy = 0.6870
Round 167: Global Test Accuracy = 0.6910
Round 168: Global Test Accuracy = 0.6840
Round 169: Global Test Accuracy = 0.6830
Round 170: Global Test Accuracy = 0.6840
Round 171: Global Test Accuracy = 0.6870
Round 172: Global Test Accuracy = 0.6880
Round 173: Global Test Accuracy = 0.6910
Round 174: Global Test Accuracy = 0.6880
Round 175: Global Test Accuracy = 0.6840
Round 176: Global Test Accuracy = 0.6870
Round 177: Global Test Accuracy = 0.6920
Round 178: Global Test Accuracy = 0.6910
Round 179: Global Test Accuracy = 0.6910
Round 180: Global Test Accuracy = 0.6870
Round 181: Global Test Accuracy = 0.6880
Round 182: Global Test Accuracy = 0.6880
Round 183: Global Test Accuracy = 0.6870
Round 184: Global Test Accuracy = 0.6890
Round 185: Global Test Accuracy = 0.6860
Round 186: Global Test Accuracy = 0.6890
Round 187: Global Test Accuracy = 0.6890
Round 188: Global Test Accuracy = 0.6910
Round 189: Global Test Accuracy = 0.6900
Round 190: Global Test Accuracy = 0.6890
Round 191: Global Test Accuracy = 0.6890
Round 192: Global Test Accuracy = 0.6860
Round 193: Global Test Accuracy = 0.6890
Round 194: Global Test Accuracy = 0.6860
Round 195: Global Test Accuracy = 0.6840
Round 196: Global Test Accuracy = 0.6890
Round 197: Global Test Accuracy = 0.6880
Round 198: Global Test Accuracy = 0.6890
Round 199: Global Test Accuracy = 0.6860
Round 200: Global Test Accuracy = 0.6840
//train_time: 12930.979000000001 ms//end
//Log Max memory for Large1: 6102638592.0 //end
//Log Max memory for Large2: 6223433728.0 //end
//Log Max memory for Large3: 5847470080.0 //end
//Log Max memory for Large4: 6569594880.0 //end
//Log Max memory for Server: 18231349248.0 //end
//Log Large1 network: 99212947.0 //end
//Log Large2 network: 148500685.0 //end
//Log Large3 network: 99286419.0 //end
//Log Large4 network: 148499264.0 //end
//Log Server network: 493862528.0 //end
//Log Total Actual Train Comm Cost: 943.53 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 1.1425396220684052
Average test accuracy, 0.684
//Log Theoretical Pretrain Comm Cost: 610.84 MB //end
//Log Theoretical Train Comm Cost: 905.85 MB //end
[36m(Trainer pid=118922, ip=192.168.14.62)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=118922, ip=192.168.14.62)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: citeseer, Trainers: 10, Distribution: average, IID Beta: 100.0, Hops: 1, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'citeseer', 'method': 'fedgcn', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 1, 'iid_beta': 100.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/citeseer/raw/ind.citeseer.x
File already exists: ./data/citeseer/raw/ind.citeseer.tx
File already exists: ./data/citeseer/raw/ind.citeseer.allx
File already exists: ./data/citeseer/raw/ind.citeseer.y
File already exists: ./data/citeseer/raw/ind.citeseer.ty
File already exists: ./data/citeseer/raw/ind.citeseer.ally
File already exists: ./data/citeseer/raw/ind.citeseer.graph
File already exists: ./data/citeseer/raw/ind.citeseer.test.index
Initialization start: network data collected.
2025-05-15 02:58:29,067	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 02:58:29,067	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 02:58:29,073	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
[36m(Trainer pid=119528, ip=192.168.14.62)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=119528, ip=192.168.14.62)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5631.969 ms //end
//Log Large1 init network: 122903.0 //end
//Log Large2 init network: 124254.0 //end
//Log Large3 init network: 164083.0 //end
//Log Large4 init network: 119401.0 //end
//Log Server init network: 51086466.0 //end
//Log Initialization Communication Cost (MB): 49.23 //end
Pretrain start time recorded.
server aggregates all local neighbor feature sums
clients received feature aggregation from server
//pretrain_time: 1586.64 ms//end
//Log Max memory for Large1: 6642245632.0 //end
//Log Max memory for Large2: 5898657792.0 //end
//Log Max memory for Large3: 6353227776.0 //end
//Log Max memory for Large4: 6205173760.0 //end
//Log Max memory for Server: 18382323712.0 //end
//Log Large1 network: 151583175.0 //end
//Log Large2 network: 100826631.0 //end
//Log Large3 network: 150273449.0 //end
//Log Large4 network: 101660286.0 //end
//Log Server network: 153116050.0 //end
//Log Total Actual Pretrain Comm Cost: 627.00 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.2550
Round 2: Global Test Accuracy = 0.3210
Round 3: Global Test Accuracy = 0.3890
Round 4: Global Test Accuracy = 0.4390
Round 5: Global Test Accuracy = 0.5200
Round 6: Global Test Accuracy = 0.5620
Round 7: Global Test Accuracy = 0.5380
Round 8: Global Test Accuracy = 0.6110
Round 9: Global Test Accuracy = 0.6210
Round 10: Global Test Accuracy = 0.6120
Round 11: Global Test Accuracy = 0.6290
Round 12: Global Test Accuracy = 0.6590
Round 13: Global Test Accuracy = 0.6310
Round 14: Global Test Accuracy = 0.6630
Round 15: Global Test Accuracy = 0.6520
Round 16: Global Test Accuracy = 0.6320
Round 17: Global Test Accuracy = 0.6300
Round 18: Global Test Accuracy = 0.6480
Round 19: Global Test Accuracy = 0.6290
Round 20: Global Test Accuracy = 0.6540
Round 21: Global Test Accuracy = 0.6470
Round 22: Global Test Accuracy = 0.6460
Round 23: Global Test Accuracy = 0.6570
Round 24: Global Test Accuracy = 0.6520
Round 25: Global Test Accuracy = 0.6510
Round 26: Global Test Accuracy = 0.6460
Round 27: Global Test Accuracy = 0.6820
Round 28: Global Test Accuracy = 0.6550
Round 29: Global Test Accuracy = 0.6550
Round 30: Global Test Accuracy = 0.6620
Round 31: Global Test Accuracy = 0.6610
Round 32: Global Test Accuracy = 0.6610
Round 33: Global Test Accuracy = 0.6740
Round 34: Global Test Accuracy = 0.6640
Round 35: Global Test Accuracy = 0.6750
Round 36: Global Test Accuracy = 0.6730
Round 37: Global Test Accuracy = 0.6830
Round 38: Global Test Accuracy = 0.6620
Round 39: Global Test Accuracy = 0.6680
Round 40: Global Test Accuracy = 0.6670
Round 41: Global Test Accuracy = 0.6640
Round 42: Global Test Accuracy = 0.6720
Round 43: Global Test Accuracy = 0.6770
Round 44: Global Test Accuracy = 0.6620
Round 45: Global Test Accuracy = 0.6620
Round 46: Global Test Accuracy = 0.6690
Round 47: Global Test Accuracy = 0.6720
Round 48: Global Test Accuracy = 0.6810
Round 49: Global Test Accuracy = 0.6550
Round 50: Global Test Accuracy = 0.6870
Round 51: Global Test Accuracy = 0.6890
Round 52: Global Test Accuracy = 0.6800
Round 53: Global Test Accuracy = 0.6810
Round 54: Global Test Accuracy = 0.6660
Round 55: Global Test Accuracy = 0.6640
Round 56: Global Test Accuracy = 0.6610
Round 57: Global Test Accuracy = 0.6680
Round 58: Global Test Accuracy = 0.6810
Round 59: Global Test Accuracy = 0.6820
Round 60: Global Test Accuracy = 0.6790
Round 61: Global Test Accuracy = 0.6760
Round 62: Global Test Accuracy = 0.6710
Round 63: Global Test Accuracy = 0.6720
Round 64: Global Test Accuracy = 0.6720
Round 65: Global Test Accuracy = 0.6710
Round 66: Global Test Accuracy = 0.6800
Round 67: Global Test Accuracy = 0.6660
Round 68: Global Test Accuracy = 0.6770
Round 69: Global Test Accuracy = 0.6830
Round 70: Global Test Accuracy = 0.6770
Round 71: Global Test Accuracy = 0.6790
Round 72: Global Test Accuracy = 0.6810
Round 73: Global Test Accuracy = 0.6820
Round 74: Global Test Accuracy = 0.6870
Round 75: Global Test Accuracy = 0.6760
Round 76: Global Test Accuracy = 0.6810
Round 77: Global Test Accuracy = 0.6720
Round 78: Global Test Accuracy = 0.6790
Round 79: Global Test Accuracy = 0.6810
Round 80: Global Test Accuracy = 0.6760
Round 81: Global Test Accuracy = 0.6830
Round 82: Global Test Accuracy = 0.6800
Round 83: Global Test Accuracy = 0.6850
Round 84: Global Test Accuracy = 0.6820
Round 85: Global Test Accuracy = 0.6810
Round 86: Global Test Accuracy = 0.6800
Round 87: Global Test Accuracy = 0.6870
Round 88: Global Test Accuracy = 0.6790
Round 89: Global Test Accuracy = 0.6790
Round 90: Global Test Accuracy = 0.6780
Round 91: Global Test Accuracy = 0.6820
Round 92: Global Test Accuracy = 0.6870
Round 93: Global Test Accuracy = 0.6800
Round 94: Global Test Accuracy = 0.6780
Round 95: Global Test Accuracy = 0.6760
Round 96: Global Test Accuracy = 0.6810
Round 97: Global Test Accuracy = 0.6780
Round 98: Global Test Accuracy = 0.6930
Round 99: Global Test Accuracy = 0.6910
Round 100: Global Test Accuracy = 0.6870
Round 101: Global Test Accuracy = 0.6850
Round 102: Global Test Accuracy = 0.6850
Round 103: Global Test Accuracy = 0.6810
Round 104: Global Test Accuracy = 0.6860
Round 105: Global Test Accuracy = 0.6890
Round 106: Global Test Accuracy = 0.6860
Round 107: Global Test Accuracy = 0.6840
Round 108: Global Test Accuracy = 0.6900
Round 109: Global Test Accuracy = 0.6910
Round 110: Global Test Accuracy = 0.6920
Round 111: Global Test Accuracy = 0.6880
Round 112: Global Test Accuracy = 0.6840
Round 113: Global Test Accuracy = 0.6890
Round 114: Global Test Accuracy = 0.6870
Round 115: Global Test Accuracy = 0.6860
Round 116: Global Test Accuracy = 0.6830
Round 117: Global Test Accuracy = 0.6830
Round 118: Global Test Accuracy = 0.6830
Round 119: Global Test Accuracy = 0.6850
Round 120: Global Test Accuracy = 0.6840
Round 121: Global Test Accuracy = 0.6840
Round 122: Global Test Accuracy = 0.6810
Round 123: Global Test Accuracy = 0.6830
Round 124: Global Test Accuracy = 0.6880
Round 125: Global Test Accuracy = 0.6870
Round 126: Global Test Accuracy = 0.6890
Round 127: Global Test Accuracy = 0.6900
Round 128: Global Test Accuracy = 0.6910
Round 129: Global Test Accuracy = 0.6890
Round 130: Global Test Accuracy = 0.6850
Round 131: Global Test Accuracy = 0.6860
Round 132: Global Test Accuracy = 0.6870
Round 133: Global Test Accuracy = 0.6800
Round 134: Global Test Accuracy = 0.6880
Round 135: Global Test Accuracy = 0.6870
Round 136: Global Test Accuracy = 0.6760
Round 137: Global Test Accuracy = 0.6860
Round 138: Global Test Accuracy = 0.6830
Round 139: Global Test Accuracy = 0.6850
Round 140: Global Test Accuracy = 0.6810
Round 141: Global Test Accuracy = 0.6830
Round 142: Global Test Accuracy = 0.6880
Round 143: Global Test Accuracy = 0.6880
Round 144: Global Test Accuracy = 0.6850
Round 145: Global Test Accuracy = 0.6790
Round 146: Global Test Accuracy = 0.6840
Round 147: Global Test Accuracy = 0.6840
Round 148: Global Test Accuracy = 0.6840
Round 149: Global Test Accuracy = 0.6820
Round 150: Global Test Accuracy = 0.6860
Round 151: Global Test Accuracy = 0.6820
Round 152: Global Test Accuracy = 0.6850
Round 153: Global Test Accuracy = 0.6850
Round 154: Global Test Accuracy = 0.6880
Round 155: Global Test Accuracy = 0.6870
Round 156: Global Test Accuracy = 0.6860
Round 157: Global Test Accuracy = 0.6840
Round 158: Global Test Accuracy = 0.6830
Round 159: Global Test Accuracy = 0.6870
Round 160: Global Test Accuracy = 0.6870
Round 161: Global Test Accuracy = 0.6850
Round 162: Global Test Accuracy = 0.6900
Round 163: Global Test Accuracy = 0.6880
Round 164: Global Test Accuracy = 0.6890
Round 165: Global Test Accuracy = 0.6870
Round 166: Global Test Accuracy = 0.6850
Round 167: Global Test Accuracy = 0.6860
Round 168: Global Test Accuracy = 0.6890
Round 169: Global Test Accuracy = 0.6920
Round 170: Global Test Accuracy = 0.6900
Round 171: Global Test Accuracy = 0.6890
Round 172: Global Test Accuracy = 0.6820
Round 173: Global Test Accuracy = 0.6830
Round 174: Global Test Accuracy = 0.6850
Round 175: Global Test Accuracy = 0.6900
Round 176: Global Test Accuracy = 0.6870
Round 177: Global Test Accuracy = 0.6870
Round 178: Global Test Accuracy = 0.6880
Round 179: Global Test Accuracy = 0.6870
Round 180: Global Test Accuracy = 0.6870
Round 181: Global Test Accuracy = 0.6860
Round 182: Global Test Accuracy = 0.6850
Round 183: Global Test Accuracy = 0.6840
Round 184: Global Test Accuracy = 0.6830
Round 185: Global Test Accuracy = 0.6880
Round 186: Global Test Accuracy = 0.6890
Round 187: Global Test Accuracy = 0.6840
Round 188: Global Test Accuracy = 0.6880
Round 189: Global Test Accuracy = 0.6870
Round 190: Global Test Accuracy = 0.6880
Round 191: Global Test Accuracy = 0.6890
Round 192: Global Test Accuracy = 0.6880
Round 193: Global Test Accuracy = 0.6830
Round 194: Global Test Accuracy = 0.6880
Round 195: Global Test Accuracy = 0.6880
Round 196: Global Test Accuracy = 0.6880
Round 197: Global Test Accuracy = 0.6880
Round 198: Global Test Accuracy = 0.6890
Round 199: Global Test Accuracy = 0.6810
Round 200: Global Test Accuracy = 0.6850
//train_time: 12912.635999999999 ms//end
//Log Max memory for Large1: 6499680256.0 //end
//Log Max memory for Large2: 5749686272.0 //end
//Log Max memory for Large3: 6220034048.0 //end
//Log Max memory for Large4: 6126915584.0 //end
//Log Max memory for Server: 18252328960.0 //end
//Log Large1 network: 148294228.0 //end
//Log Large2 network: 99230534.0 //end
//Log Large3 network: 148392290.0 //end
//Log Large4 network: 99170833.0 //end
//Log Server network: 493903999.0 //end
//Log Total Actual Train Comm Cost: 943.18 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 1.1601033034920691
Average test accuracy, 0.685
//Log Theoretical Pretrain Comm Cost: 611.66 MB //end
//Log Theoretical Train Comm Cost: 905.85 MB //end
[36m(Trainer pid=115394, ip=192.168.39.156)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=115394, ip=192.168.39.156)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: citeseer, Trainers: 10, Distribution: average, IID Beta: 10.0, Hops: 1, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'citeseer', 'method': 'fedgcn', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 1, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/citeseer/raw/ind.citeseer.x
File already exists: ./data/citeseer/raw/ind.citeseer.tx
File already exists: ./data/citeseer/raw/ind.citeseer.allx
File already exists: ./data/citeseer/raw/ind.citeseer.y
File already exists: ./data/citeseer/raw/ind.citeseer.ty
File already exists: ./data/citeseer/raw/ind.citeseer.ally
File already exists: ./data/citeseer/raw/ind.citeseer.graph
File already exists: ./data/citeseer/raw/ind.citeseer.test.index
Initialization start: network data collected.
2025-05-15 02:59:55,267	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 02:59:55,268	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 02:59:55,275	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
[36m(Trainer pid=116167, ip=192.168.14.54)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=116167, ip=192.168.14.54)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5634.98 ms //end
//Log Large1 init network: 114345.0 //end
//Log Large2 init network: 157664.0 //end
//Log Large3 init network: 118842.0 //end
//Log Large4 init network: 138998.0 //end
//Log Server init network: 51117215.0 //end
//Log Initialization Communication Cost (MB): 49.25 //end
Pretrain start time recorded.
server aggregates all local neighbor feature sums
clients received feature aggregation from server
//pretrain_time: 1481.473 ms//end
//Log Max memory for Large1: 6179958784.0 //end
//Log Max memory for Large2: 6320455680.0 //end
//Log Max memory for Large3: 5907849216.0 //end
//Log Max memory for Large4: 6664925184.0 //end
//Log Max memory for Server: 18365968384.0 //end
//Log Large1 network: 100871573.0 //end
//Log Large2 network: 151672683.0 //end
//Log Large3 network: 101097113.0 //end
//Log Large4 network: 150540112.0 //end
//Log Server network: 148988970.0 //end
//Log Total Actual Pretrain Comm Cost: 622.91 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.2510
Round 2: Global Test Accuracy = 0.3380
Round 3: Global Test Accuracy = 0.4700
Round 4: Global Test Accuracy = 0.5470
Round 5: Global Test Accuracy = 0.5870
Round 6: Global Test Accuracy = 0.6080
Round 7: Global Test Accuracy = 0.6430
Round 8: Global Test Accuracy = 0.6490
Round 9: Global Test Accuracy = 0.6560
Round 10: Global Test Accuracy = 0.6610
Round 11: Global Test Accuracy = 0.6660
Round 12: Global Test Accuracy = 0.6590
Round 13: Global Test Accuracy = 0.6780
Round 14: Global Test Accuracy = 0.6620
Round 15: Global Test Accuracy = 0.6710
Round 16: Global Test Accuracy = 0.6600
Round 17: Global Test Accuracy = 0.6740
Round 18: Global Test Accuracy = 0.6710
Round 19: Global Test Accuracy = 0.6790
Round 20: Global Test Accuracy = 0.6520
Round 21: Global Test Accuracy = 0.6680
Round 22: Global Test Accuracy = 0.6770
Round 23: Global Test Accuracy = 0.6650
Round 24: Global Test Accuracy = 0.6840
Round 25: Global Test Accuracy = 0.6770
Round 26: Global Test Accuracy = 0.6820
Round 27: Global Test Accuracy = 0.6820
Round 28: Global Test Accuracy = 0.6770
Round 29: Global Test Accuracy = 0.6750
Round 30: Global Test Accuracy = 0.6880
Round 31: Global Test Accuracy = 0.6850
Round 32: Global Test Accuracy = 0.6750
Round 33: Global Test Accuracy = 0.6680
Round 34: Global Test Accuracy = 0.6820
Round 35: Global Test Accuracy = 0.6870
Round 36: Global Test Accuracy = 0.6810
Round 37: Global Test Accuracy = 0.6740
Round 38: Global Test Accuracy = 0.6790
Round 39: Global Test Accuracy = 0.6770
Round 40: Global Test Accuracy = 0.6720
Round 41: Global Test Accuracy = 0.6800
Round 42: Global Test Accuracy = 0.6730
Round 43: Global Test Accuracy = 0.6810
Round 44: Global Test Accuracy = 0.6740
Round 45: Global Test Accuracy = 0.6780
Round 46: Global Test Accuracy = 0.6840
Round 47: Global Test Accuracy = 0.6840
Round 48: Global Test Accuracy = 0.6910
Round 49: Global Test Accuracy = 0.6790
Round 50: Global Test Accuracy = 0.6760
Round 51: Global Test Accuracy = 0.6850
Round 52: Global Test Accuracy = 0.6800
Round 53: Global Test Accuracy = 0.6870
Round 54: Global Test Accuracy = 0.6800
Round 55: Global Test Accuracy = 0.6820
Round 56: Global Test Accuracy = 0.6900
Round 57: Global Test Accuracy = 0.6770
Round 58: Global Test Accuracy = 0.6830
Round 59: Global Test Accuracy = 0.6860
Round 60: Global Test Accuracy = 0.6830
Round 61: Global Test Accuracy = 0.6800
Round 62: Global Test Accuracy = 0.6770
Round 63: Global Test Accuracy = 0.6780
Round 64: Global Test Accuracy = 0.6660
Round 65: Global Test Accuracy = 0.6670
Round 66: Global Test Accuracy = 0.6760
Round 67: Global Test Accuracy = 0.6720
Round 68: Global Test Accuracy = 0.6680
Round 69: Global Test Accuracy = 0.6790
Round 70: Global Test Accuracy = 0.6770
Round 71: Global Test Accuracy = 0.6720
Round 72: Global Test Accuracy = 0.6870
Round 73: Global Test Accuracy = 0.6850
Round 74: Global Test Accuracy = 0.6880
Round 75: Global Test Accuracy = 0.6820
Round 76: Global Test Accuracy = 0.6780
Round 77: Global Test Accuracy = 0.6850
Round 78: Global Test Accuracy = 0.6900
Round 79: Global Test Accuracy = 0.6820
Round 80: Global Test Accuracy = 0.6820
Round 81: Global Test Accuracy = 0.6800
Round 82: Global Test Accuracy = 0.6860
Round 83: Global Test Accuracy = 0.6870
Round 84: Global Test Accuracy = 0.6870
Round 85: Global Test Accuracy = 0.6880
Round 86: Global Test Accuracy = 0.6870
Round 87: Global Test Accuracy = 0.6840
Round 88: Global Test Accuracy = 0.6800
Round 89: Global Test Accuracy = 0.6830
Round 90: Global Test Accuracy = 0.6840
Round 91: Global Test Accuracy = 0.6860
Round 92: Global Test Accuracy = 0.6830
Round 93: Global Test Accuracy = 0.6750
Round 94: Global Test Accuracy = 0.6800
Round 95: Global Test Accuracy = 0.6840
Round 96: Global Test Accuracy = 0.6800
Round 97: Global Test Accuracy = 0.6860
Round 98: Global Test Accuracy = 0.6850
Round 99: Global Test Accuracy = 0.6820
Round 100: Global Test Accuracy = 0.6810
Round 101: Global Test Accuracy = 0.6910
Round 102: Global Test Accuracy = 0.6850
Round 103: Global Test Accuracy = 0.6900
Round 104: Global Test Accuracy = 0.6860
Round 105: Global Test Accuracy = 0.6860
Round 106: Global Test Accuracy = 0.6850
Round 107: Global Test Accuracy = 0.6840
Round 108: Global Test Accuracy = 0.6840
Round 109: Global Test Accuracy = 0.6720
Round 110: Global Test Accuracy = 0.6840
Round 111: Global Test Accuracy = 0.6860
Round 112: Global Test Accuracy = 0.6750
Round 113: Global Test Accuracy = 0.6890
Round 114: Global Test Accuracy = 0.6910
Round 115: Global Test Accuracy = 0.6870
Round 116: Global Test Accuracy = 0.6930
Round 117: Global Test Accuracy = 0.6900
Round 118: Global Test Accuracy = 0.6820
Round 119: Global Test Accuracy = 0.6890
Round 120: Global Test Accuracy = 0.6870
Round 121: Global Test Accuracy = 0.6910
Round 122: Global Test Accuracy = 0.6820
Round 123: Global Test Accuracy = 0.6870
Round 124: Global Test Accuracy = 0.6920
Round 125: Global Test Accuracy = 0.6840
Round 126: Global Test Accuracy = 0.6850
Round 127: Global Test Accuracy = 0.6840
Round 128: Global Test Accuracy = 0.6840
Round 129: Global Test Accuracy = 0.6880
Round 130: Global Test Accuracy = 0.6790
Round 131: Global Test Accuracy = 0.6800
Round 132: Global Test Accuracy = 0.6900
Round 133: Global Test Accuracy = 0.6790
Round 134: Global Test Accuracy = 0.6790
Round 135: Global Test Accuracy = 0.6810
Round 136: Global Test Accuracy = 0.6870
Round 137: Global Test Accuracy = 0.6860
Round 138: Global Test Accuracy = 0.6880
Round 139: Global Test Accuracy = 0.6820
Round 140: Global Test Accuracy = 0.6850
Round 141: Global Test Accuracy = 0.6870
Round 142: Global Test Accuracy = 0.6810
Round 143: Global Test Accuracy = 0.6810
Round 144: Global Test Accuracy = 0.6860
Round 145: Global Test Accuracy = 0.6820
Round 146: Global Test Accuracy = 0.6880
Round 147: Global Test Accuracy = 0.6880
Round 148: Global Test Accuracy = 0.6890
Round 149: Global Test Accuracy = 0.6870
Round 150: Global Test Accuracy = 0.6860
Round 151: Global Test Accuracy = 0.6930
Round 152: Global Test Accuracy = 0.6810
Round 153: Global Test Accuracy = 0.6810
Round 154: Global Test Accuracy = 0.6880
Round 155: Global Test Accuracy = 0.6860
Round 156: Global Test Accuracy = 0.6830
Round 157: Global Test Accuracy = 0.6840
Round 158: Global Test Accuracy = 0.6850
Round 159: Global Test Accuracy = 0.6840
Round 160: Global Test Accuracy = 0.6830
Round 161: Global Test Accuracy = 0.6820
Round 162: Global Test Accuracy = 0.6790
Round 163: Global Test Accuracy = 0.6840
Round 164: Global Test Accuracy = 0.6850
Round 165: Global Test Accuracy = 0.6840
Round 166: Global Test Accuracy = 0.6870
Round 167: Global Test Accuracy = 0.6880
Round 168: Global Test Accuracy = 0.6880
Round 169: Global Test Accuracy = 0.6840
Round 170: Global Test Accuracy = 0.6860
Round 171: Global Test Accuracy = 0.6850
Round 172: Global Test Accuracy = 0.6870
Round 173: Global Test Accuracy = 0.6830
Round 174: Global Test Accuracy = 0.6860
Round 175: Global Test Accuracy = 0.6870
Round 176: Global Test Accuracy = 0.6850
Round 177: Global Test Accuracy = 0.6800
Round 178: Global Test Accuracy = 0.6810
Round 179: Global Test Accuracy = 0.6800
Round 180: Global Test Accuracy = 0.6810
Round 181: Global Test Accuracy = 0.6910
Round 182: Global Test Accuracy = 0.6820
Round 183: Global Test Accuracy = 0.6840
Round 184: Global Test Accuracy = 0.6870
Round 185: Global Test Accuracy = 0.6880
Round 186: Global Test Accuracy = 0.6860
Round 187: Global Test Accuracy = 0.6840
Round 188: Global Test Accuracy = 0.6860
Round 189: Global Test Accuracy = 0.6880
Round 190: Global Test Accuracy = 0.6910
Round 191: Global Test Accuracy = 0.6900
Round 192: Global Test Accuracy = 0.6850
Round 193: Global Test Accuracy = 0.6840
Round 194: Global Test Accuracy = 0.6830
Round 195: Global Test Accuracy = 0.6850
Round 196: Global Test Accuracy = 0.6800
Round 197: Global Test Accuracy = 0.6790
Round 198: Global Test Accuracy = 0.6850
Round 199: Global Test Accuracy = 0.6860
Round 200: Global Test Accuracy = 0.6840
//train_time: 13009.239000000001 ms//end
//Log Max memory for Large1: 6056972288.0 //end
//Log Max memory for Large2: 6192099328.0 //end
//Log Max memory for Large3: 5821108224.0 //end
//Log Max memory for Large4: 6535106560.0 //end
//Log Max memory for Server: 18251456512.0 //end
//Log Large1 network: 99310990.0 //end
//Log Large2 network: 148626821.0 //end
//Log Large3 network: 99201387.0 //end
//Log Large4 network: 148544743.0 //end
//Log Server network: 493989260.0 //end
//Log Total Actual Train Comm Cost: 943.83 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 1.181108494400978
Average test accuracy, 0.684
//Log Theoretical Pretrain Comm Cost: 607.82 MB //end
//Log Theoretical Train Comm Cost: 905.85 MB //end
[36m(Trainer pid=116001, ip=192.168.39.156)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=116001, ip=192.168.39.156)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: pubmed, Trainers: 10, Distribution: average, IID Beta: 10000.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'pubmed', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 10000.0, 'distribution_type': 'average', 'gpu': False}
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x to ./data/pubmed/raw/ind.pubmed.x...
Downloaded ./data/pubmed/raw/ind.pubmed.x
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx to ./data/pubmed/raw/ind.pubmed.tx...
Downloaded ./data/pubmed/raw/ind.pubmed.tx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx to ./data/pubmed/raw/ind.pubmed.allx...
Downloaded ./data/pubmed/raw/ind.pubmed.allx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y to ./data/pubmed/raw/ind.pubmed.y...
Downloaded ./data/pubmed/raw/ind.pubmed.y
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty to ./data/pubmed/raw/ind.pubmed.ty...
Downloaded ./data/pubmed/raw/ind.pubmed.ty
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally to ./data/pubmed/raw/ind.pubmed.ally...
Downloaded ./data/pubmed/raw/ind.pubmed.ally
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph to ./data/pubmed/raw/ind.pubmed.graph...
Downloaded ./data/pubmed/raw/ind.pubmed.graph
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index to ./data/pubmed/raw/ind.pubmed.test.index...
Downloaded ./data/pubmed/raw/ind.pubmed.test.index
Initialization start: network data collected.
2025-05-15 03:01:28,439	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 03:01:28,439	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 03:01:28,446	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=120679, ip=192.168.14.62)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=120679, ip=192.168.14.62)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5587.802 ms //end
//Log Large1 init network: 131651.0 //end
//Log Large2 init network: 130232.0 //end
//Log Large3 init network: 139001.0 //end
//Log Large4 init network: 118104.0 //end
//Log Server init network: 41244016.0 //end
//Log Initialization Communication Cost (MB): 39.83 //end
Pretrain start time recorded.
//pretrain_time: 5.794 ms//end
//Log Max memory for Large1: 6420672512.0 //end
//Log Max memory for Large2: 5706674176.0 //end
//Log Max memory for Large3: 6207188992.0 //end
//Log Max memory for Large4: 6058164224.0 //end
//Log Max memory for Server: 18193063936.0 //end
//Log Large1 network: 694594.0 //end
//Log Large2 network: 654437.0 //end
//Log Large3 network: 695854.0 //end
//Log Large4 network: 589198.0 //end
//Log Server network: 1514698.0 //end
//Log Total Actual Pretrain Comm Cost: 3.96 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.3920
Round 2: Global Test Accuracy = 0.3910
Round 3: Global Test Accuracy = 0.3670
Round 4: Global Test Accuracy = 0.4000
Round 5: Global Test Accuracy = 0.4000
Round 6: Global Test Accuracy = 0.4100
Round 7: Global Test Accuracy = 0.4080
Round 8: Global Test Accuracy = 0.4080
Round 9: Global Test Accuracy = 0.4090
Round 10: Global Test Accuracy = 0.4070
Round 11: Global Test Accuracy = 0.4080
Round 12: Global Test Accuracy = 0.4160
Round 13: Global Test Accuracy = 0.4200
Round 14: Global Test Accuracy = 0.4180
Round 15: Global Test Accuracy = 0.4120
Round 16: Global Test Accuracy = 0.4160
Round 17: Global Test Accuracy = 0.4090
Round 18: Global Test Accuracy = 0.4070
Round 19: Global Test Accuracy = 0.4140
Round 20: Global Test Accuracy = 0.4200
Round 21: Global Test Accuracy = 0.4210
Round 22: Global Test Accuracy = 0.4200
Round 23: Global Test Accuracy = 0.4190
Round 24: Global Test Accuracy = 0.4190
Round 25: Global Test Accuracy = 0.4210
Round 26: Global Test Accuracy = 0.4180
Round 27: Global Test Accuracy = 0.4180
Round 28: Global Test Accuracy = 0.4180
Round 29: Global Test Accuracy = 0.4190
Round 30: Global Test Accuracy = 0.4190
Round 31: Global Test Accuracy = 0.4180
Round 32: Global Test Accuracy = 0.4190
Round 33: Global Test Accuracy = 0.4170
Round 34: Global Test Accuracy = 0.4170
Round 35: Global Test Accuracy = 0.4170
Round 36: Global Test Accuracy = 0.4220
Round 37: Global Test Accuracy = 0.4190
Round 38: Global Test Accuracy = 0.4230
Round 39: Global Test Accuracy = 0.4170
Round 40: Global Test Accuracy = 0.4170
Round 41: Global Test Accuracy = 0.4180
Round 42: Global Test Accuracy = 0.4200
Round 43: Global Test Accuracy = 0.4190
Round 44: Global Test Accuracy = 0.4170
Round 45: Global Test Accuracy = 0.4160
Round 46: Global Test Accuracy = 0.4160
Round 47: Global Test Accuracy = 0.4160
Round 48: Global Test Accuracy = 0.4160
Round 49: Global Test Accuracy = 0.4180
Round 50: Global Test Accuracy = 0.4180
Round 51: Global Test Accuracy = 0.4160
Round 52: Global Test Accuracy = 0.4160
Round 53: Global Test Accuracy = 0.4160
Round 54: Global Test Accuracy = 0.4180
Round 55: Global Test Accuracy = 0.4160
Round 56: Global Test Accuracy = 0.4160
Round 57: Global Test Accuracy = 0.4190
Round 58: Global Test Accuracy = 0.4160
Round 59: Global Test Accuracy = 0.4150
Round 60: Global Test Accuracy = 0.4160
Round 61: Global Test Accuracy = 0.4160
Round 62: Global Test Accuracy = 0.4150
Round 63: Global Test Accuracy = 0.4160
Round 64: Global Test Accuracy = 0.4170
Round 65: Global Test Accuracy = 0.4170
Round 66: Global Test Accuracy = 0.4170
Round 67: Global Test Accuracy = 0.4170
Round 68: Global Test Accuracy = 0.4170
Round 69: Global Test Accuracy = 0.4160
Round 70: Global Test Accuracy = 0.4170
Round 71: Global Test Accuracy = 0.4170
Round 72: Global Test Accuracy = 0.4180
Round 73: Global Test Accuracy = 0.4170
Round 74: Global Test Accuracy = 0.4180
Round 75: Global Test Accuracy = 0.4180
Round 76: Global Test Accuracy = 0.4180
Round 77: Global Test Accuracy = 0.4180
Round 78: Global Test Accuracy = 0.4180
Round 79: Global Test Accuracy = 0.4180
Round 80: Global Test Accuracy = 0.4170
Round 81: Global Test Accuracy = 0.4180
Round 82: Global Test Accuracy = 0.4170
Round 83: Global Test Accuracy = 0.4170
Round 84: Global Test Accuracy = 0.4170
Round 85: Global Test Accuracy = 0.4170
Round 86: Global Test Accuracy = 0.4170
Round 87: Global Test Accuracy = 0.4190
Round 88: Global Test Accuracy = 0.4170
Round 89: Global Test Accuracy = 0.4180
Round 90: Global Test Accuracy = 0.4180
Round 91: Global Test Accuracy = 0.4180
Round 92: Global Test Accuracy = 0.4180
Round 93: Global Test Accuracy = 0.4180
Round 94: Global Test Accuracy = 0.4180
Round 95: Global Test Accuracy = 0.4190
Round 96: Global Test Accuracy = 0.4180
Round 97: Global Test Accuracy = 0.4190
Round 98: Global Test Accuracy = 0.4210
Round 99: Global Test Accuracy = 0.4180
Round 100: Global Test Accuracy = 0.4180
Round 101: Global Test Accuracy = 0.4220
Round 102: Global Test Accuracy = 0.4200
Round 103: Global Test Accuracy = 0.4190
Round 104: Global Test Accuracy = 0.4190
Round 105: Global Test Accuracy = 0.4200
Round 106: Global Test Accuracy = 0.4190
Round 107: Global Test Accuracy = 0.4190
Round 108: Global Test Accuracy = 0.4190
Round 109: Global Test Accuracy = 0.4180
Round 110: Global Test Accuracy = 0.4200
Round 111: Global Test Accuracy = 0.4190
Round 112: Global Test Accuracy = 0.4190
Round 113: Global Test Accuracy = 0.4180
Round 114: Global Test Accuracy = 0.4200
Round 115: Global Test Accuracy = 0.4190
Round 116: Global Test Accuracy = 0.4210
Round 117: Global Test Accuracy = 0.4210
Round 118: Global Test Accuracy = 0.4250
Round 119: Global Test Accuracy = 0.4220
Round 120: Global Test Accuracy = 0.4280
Round 121: Global Test Accuracy = 0.4310
Round 122: Global Test Accuracy = 0.4300
Round 123: Global Test Accuracy = 0.4300
Round 124: Global Test Accuracy = 0.4300
Round 125: Global Test Accuracy = 0.4300
Round 126: Global Test Accuracy = 0.4290
Round 127: Global Test Accuracy = 0.4300
Round 128: Global Test Accuracy = 0.4240
Round 129: Global Test Accuracy = 0.4220
Round 130: Global Test Accuracy = 0.4250
Round 131: Global Test Accuracy = 0.4250
Round 132: Global Test Accuracy = 0.4290
Round 133: Global Test Accuracy = 0.4300
Round 134: Global Test Accuracy = 0.4340
Round 135: Global Test Accuracy = 0.4340
Round 136: Global Test Accuracy = 0.4270
Round 137: Global Test Accuracy = 0.4220
Round 138: Global Test Accuracy = 0.4220
Round 139: Global Test Accuracy = 0.4290
Round 140: Global Test Accuracy = 0.4250
Round 141: Global Test Accuracy = 0.4310
Round 142: Global Test Accuracy = 0.4420
Round 143: Global Test Accuracy = 0.4420
Round 144: Global Test Accuracy = 0.4460
Round 145: Global Test Accuracy = 0.4450
Round 146: Global Test Accuracy = 0.4470
Round 147: Global Test Accuracy = 0.4590
Round 148: Global Test Accuracy = 0.4480
Round 149: Global Test Accuracy = 0.4350
Round 150: Global Test Accuracy = 0.4440
Round 151: Global Test Accuracy = 0.4430
Round 152: Global Test Accuracy = 0.4410
Round 153: Global Test Accuracy = 0.4460
Round 154: Global Test Accuracy = 0.4370
Round 155: Global Test Accuracy = 0.4400
Round 156: Global Test Accuracy = 0.4470
Round 157: Global Test Accuracy = 0.4390
Round 158: Global Test Accuracy = 0.4280
Round 159: Global Test Accuracy = 0.4290
Round 160: Global Test Accuracy = 0.4280
Round 161: Global Test Accuracy = 0.4400
Round 162: Global Test Accuracy = 0.4450
Round 163: Global Test Accuracy = 0.4430
Round 164: Global Test Accuracy = 0.4260
Round 165: Global Test Accuracy = 0.4260
Round 166: Global Test Accuracy = 0.4310
Round 167: Global Test Accuracy = 0.4500
Round 168: Global Test Accuracy = 0.4510
Round 169: Global Test Accuracy = 0.4490
Round 170: Global Test Accuracy = 0.4670
Round 171: Global Test Accuracy = 0.4550
Round 172: Global Test Accuracy = 0.4700
Round 173: Global Test Accuracy = 0.4730
Round 174: Global Test Accuracy = 0.4660
Round 175: Global Test Accuracy = 0.4590
Round 176: Global Test Accuracy = 0.4560
Round 177: Global Test Accuracy = 0.4590
Round 178: Global Test Accuracy = 0.4590
Round 179: Global Test Accuracy = 0.4680
Round 180: Global Test Accuracy = 0.4660
Round 181: Global Test Accuracy = 0.4570
Round 182: Global Test Accuracy = 0.4500
Round 183: Global Test Accuracy = 0.4590
Round 184: Global Test Accuracy = 0.4450
Round 185: Global Test Accuracy = 0.4670
Round 186: Global Test Accuracy = 0.4490
Round 187: Global Test Accuracy = 0.4600
Round 188: Global Test Accuracy = 0.4640
Round 189: Global Test Accuracy = 0.4590
Round 190: Global Test Accuracy = 0.4580
Round 191: Global Test Accuracy = 0.4600
Round 192: Global Test Accuracy = 0.4480
Round 193: Global Test Accuracy = 0.4710
Round 194: Global Test Accuracy = 0.4700
Round 195: Global Test Accuracy = 0.4840
Round 196: Global Test Accuracy = 0.4830
Round 197: Global Test Accuracy = 0.4730
Round 198: Global Test Accuracy = 0.4770
Round 199: Global Test Accuracy = 0.4880
Round 200: Global Test Accuracy = 0.4900
//train_time: 4809.686 ms//end
//Log Max memory for Large1: 6446415872.0 //end
//Log Max memory for Large2: 5725274112.0 //end
//Log Max memory for Large3: 6233821184.0 //end
//Log Max memory for Large4: 6075777024.0 //end
//Log Max memory for Server: 18235031552.0 //end
//Log Large1 network: 22408320.0 //end
//Log Large2 network: 15141797.0 //end
//Log Large3 network: 22387336.0 //end
//Log Large4 network: 15090900.0 //end
//Log Server network: 75265553.0 //end
//Log Total Actual Train Comm Cost: 143.33 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 1.0683445255756379
Average test accuracy, 0.49
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 123.09 MB //end
[36m(Trainer pid=116551, ip=192.168.39.156)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=116551, ip=192.168.39.156)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: pubmed, Trainers: 10, Distribution: average, IID Beta: 100.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'pubmed', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 100.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/pubmed/raw/ind.pubmed.x
File already exists: ./data/pubmed/raw/ind.pubmed.tx
File already exists: ./data/pubmed/raw/ind.pubmed.allx
File already exists: ./data/pubmed/raw/ind.pubmed.y
File already exists: ./data/pubmed/raw/ind.pubmed.ty
File already exists: ./data/pubmed/raw/ind.pubmed.ally
File already exists: ./data/pubmed/raw/ind.pubmed.graph
File already exists: ./data/pubmed/raw/ind.pubmed.test.index
Initialization start: network data collected.
2025-05-15 03:02:50,068	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 03:02:50,068	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 03:02:50,073	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=117301, ip=192.168.14.54)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=117301, ip=192.168.14.54)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5937.214 ms //end
//Log Large1 init network: 118889.0 //end
//Log Large2 init network: 140211.0 //end
//Log Large3 init network: 114032.0 //end
//Log Large4 init network: 126119.0 //end
//Log Server init network: 41104146.0 //end
//Log Initialization Communication Cost (MB): 39.68 //end
Pretrain start time recorded.
//pretrain_time: 5.3309999999999995 ms//end
//Log Max memory for Large1: 6008504320.0 //end
//Log Max memory for Large2: 6140215296.0 //end
//Log Max memory for Large3: 5791531008.0 //end
//Log Max memory for Large4: 6486867968.0 //end
//Log Max memory for Server: 18241351680.0 //end
//Log Large1 network: 638507.0 //end
//Log Large2 network: 746444.0 //end
//Log Large3 network: 634538.0 //end
//Log Large4 network: 747661.0 //end
//Log Server network: 1519449.0 //end
//Log Total Actual Pretrain Comm Cost: 4.09 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.3900
Round 2: Global Test Accuracy = 0.3830
Round 3: Global Test Accuracy = 0.3990
Round 4: Global Test Accuracy = 0.3740
Round 5: Global Test Accuracy = 0.3790
Round 6: Global Test Accuracy = 0.3690
Round 7: Global Test Accuracy = 0.3740
Round 8: Global Test Accuracy = 0.3730
Round 9: Global Test Accuracy = 0.3710
Round 10: Global Test Accuracy = 0.3750
Round 11: Global Test Accuracy = 0.3560
Round 12: Global Test Accuracy = 0.3660
Round 13: Global Test Accuracy = 0.3830
Round 14: Global Test Accuracy = 0.3820
Round 15: Global Test Accuracy = 0.3800
Round 16: Global Test Accuracy = 0.3770
Round 17: Global Test Accuracy = 0.3830
Round 18: Global Test Accuracy = 0.3860
Round 19: Global Test Accuracy = 0.3800
Round 20: Global Test Accuracy = 0.3790
Round 21: Global Test Accuracy = 0.3680
Round 22: Global Test Accuracy = 0.3620
Round 23: Global Test Accuracy = 0.3600
Round 24: Global Test Accuracy = 0.3550
Round 25: Global Test Accuracy = 0.3560
Round 26: Global Test Accuracy = 0.3600
Round 27: Global Test Accuracy = 0.3710
Round 28: Global Test Accuracy = 0.3660
Round 29: Global Test Accuracy = 0.3830
Round 30: Global Test Accuracy = 0.3870
Round 31: Global Test Accuracy = 0.3620
Round 32: Global Test Accuracy = 0.3860
Round 33: Global Test Accuracy = 0.3900
Round 34: Global Test Accuracy = 0.3940
Round 35: Global Test Accuracy = 0.3920
Round 36: Global Test Accuracy = 0.3930
Round 37: Global Test Accuracy = 0.3990
Round 38: Global Test Accuracy = 0.3960
Round 39: Global Test Accuracy = 0.4000
Round 40: Global Test Accuracy = 0.3980
Round 41: Global Test Accuracy = 0.3950
Round 42: Global Test Accuracy = 0.4000
Round 43: Global Test Accuracy = 0.4000
Round 44: Global Test Accuracy = 0.3960
Round 45: Global Test Accuracy = 0.4030
Round 46: Global Test Accuracy = 0.3900
Round 47: Global Test Accuracy = 0.3980
Round 48: Global Test Accuracy = 0.4000
Round 49: Global Test Accuracy = 0.4040
Round 50: Global Test Accuracy = 0.4090
Round 51: Global Test Accuracy = 0.4050
Round 52: Global Test Accuracy = 0.4050
Round 53: Global Test Accuracy = 0.4050
Round 54: Global Test Accuracy = 0.4200
Round 55: Global Test Accuracy = 0.4180
Round 56: Global Test Accuracy = 0.4070
Round 57: Global Test Accuracy = 0.4110
Round 58: Global Test Accuracy = 0.4090
Round 59: Global Test Accuracy = 0.4160
Round 60: Global Test Accuracy = 0.4160
Round 61: Global Test Accuracy = 0.4210
Round 62: Global Test Accuracy = 0.4230
Round 63: Global Test Accuracy = 0.4210
Round 64: Global Test Accuracy = 0.4290
Round 65: Global Test Accuracy = 0.4380
Round 66: Global Test Accuracy = 0.4360
Round 67: Global Test Accuracy = 0.4300
Round 68: Global Test Accuracy = 0.4360
Round 69: Global Test Accuracy = 0.4320
Round 70: Global Test Accuracy = 0.4210
Round 71: Global Test Accuracy = 0.4290
Round 72: Global Test Accuracy = 0.4250
Round 73: Global Test Accuracy = 0.4330
Round 74: Global Test Accuracy = 0.4360
Round 75: Global Test Accuracy = 0.4270
Round 76: Global Test Accuracy = 0.4350
Round 77: Global Test Accuracy = 0.4380
Round 78: Global Test Accuracy = 0.4420
Round 79: Global Test Accuracy = 0.4420
Round 80: Global Test Accuracy = 0.4410
Round 81: Global Test Accuracy = 0.4450
Round 82: Global Test Accuracy = 0.4470
Round 83: Global Test Accuracy = 0.4510
Round 84: Global Test Accuracy = 0.4550
Round 85: Global Test Accuracy = 0.4610
Round 86: Global Test Accuracy = 0.4660
Round 87: Global Test Accuracy = 0.4630
Round 88: Global Test Accuracy = 0.4680
Round 89: Global Test Accuracy = 0.4670
Round 90: Global Test Accuracy = 0.4710
Round 91: Global Test Accuracy = 0.4650
Round 92: Global Test Accuracy = 0.4740
Round 93: Global Test Accuracy = 0.4560
Round 94: Global Test Accuracy = 0.4680
Round 95: Global Test Accuracy = 0.4600
Round 96: Global Test Accuracy = 0.4620
Round 97: Global Test Accuracy = 0.4640
Round 98: Global Test Accuracy = 0.4780
Round 99: Global Test Accuracy = 0.4770
Round 100: Global Test Accuracy = 0.4690
Round 101: Global Test Accuracy = 0.4820
Round 102: Global Test Accuracy = 0.4750
Round 103: Global Test Accuracy = 0.4770
Round 104: Global Test Accuracy = 0.4780
Round 105: Global Test Accuracy = 0.4840
Round 106: Global Test Accuracy = 0.4820
Round 107: Global Test Accuracy = 0.4780
Round 108: Global Test Accuracy = 0.4830
Round 109: Global Test Accuracy = 0.4910
Round 110: Global Test Accuracy = 0.4880
Round 111: Global Test Accuracy = 0.4910
Round 112: Global Test Accuracy = 0.4890
Round 113: Global Test Accuracy = 0.4850
Round 114: Global Test Accuracy = 0.4850
Round 115: Global Test Accuracy = 0.4950
Round 116: Global Test Accuracy = 0.4970
Round 117: Global Test Accuracy = 0.4990
Round 118: Global Test Accuracy = 0.5030
Round 119: Global Test Accuracy = 0.5020
Round 120: Global Test Accuracy = 0.5130
Round 121: Global Test Accuracy = 0.5030
Round 122: Global Test Accuracy = 0.5160
Round 123: Global Test Accuracy = 0.5170
Round 124: Global Test Accuracy = 0.5210
Round 125: Global Test Accuracy = 0.5230
Round 126: Global Test Accuracy = 0.5200
Round 127: Global Test Accuracy = 0.5080
Round 128: Global Test Accuracy = 0.5110
Round 129: Global Test Accuracy = 0.5170
Round 130: Global Test Accuracy = 0.5260
Round 131: Global Test Accuracy = 0.5250
Round 132: Global Test Accuracy = 0.5280
Round 133: Global Test Accuracy = 0.5390
Round 134: Global Test Accuracy = 0.5350
Round 135: Global Test Accuracy = 0.5340
Round 136: Global Test Accuracy = 0.5350
Round 137: Global Test Accuracy = 0.5420
Round 138: Global Test Accuracy = 0.5370
Round 139: Global Test Accuracy = 0.5340
Round 140: Global Test Accuracy = 0.5410
Round 141: Global Test Accuracy = 0.5340
Round 142: Global Test Accuracy = 0.5350
Round 143: Global Test Accuracy = 0.5370
Round 144: Global Test Accuracy = 0.5400
Round 145: Global Test Accuracy = 0.5600
Round 146: Global Test Accuracy = 0.5560
Round 147: Global Test Accuracy = 0.5540
Round 148: Global Test Accuracy = 0.5400
Round 149: Global Test Accuracy = 0.5540
Round 150: Global Test Accuracy = 0.5470
Round 151: Global Test Accuracy = 0.5390
Round 152: Global Test Accuracy = 0.5610
Round 153: Global Test Accuracy = 0.5620
Round 154: Global Test Accuracy = 0.5670
Round 155: Global Test Accuracy = 0.5550
Round 156: Global Test Accuracy = 0.5650
Round 157: Global Test Accuracy = 0.5580
Round 158: Global Test Accuracy = 0.5680
Round 159: Global Test Accuracy = 0.5690
Round 160: Global Test Accuracy = 0.5690
Round 161: Global Test Accuracy = 0.5570
Round 162: Global Test Accuracy = 0.5670
Round 163: Global Test Accuracy = 0.5650
Round 164: Global Test Accuracy = 0.5630
Round 165: Global Test Accuracy = 0.5550
Round 166: Global Test Accuracy = 0.5440
Round 167: Global Test Accuracy = 0.5600
Round 168: Global Test Accuracy = 0.5560
Round 169: Global Test Accuracy = 0.5610
Round 170: Global Test Accuracy = 0.5670
Round 171: Global Test Accuracy = 0.5630
Round 172: Global Test Accuracy = 0.5580
Round 173: Global Test Accuracy = 0.5640
Round 174: Global Test Accuracy = 0.5710
Round 175: Global Test Accuracy = 0.5710
Round 176: Global Test Accuracy = 0.5700
Round 177: Global Test Accuracy = 0.5640
Round 178: Global Test Accuracy = 0.5670
Round 179: Global Test Accuracy = 0.5730
Round 180: Global Test Accuracy = 0.5640
Round 181: Global Test Accuracy = 0.5670
Round 182: Global Test Accuracy = 0.5620
Round 183: Global Test Accuracy = 0.5630
Round 184: Global Test Accuracy = 0.5730
Round 185: Global Test Accuracy = 0.5710
Round 186: Global Test Accuracy = 0.5730
Round 187: Global Test Accuracy = 0.5820
Round 188: Global Test Accuracy = 0.5740
Round 189: Global Test Accuracy = 0.5790
Round 190: Global Test Accuracy = 0.5850
Round 191: Global Test Accuracy = 0.5780
Round 192: Global Test Accuracy = 0.5850
Round 193: Global Test Accuracy = 0.5750
Round 194: Global Test Accuracy = 0.5870
Round 195: Global Test Accuracy = 0.5860
Round 196: Global Test Accuracy = 0.5780
Round 197: Global Test Accuracy = 0.5820
Round 198: Global Test Accuracy = 0.5810
Round 199: Global Test Accuracy = 0.5790
Round 200: Global Test Accuracy = 0.5780
//train_time: 4790.736 ms//end
//Log Max memory for Large1: 6029447168.0 //end
//Log Max memory for Large2: 6168145920.0 //end
//Log Max memory for Large3: 5811531776.0 //end
//Log Max memory for Large4: 6512820224.0 //end
//Log Max memory for Server: 18273505280.0 //end
//Log Large1 network: 15071166.0 //end
//Log Large2 network: 22524959.0 //end
//Log Large3 network: 15025460.0 //end
//Log Large4 network: 22331577.0 //end
//Log Server network: 75168371.0 //end
//Log Total Actual Train Comm Cost: 143.17 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 1.0740686126947403
Average test accuracy, 0.578
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 123.09 MB //end
[36m(Trainer pid=121194, ip=192.168.14.62)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=121194, ip=192.168.14.62)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: pubmed, Trainers: 10, Distribution: average, IID Beta: 10.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'pubmed', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/pubmed/raw/ind.pubmed.x
File already exists: ./data/pubmed/raw/ind.pubmed.tx
File already exists: ./data/pubmed/raw/ind.pubmed.allx
File already exists: ./data/pubmed/raw/ind.pubmed.y
File already exists: ./data/pubmed/raw/ind.pubmed.ty
File already exists: ./data/pubmed/raw/ind.pubmed.ally
File already exists: ./data/pubmed/raw/ind.pubmed.graph
File already exists: ./data/pubmed/raw/ind.pubmed.test.index
Initialization start: network data collected.
2025-05-15 03:04:12,047	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 03:04:12,047	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 03:04:12,053	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=117811, ip=192.168.14.54)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=117811, ip=192.168.14.54)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5662.433 ms //end
//Log Large1 init network: 176877.0 //end
//Log Large2 init network: 118837.0 //end
//Log Large3 init network: 197844.0 //end
//Log Large4 init network: 155559.0 //end
//Log Server init network: 41093412.0 //end
//Log Initialization Communication Cost (MB): 39.81 //end
Pretrain start time recorded.
//pretrain_time: 5.525 ms//end
//Log Max memory for Large1: 6434967552.0 //end
//Log Max memory for Large2: 5728362496.0 //end
//Log Max memory for Large3: 6218579968.0 //end
//Log Max memory for Large4: 6065901568.0 //end
//Log Max memory for Server: 18288095232.0 //end
//Log Large1 network: 690256.0 //end
//Log Large2 network: 708927.0 //end
//Log Large3 network: 675971.0 //end
//Log Large4 network: 587522.0 //end
//Log Server network: 1450859.0 //end
//Log Total Actual Pretrain Comm Cost: 3.92 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.3760
Round 2: Global Test Accuracy = 0.3820
Round 3: Global Test Accuracy = 0.3700
Round 4: Global Test Accuracy = 0.3640
Round 5: Global Test Accuracy = 0.3600
Round 6: Global Test Accuracy = 0.3600
Round 7: Global Test Accuracy = 0.3440
Round 8: Global Test Accuracy = 0.3500
Round 9: Global Test Accuracy = 0.3390
Round 10: Global Test Accuracy = 0.3440
Round 11: Global Test Accuracy = 0.3560
Round 12: Global Test Accuracy = 0.3650
Round 13: Global Test Accuracy = 0.3700
Round 14: Global Test Accuracy = 0.3680
Round 15: Global Test Accuracy = 0.3600
Round 16: Global Test Accuracy = 0.3680
Round 17: Global Test Accuracy = 0.3710
Round 18: Global Test Accuracy = 0.3720
Round 19: Global Test Accuracy = 0.3720
Round 20: Global Test Accuracy = 0.3670
Round 21: Global Test Accuracy = 0.3600
Round 22: Global Test Accuracy = 0.3660
Round 23: Global Test Accuracy = 0.3650
Round 24: Global Test Accuracy = 0.3700
Round 25: Global Test Accuracy = 0.3920
Round 26: Global Test Accuracy = 0.3960
Round 27: Global Test Accuracy = 0.3760
Round 28: Global Test Accuracy = 0.3930
Round 29: Global Test Accuracy = 0.3820
Round 30: Global Test Accuracy = 0.4060
Round 31: Global Test Accuracy = 0.3850
Round 32: Global Test Accuracy = 0.4120
Round 33: Global Test Accuracy = 0.3980
Round 34: Global Test Accuracy = 0.4010
Round 35: Global Test Accuracy = 0.4020
Round 36: Global Test Accuracy = 0.3950
Round 37: Global Test Accuracy = 0.3690
Round 38: Global Test Accuracy = 0.3730
Round 39: Global Test Accuracy = 0.3730
Round 40: Global Test Accuracy = 0.3680
Round 41: Global Test Accuracy = 0.3660
Round 42: Global Test Accuracy = 0.3570
Round 43: Global Test Accuracy = 0.3590
Round 44: Global Test Accuracy = 0.3660
Round 45: Global Test Accuracy = 0.3510
Round 46: Global Test Accuracy = 0.3700
Round 47: Global Test Accuracy = 0.3610
Round 48: Global Test Accuracy = 0.3730
Round 49: Global Test Accuracy = 0.3640
Round 50: Global Test Accuracy = 0.3700
Round 51: Global Test Accuracy = 0.3750
Round 52: Global Test Accuracy = 0.3740
Round 53: Global Test Accuracy = 0.4160
Round 54: Global Test Accuracy = 0.4180
Round 55: Global Test Accuracy = 0.4100
Round 56: Global Test Accuracy = 0.4250
Round 57: Global Test Accuracy = 0.4320
Round 58: Global Test Accuracy = 0.4320
Round 59: Global Test Accuracy = 0.4310
Round 60: Global Test Accuracy = 0.4350
Round 61: Global Test Accuracy = 0.4370
Round 62: Global Test Accuracy = 0.4370
Round 63: Global Test Accuracy = 0.4380
Round 64: Global Test Accuracy = 0.4420
Round 65: Global Test Accuracy = 0.4380
Round 66: Global Test Accuracy = 0.4420
Round 67: Global Test Accuracy = 0.4440
Round 68: Global Test Accuracy = 0.4450
Round 69: Global Test Accuracy = 0.4440
Round 70: Global Test Accuracy = 0.4430
Round 71: Global Test Accuracy = 0.4380
Round 72: Global Test Accuracy = 0.4400
Round 73: Global Test Accuracy = 0.4250
Round 74: Global Test Accuracy = 0.4250
Round 75: Global Test Accuracy = 0.4290
Round 76: Global Test Accuracy = 0.4310
Round 77: Global Test Accuracy = 0.4310
Round 78: Global Test Accuracy = 0.4120
Round 79: Global Test Accuracy = 0.4170
Round 80: Global Test Accuracy = 0.4250
Round 81: Global Test Accuracy = 0.4250
Round 82: Global Test Accuracy = 0.4260
Round 83: Global Test Accuracy = 0.4310
Round 84: Global Test Accuracy = 0.4160
Round 85: Global Test Accuracy = 0.4090
Round 86: Global Test Accuracy = 0.4280
Round 87: Global Test Accuracy = 0.4110
Round 88: Global Test Accuracy = 0.4310
Round 89: Global Test Accuracy = 0.4370
Round 90: Global Test Accuracy = 0.4400
Round 91: Global Test Accuracy = 0.4390
Round 92: Global Test Accuracy = 0.4360
Round 93: Global Test Accuracy = 0.4380
Round 94: Global Test Accuracy = 0.4410
Round 95: Global Test Accuracy = 0.4290
Round 96: Global Test Accuracy = 0.4440
Round 97: Global Test Accuracy = 0.4400
Round 98: Global Test Accuracy = 0.4410
Round 99: Global Test Accuracy = 0.4470
Round 100: Global Test Accuracy = 0.4300
Round 101: Global Test Accuracy = 0.4110
Round 102: Global Test Accuracy = 0.4070
Round 103: Global Test Accuracy = 0.4280
Round 104: Global Test Accuracy = 0.4250
Round 105: Global Test Accuracy = 0.4240
Round 106: Global Test Accuracy = 0.4240
Round 107: Global Test Accuracy = 0.3870
Round 108: Global Test Accuracy = 0.3960
Round 109: Global Test Accuracy = 0.4160
Round 110: Global Test Accuracy = 0.4210
Round 111: Global Test Accuracy = 0.4170
Round 112: Global Test Accuracy = 0.4200
Round 113: Global Test Accuracy = 0.4110
Round 114: Global Test Accuracy = 0.3860
Round 115: Global Test Accuracy = 0.3810
Round 116: Global Test Accuracy = 0.3860
Round 117: Global Test Accuracy = 0.3750
Round 118: Global Test Accuracy = 0.3840
Round 119: Global Test Accuracy = 0.3780
Round 120: Global Test Accuracy = 0.4020
Round 121: Global Test Accuracy = 0.4110
Round 122: Global Test Accuracy = 0.4040
Round 123: Global Test Accuracy = 0.3950
Round 124: Global Test Accuracy = 0.4500
Round 125: Global Test Accuracy = 0.4600
Round 126: Global Test Accuracy = 0.4600
Round 127: Global Test Accuracy = 0.4630
Round 128: Global Test Accuracy = 0.4660
Round 129: Global Test Accuracy = 0.4370
Round 130: Global Test Accuracy = 0.4340
Round 131: Global Test Accuracy = 0.4460
Round 132: Global Test Accuracy = 0.4480
Round 133: Global Test Accuracy = 0.4490
Round 134: Global Test Accuracy = 0.4660
Round 135: Global Test Accuracy = 0.4430
Round 136: Global Test Accuracy = 0.4400
Round 137: Global Test Accuracy = 0.4130
Round 138: Global Test Accuracy = 0.4480
Round 139: Global Test Accuracy = 0.4470
Round 140: Global Test Accuracy = 0.4360
Round 141: Global Test Accuracy = 0.4180
Round 142: Global Test Accuracy = 0.4330
Round 143: Global Test Accuracy = 0.4040
Round 144: Global Test Accuracy = 0.4000
Round 145: Global Test Accuracy = 0.4090
Round 146: Global Test Accuracy = 0.4430
Round 147: Global Test Accuracy = 0.4730
Round 148: Global Test Accuracy = 0.4700
Round 149: Global Test Accuracy = 0.4660
Round 150: Global Test Accuracy = 0.4710
Round 151: Global Test Accuracy = 0.4550
Round 152: Global Test Accuracy = 0.4530
Round 153: Global Test Accuracy = 0.4500
Round 154: Global Test Accuracy = 0.4330
Round 155: Global Test Accuracy = 0.4440
Round 156: Global Test Accuracy = 0.4530
Round 157: Global Test Accuracy = 0.4320
Round 158: Global Test Accuracy = 0.4220
Round 159: Global Test Accuracy = 0.4230
Round 160: Global Test Accuracy = 0.3930
Round 161: Global Test Accuracy = 0.3970
Round 162: Global Test Accuracy = 0.4490
Round 163: Global Test Accuracy = 0.4470
Round 164: Global Test Accuracy = 0.4610
Round 165: Global Test Accuracy = 0.4760
Round 166: Global Test Accuracy = 0.4790
Round 167: Global Test Accuracy = 0.4810
Round 168: Global Test Accuracy = 0.4870
Round 169: Global Test Accuracy = 0.4850
Round 170: Global Test Accuracy = 0.4870
Round 171: Global Test Accuracy = 0.4830
Round 172: Global Test Accuracy = 0.4900
Round 173: Global Test Accuracy = 0.4870
Round 174: Global Test Accuracy = 0.4860
Round 175: Global Test Accuracy = 0.4850
Round 176: Global Test Accuracy = 0.4850
Round 177: Global Test Accuracy = 0.4870
Round 178: Global Test Accuracy = 0.4920
Round 179: Global Test Accuracy = 0.4880
Round 180: Global Test Accuracy = 0.4890
Round 181: Global Test Accuracy = 0.4870
Round 182: Global Test Accuracy = 0.4780
Round 183: Global Test Accuracy = 0.4830
Round 184: Global Test Accuracy = 0.4740
Round 185: Global Test Accuracy = 0.4880
Round 186: Global Test Accuracy = 0.4860
Round 187: Global Test Accuracy = 0.4930
Round 188: Global Test Accuracy = 0.4920
Round 189: Global Test Accuracy = 0.4920
Round 190: Global Test Accuracy = 0.4930
Round 191: Global Test Accuracy = 0.4950
Round 192: Global Test Accuracy = 0.4670
Round 193: Global Test Accuracy = 0.4540
Round 194: Global Test Accuracy = 0.4280
Round 195: Global Test Accuracy = 0.4360
Round 196: Global Test Accuracy = 0.4410
Round 197: Global Test Accuracy = 0.4280
Round 198: Global Test Accuracy = 0.4230
Round 199: Global Test Accuracy = 0.4100
Round 200: Global Test Accuracy = 0.4560
//train_time: 4836.558 ms//end
//Log Max memory for Large1: 6460346368.0 //end
//Log Max memory for Large2: 5748060160.0 //end
//Log Max memory for Large3: 6244990976.0 //end
//Log Max memory for Large4: 6085554176.0 //end
//Log Max memory for Server: 18333380608.0 //end
//Log Large1 network: 22427797.0 //end
//Log Large2 network: 15129207.0 //end
//Log Large3 network: 22334189.0 //end
//Log Large4 network: 15084670.0 //end
//Log Server network: 75334393.0 //end
//Log Total Actual Train Comm Cost: 143.35 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 1.0904192227125167
Average test accuracy, 0.456
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 123.09 MB //end
[36m(Trainer pid=117640, ip=192.168.39.156)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=117640, ip=192.168.39.156)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: pubmed, Trainers: 10, Distribution: average, IID Beta: 10000.0, Hops: 1, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'pubmed', 'method': 'fedgcn', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 1, 'iid_beta': 10000.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/pubmed/raw/ind.pubmed.x
File already exists: ./data/pubmed/raw/ind.pubmed.tx
File already exists: ./data/pubmed/raw/ind.pubmed.allx
File already exists: ./data/pubmed/raw/ind.pubmed.y
File already exists: ./data/pubmed/raw/ind.pubmed.ty
File already exists: ./data/pubmed/raw/ind.pubmed.ally
File already exists: ./data/pubmed/raw/ind.pubmed.graph
File already exists: ./data/pubmed/raw/ind.pubmed.test.index
Initialization start: network data collected.
2025-05-15 03:05:33,584	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 03:05:33,585	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 03:05:33,590	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
[36m(Trainer pid=122277, ip=192.168.14.62)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=122277, ip=192.168.14.62)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 6024.831 ms //end
//Log Large1 init network: 117479.0 //end
//Log Large2 init network: 318334.0 //end
//Log Large3 init network: 120670.0 //end
//Log Large4 init network: 148827.0 //end
//Log Server init network: 47945334.0 //end
//Log Initialization Communication Cost (MB): 46.40 //end
Pretrain start time recorded.
server aggregates all local neighbor feature sums
clients received feature aggregation from server
//pretrain_time: 1191.928 ms//end
//Log Max memory for Large1: 6176108544.0 //end
//Log Max memory for Large2: 6305607680.0 //end
//Log Max memory for Large3: 5908938752.0 //end
//Log Max memory for Large4: 6660222976.0 //end
//Log Max memory for Server: 18520432640.0 //end
//Log Large1 network: 81083194.0 //end
//Log Large2 network: 120342408.0 //end
//Log Large3 network: 81223678.0 //end
//Log Large4 network: 122281717.0 //end
//Log Server network: 140540696.0 //end
//Log Total Actual Pretrain Comm Cost: 520.20 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.2560
Round 2: Global Test Accuracy = 0.2730
Round 3: Global Test Accuracy = 0.2820
Round 4: Global Test Accuracy = 0.2800
Round 5: Global Test Accuracy = 0.2880
Round 6: Global Test Accuracy = 0.2870
Round 7: Global Test Accuracy = 0.2970
Round 8: Global Test Accuracy = 0.3340
Round 9: Global Test Accuracy = 0.3630
Round 10: Global Test Accuracy = 0.3670
Round 11: Global Test Accuracy = 0.3920
Round 12: Global Test Accuracy = 0.3860
Round 13: Global Test Accuracy = 0.3860
Round 14: Global Test Accuracy = 0.4010
Round 15: Global Test Accuracy = 0.4200
Round 16: Global Test Accuracy = 0.4450
Round 17: Global Test Accuracy = 0.4820
Round 18: Global Test Accuracy = 0.4940
Round 19: Global Test Accuracy = 0.5120
Round 20: Global Test Accuracy = 0.5220
Round 21: Global Test Accuracy = 0.5290
Round 22: Global Test Accuracy = 0.5310
Round 23: Global Test Accuracy = 0.5360
Round 24: Global Test Accuracy = 0.5410
Round 25: Global Test Accuracy = 0.5380
Round 26: Global Test Accuracy = 0.5400
Round 27: Global Test Accuracy = 0.5410
Round 28: Global Test Accuracy = 0.5410
Round 29: Global Test Accuracy = 0.5410
Round 30: Global Test Accuracy = 0.5390
Round 31: Global Test Accuracy = 0.5490
Round 32: Global Test Accuracy = 0.5480
Round 33: Global Test Accuracy = 0.5510
Round 34: Global Test Accuracy = 0.5510
Round 35: Global Test Accuracy = 0.5500
Round 36: Global Test Accuracy = 0.5500
Round 37: Global Test Accuracy = 0.5490
Round 38: Global Test Accuracy = 0.5490
Round 39: Global Test Accuracy = 0.5480
Round 40: Global Test Accuracy = 0.5510
Round 41: Global Test Accuracy = 0.5540
Round 42: Global Test Accuracy = 0.5500
Round 43: Global Test Accuracy = 0.5470
Round 44: Global Test Accuracy = 0.5520
Round 45: Global Test Accuracy = 0.5500
Round 46: Global Test Accuracy = 0.5530
Round 47: Global Test Accuracy = 0.5540
Round 48: Global Test Accuracy = 0.5680
Round 49: Global Test Accuracy = 0.5690
Round 50: Global Test Accuracy = 0.5700
Round 51: Global Test Accuracy = 0.5670
Round 52: Global Test Accuracy = 0.5620
Round 53: Global Test Accuracy = 0.5700
Round 54: Global Test Accuracy = 0.5720
Round 55: Global Test Accuracy = 0.5750
Round 56: Global Test Accuracy = 0.5770
Round 57: Global Test Accuracy = 0.5780
Round 58: Global Test Accuracy = 0.5780
Round 59: Global Test Accuracy = 0.5780
Round 60: Global Test Accuracy = 0.5810
Round 61: Global Test Accuracy = 0.5850
Round 62: Global Test Accuracy = 0.5890
Round 63: Global Test Accuracy = 0.5920
Round 64: Global Test Accuracy = 0.5930
Round 65: Global Test Accuracy = 0.5940
Round 66: Global Test Accuracy = 0.5930
Round 67: Global Test Accuracy = 0.5960
Round 68: Global Test Accuracy = 0.5990
Round 69: Global Test Accuracy = 0.6010
Round 70: Global Test Accuracy = 0.6010
Round 71: Global Test Accuracy = 0.6060
Round 72: Global Test Accuracy = 0.6080
Round 73: Global Test Accuracy = 0.6120
Round 74: Global Test Accuracy = 0.6140
Round 75: Global Test Accuracy = 0.6150
Round 76: Global Test Accuracy = 0.6180
Round 77: Global Test Accuracy = 0.6200
Round 78: Global Test Accuracy = 0.6210
Round 79: Global Test Accuracy = 0.6210
Round 80: Global Test Accuracy = 0.6240
Round 81: Global Test Accuracy = 0.6230
Round 82: Global Test Accuracy = 0.6290
Round 83: Global Test Accuracy = 0.6290
Round 84: Global Test Accuracy = 0.6290
Round 85: Global Test Accuracy = 0.6250
Round 86: Global Test Accuracy = 0.6260
Round 87: Global Test Accuracy = 0.6260
Round 88: Global Test Accuracy = 0.6310
Round 89: Global Test Accuracy = 0.6400
Round 90: Global Test Accuracy = 0.6390
Round 91: Global Test Accuracy = 0.6400
Round 92: Global Test Accuracy = 0.6430
Round 93: Global Test Accuracy = 0.6470
Round 94: Global Test Accuracy = 0.6490
Round 95: Global Test Accuracy = 0.6460
Round 96: Global Test Accuracy = 0.6480
Round 97: Global Test Accuracy = 0.6500
Round 98: Global Test Accuracy = 0.6500
Round 99: Global Test Accuracy = 0.6610
Round 100: Global Test Accuracy = 0.6700
Round 101: Global Test Accuracy = 0.6670
Round 102: Global Test Accuracy = 0.6690
Round 103: Global Test Accuracy = 0.6680
Round 104: Global Test Accuracy = 0.6740
Round 105: Global Test Accuracy = 0.6750
Round 106: Global Test Accuracy = 0.6750
Round 107: Global Test Accuracy = 0.6740
Round 108: Global Test Accuracy = 0.6760
Round 109: Global Test Accuracy = 0.6770
Round 110: Global Test Accuracy = 0.6760
Round 111: Global Test Accuracy = 0.6770
Round 112: Global Test Accuracy = 0.6810
Round 113: Global Test Accuracy = 0.6790
Round 114: Global Test Accuracy = 0.6830
Round 115: Global Test Accuracy = 0.6830
Round 116: Global Test Accuracy = 0.6840
Round 117: Global Test Accuracy = 0.6810
Round 118: Global Test Accuracy = 0.6820
Round 119: Global Test Accuracy = 0.6840
Round 120: Global Test Accuracy = 0.6880
Round 121: Global Test Accuracy = 0.6890
Round 122: Global Test Accuracy = 0.6890
Round 123: Global Test Accuracy = 0.6910
Round 124: Global Test Accuracy = 0.6930
Round 125: Global Test Accuracy = 0.6930
Round 126: Global Test Accuracy = 0.7030
Round 127: Global Test Accuracy = 0.7000
Round 128: Global Test Accuracy = 0.7010
Round 129: Global Test Accuracy = 0.7010
Round 130: Global Test Accuracy = 0.7030
Round 131: Global Test Accuracy = 0.7030
Round 132: Global Test Accuracy = 0.7020
Round 133: Global Test Accuracy = 0.7030
Round 134: Global Test Accuracy = 0.7030
Round 135: Global Test Accuracy = 0.7020
Round 136: Global Test Accuracy = 0.7040
Round 137: Global Test Accuracy = 0.7040
Round 138: Global Test Accuracy = 0.7050
Round 139: Global Test Accuracy = 0.7040
Round 140: Global Test Accuracy = 0.7040
Round 141: Global Test Accuracy = 0.7040
Round 142: Global Test Accuracy = 0.7050
Round 143: Global Test Accuracy = 0.7060
Round 144: Global Test Accuracy = 0.7060
Round 145: Global Test Accuracy = 0.7080
Round 146: Global Test Accuracy = 0.7080
Round 147: Global Test Accuracy = 0.7120
Round 148: Global Test Accuracy = 0.7100
Round 149: Global Test Accuracy = 0.7110
Round 150: Global Test Accuracy = 0.7170
Round 151: Global Test Accuracy = 0.7150
Round 152: Global Test Accuracy = 0.7170
Round 153: Global Test Accuracy = 0.7180
Round 154: Global Test Accuracy = 0.7170
Round 155: Global Test Accuracy = 0.7170
Round 156: Global Test Accuracy = 0.7190
Round 157: Global Test Accuracy = 0.7180
Round 158: Global Test Accuracy = 0.7170
Round 159: Global Test Accuracy = 0.7190
Round 160: Global Test Accuracy = 0.7190
Round 161: Global Test Accuracy = 0.7190
Round 162: Global Test Accuracy = 0.7180
Round 163: Global Test Accuracy = 0.7180
Round 164: Global Test Accuracy = 0.7190
Round 165: Global Test Accuracy = 0.7200
Round 166: Global Test Accuracy = 0.7200
Round 167: Global Test Accuracy = 0.7200
Round 168: Global Test Accuracy = 0.7250
Round 169: Global Test Accuracy = 0.7260
Round 170: Global Test Accuracy = 0.7290
Round 171: Global Test Accuracy = 0.7250
Round 172: Global Test Accuracy = 0.7340
Round 173: Global Test Accuracy = 0.7300
Round 174: Global Test Accuracy = 0.7340
Round 175: Global Test Accuracy = 0.7270
Round 176: Global Test Accuracy = 0.7260
Round 177: Global Test Accuracy = 0.7270
Round 178: Global Test Accuracy = 0.7340
Round 179: Global Test Accuracy = 0.7300
Round 180: Global Test Accuracy = 0.7310
Round 181: Global Test Accuracy = 0.7310
Round 182: Global Test Accuracy = 0.7300
Round 183: Global Test Accuracy = 0.7300
Round 184: Global Test Accuracy = 0.7310
Round 185: Global Test Accuracy = 0.7320
Round 186: Global Test Accuracy = 0.7320
Round 187: Global Test Accuracy = 0.7350
Round 188: Global Test Accuracy = 0.7360
Round 189: Global Test Accuracy = 0.7340
Round 190: Global Test Accuracy = 0.7370
Round 191: Global Test Accuracy = 0.7380
Round 192: Global Test Accuracy = 0.7360
Round 193: Global Test Accuracy = 0.7360
Round 194: Global Test Accuracy = 0.7410
Round 195: Global Test Accuracy = 0.7420
Round 196: Global Test Accuracy = 0.7340
Round 197: Global Test Accuracy = 0.7360
Round 198: Global Test Accuracy = 0.7370
Round 199: Global Test Accuracy = 0.7410
Round 200: Global Test Accuracy = 0.7450
//train_time: 6925.259999999999 ms//end
//Log Max memory for Large1: 6207696896.0 //end
//Log Max memory for Large2: 6353461248.0 //end
//Log Max memory for Large3: 5943504896.0 //end
//Log Max memory for Large4: 6716162048.0 //end
//Log Max memory for Server: 18520117248.0 //end
//Log Large1 network: 15126682.0 //end
//Log Large2 network: 22514872.0 //end
//Log Large3 network: 15105201.0 //end
//Log Large4 network: 22412148.0 //end
//Log Server network: 75436727.0 //end
//Log Total Actual Train Comm Cost: 143.62 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 0.6973969051837922
Average test accuracy, 0.745
//Log Theoretical Pretrain Comm Cost: 507.87 MB //end
//Log Theoretical Train Comm Cost: 123.09 MB //end
[36m(Trainer pid=122281, ip=192.168.14.62)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=122281, ip=192.168.14.62)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: pubmed, Trainers: 10, Distribution: average, IID Beta: 100.0, Hops: 1, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'pubmed', 'method': 'fedgcn', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 1, 'iid_beta': 100.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/pubmed/raw/ind.pubmed.x
File already exists: ./data/pubmed/raw/ind.pubmed.tx
File already exists: ./data/pubmed/raw/ind.pubmed.allx
File already exists: ./data/pubmed/raw/ind.pubmed.y
File already exists: ./data/pubmed/raw/ind.pubmed.ty
File already exists: ./data/pubmed/raw/ind.pubmed.ally
File already exists: ./data/pubmed/raw/ind.pubmed.graph
File already exists: ./data/pubmed/raw/ind.pubmed.test.index
Initialization start: network data collected.
2025-05-15 03:06:58,823	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 03:06:58,823	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 03:06:58,829	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
[36m(Trainer pid=122877, ip=192.168.14.62)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=122877, ip=192.168.14.62)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5638.934 ms //end
//Log Large1 init network: 244865.0 //end
//Log Large2 init network: 128060.0 //end
//Log Large3 init network: 184231.0 //end
//Log Large4 init network: 166153.0 //end
//Log Server init network: 47841368.0 //end
//Log Initialization Communication Cost (MB): 46.31 //end
Pretrain start time recorded.
server aggregates all local neighbor feature sums
clients received feature aggregation from server
//pretrain_time: 1364.393 ms//end
//Log Max memory for Large1: 6630248448.0 //end
//Log Max memory for Large2: 5895561216.0 //end
//Log Max memory for Large3: 6384766976.0 //end
//Log Max memory for Large4: 6244298752.0 //end
//Log Max memory for Server: 18513887232.0 //end
//Log Large1 network: 120947789.0 //end
//Log Large2 network: 81421639.0 //end
//Log Large3 network: 122330085.0 //end
//Log Large4 network: 81241447.0 //end
//Log Server network: 140186710.0 //end
//Log Total Actual Pretrain Comm Cost: 520.83 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.2640
Round 2: Global Test Accuracy = 0.3190
Round 3: Global Test Accuracy = 0.3490
Round 4: Global Test Accuracy = 0.3880
Round 5: Global Test Accuracy = 0.4240
Round 6: Global Test Accuracy = 0.4400
Round 7: Global Test Accuracy = 0.4470
Round 8: Global Test Accuracy = 0.4450
Round 9: Global Test Accuracy = 0.4470
Round 10: Global Test Accuracy = 0.4500
Round 11: Global Test Accuracy = 0.4630
Round 12: Global Test Accuracy = 0.4710
Round 13: Global Test Accuracy = 0.4820
Round 14: Global Test Accuracy = 0.4870
Round 15: Global Test Accuracy = 0.4930
Round 16: Global Test Accuracy = 0.4940
Round 17: Global Test Accuracy = 0.4990
Round 18: Global Test Accuracy = 0.5080
Round 19: Global Test Accuracy = 0.5100
Round 20: Global Test Accuracy = 0.5160
Round 21: Global Test Accuracy = 0.5110
Round 22: Global Test Accuracy = 0.5210
Round 23: Global Test Accuracy = 0.5310
Round 24: Global Test Accuracy = 0.5320
Round 25: Global Test Accuracy = 0.5410
Round 26: Global Test Accuracy = 0.5510
Round 27: Global Test Accuracy = 0.5550
Round 28: Global Test Accuracy = 0.5540
Round 29: Global Test Accuracy = 0.5750
Round 30: Global Test Accuracy = 0.5830
Round 31: Global Test Accuracy = 0.5980
Round 32: Global Test Accuracy = 0.6040
Round 33: Global Test Accuracy = 0.6080
Round 34: Global Test Accuracy = 0.6060
Round 35: Global Test Accuracy = 0.6060
Round 36: Global Test Accuracy = 0.6080
Round 37: Global Test Accuracy = 0.6100
Round 38: Global Test Accuracy = 0.6260
Round 39: Global Test Accuracy = 0.6340
Round 40: Global Test Accuracy = 0.6340
Round 41: Global Test Accuracy = 0.6360
Round 42: Global Test Accuracy = 0.6380
Round 43: Global Test Accuracy = 0.6490
Round 44: Global Test Accuracy = 0.6480
Round 45: Global Test Accuracy = 0.6530
Round 46: Global Test Accuracy = 0.6560
Round 47: Global Test Accuracy = 0.6510
Round 48: Global Test Accuracy = 0.6560
Round 49: Global Test Accuracy = 0.6520
Round 50: Global Test Accuracy = 0.6540
Round 51: Global Test Accuracy = 0.6590
Round 52: Global Test Accuracy = 0.6630
Round 53: Global Test Accuracy = 0.6640
Round 54: Global Test Accuracy = 0.6670
Round 55: Global Test Accuracy = 0.6670
Round 56: Global Test Accuracy = 0.6660
Round 57: Global Test Accuracy = 0.6680
Round 58: Global Test Accuracy = 0.6700
Round 59: Global Test Accuracy = 0.6720
Round 60: Global Test Accuracy = 0.6680
Round 61: Global Test Accuracy = 0.6730
Round 62: Global Test Accuracy = 0.6710
Round 63: Global Test Accuracy = 0.6750
Round 64: Global Test Accuracy = 0.6800
Round 65: Global Test Accuracy = 0.6820
Round 66: Global Test Accuracy = 0.6830
Round 67: Global Test Accuracy = 0.6820
Round 68: Global Test Accuracy = 0.6830
Round 69: Global Test Accuracy = 0.6800
Round 70: Global Test Accuracy = 0.6830
Round 71: Global Test Accuracy = 0.6820
Round 72: Global Test Accuracy = 0.6880
Round 73: Global Test Accuracy = 0.6870
Round 74: Global Test Accuracy = 0.6890
Round 75: Global Test Accuracy = 0.6900
Round 76: Global Test Accuracy = 0.6900
Round 77: Global Test Accuracy = 0.6890
Round 78: Global Test Accuracy = 0.6920
Round 79: Global Test Accuracy = 0.6880
Round 80: Global Test Accuracy = 0.6940
Round 81: Global Test Accuracy = 0.6960
Round 82: Global Test Accuracy = 0.6950
Round 83: Global Test Accuracy = 0.6960
Round 84: Global Test Accuracy = 0.6940
Round 85: Global Test Accuracy = 0.7040
Round 86: Global Test Accuracy = 0.7060
Round 87: Global Test Accuracy = 0.7090
Round 88: Global Test Accuracy = 0.7070
Round 89: Global Test Accuracy = 0.7100
Round 90: Global Test Accuracy = 0.7140
Round 91: Global Test Accuracy = 0.7130
Round 92: Global Test Accuracy = 0.7070
Round 93: Global Test Accuracy = 0.7070
Round 94: Global Test Accuracy = 0.7060
Round 95: Global Test Accuracy = 0.7050
Round 96: Global Test Accuracy = 0.7060
Round 97: Global Test Accuracy = 0.7080
Round 98: Global Test Accuracy = 0.7090
Round 99: Global Test Accuracy = 0.7100
Round 100: Global Test Accuracy = 0.7080
Round 101: Global Test Accuracy = 0.7080
Round 102: Global Test Accuracy = 0.7120
Round 103: Global Test Accuracy = 0.7090
Round 104: Global Test Accuracy = 0.7120
Round 105: Global Test Accuracy = 0.7140
Round 106: Global Test Accuracy = 0.7080
Round 107: Global Test Accuracy = 0.7080
Round 108: Global Test Accuracy = 0.7070
Round 109: Global Test Accuracy = 0.7080
Round 110: Global Test Accuracy = 0.7130
Round 111: Global Test Accuracy = 0.7110
Round 112: Global Test Accuracy = 0.7140
Round 113: Global Test Accuracy = 0.7160
Round 114: Global Test Accuracy = 0.7160
Round 115: Global Test Accuracy = 0.7150
Round 116: Global Test Accuracy = 0.7160
Round 117: Global Test Accuracy = 0.7140
Round 118: Global Test Accuracy = 0.7160
Round 119: Global Test Accuracy = 0.7180
Round 120: Global Test Accuracy = 0.7130
Round 121: Global Test Accuracy = 0.7190
Round 122: Global Test Accuracy = 0.7200
Round 123: Global Test Accuracy = 0.7200
Round 124: Global Test Accuracy = 0.7200
Round 125: Global Test Accuracy = 0.7190
Round 126: Global Test Accuracy = 0.7200
Round 127: Global Test Accuracy = 0.7220
Round 128: Global Test Accuracy = 0.7220
Round 129: Global Test Accuracy = 0.7270
Round 130: Global Test Accuracy = 0.7190
Round 131: Global Test Accuracy = 0.7260
Round 132: Global Test Accuracy = 0.7270
Round 133: Global Test Accuracy = 0.7290
Round 134: Global Test Accuracy = 0.7280
Round 135: Global Test Accuracy = 0.7290
Round 136: Global Test Accuracy = 0.7290
Round 137: Global Test Accuracy = 0.7290
Round 138: Global Test Accuracy = 0.7300
Round 139: Global Test Accuracy = 0.7280
Round 140: Global Test Accuracy = 0.7300
Round 141: Global Test Accuracy = 0.7300
Round 142: Global Test Accuracy = 0.7320
Round 143: Global Test Accuracy = 0.7300
Round 144: Global Test Accuracy = 0.7310
Round 145: Global Test Accuracy = 0.7300
Round 146: Global Test Accuracy = 0.7270
Round 147: Global Test Accuracy = 0.7290
Round 148: Global Test Accuracy = 0.7300
Round 149: Global Test Accuracy = 0.7310
Round 150: Global Test Accuracy = 0.7310
Round 151: Global Test Accuracy = 0.7330
Round 152: Global Test Accuracy = 0.7350
Round 153: Global Test Accuracy = 0.7390
Round 154: Global Test Accuracy = 0.7400
Round 155: Global Test Accuracy = 0.7410
Round 156: Global Test Accuracy = 0.7410
Round 157: Global Test Accuracy = 0.7370
Round 158: Global Test Accuracy = 0.7360
Round 159: Global Test Accuracy = 0.7370
Round 160: Global Test Accuracy = 0.7410
Round 161: Global Test Accuracy = 0.7410
Round 162: Global Test Accuracy = 0.7400
Round 163: Global Test Accuracy = 0.7350
Round 164: Global Test Accuracy = 0.7360
Round 165: Global Test Accuracy = 0.7370
Round 166: Global Test Accuracy = 0.7390
Round 167: Global Test Accuracy = 0.7380
Round 168: Global Test Accuracy = 0.7380
Round 169: Global Test Accuracy = 0.7400
Round 170: Global Test Accuracy = 0.7420
Round 171: Global Test Accuracy = 0.7360
Round 172: Global Test Accuracy = 0.7370
Round 173: Global Test Accuracy = 0.7360
Round 174: Global Test Accuracy = 0.7420
Round 175: Global Test Accuracy = 0.7470
Round 176: Global Test Accuracy = 0.7420
Round 177: Global Test Accuracy = 0.7440
Round 178: Global Test Accuracy = 0.7450
Round 179: Global Test Accuracy = 0.7470
Round 180: Global Test Accuracy = 0.7490
Round 181: Global Test Accuracy = 0.7480
Round 182: Global Test Accuracy = 0.7520
Round 183: Global Test Accuracy = 0.7510
Round 184: Global Test Accuracy = 0.7490
Round 185: Global Test Accuracy = 0.7520
Round 186: Global Test Accuracy = 0.7480
Round 187: Global Test Accuracy = 0.7490
Round 188: Global Test Accuracy = 0.7490
Round 189: Global Test Accuracy = 0.7510
Round 190: Global Test Accuracy = 0.7520
Round 191: Global Test Accuracy = 0.7530
Round 192: Global Test Accuracy = 0.7490
Round 193: Global Test Accuracy = 0.7480
Round 194: Global Test Accuracy = 0.7500
Round 195: Global Test Accuracy = 0.7500
Round 196: Global Test Accuracy = 0.7550
Round 197: Global Test Accuracy = 0.7530
Round 198: Global Test Accuracy = 0.7550
Round 199: Global Test Accuracy = 0.7500
Round 200: Global Test Accuracy = 0.7550
//train_time: 7005.544000000001 ms//end
//Log Max memory for Large1: 6671667200.0 //end
//Log Max memory for Large2: 5926481920.0 //end
//Log Max memory for Large3: 6433775616.0 //end
//Log Max memory for Large4: 6274056192.0 //end
//Log Max memory for Server: 18537738240.0 //end
//Log Large1 network: 22473236.0 //end
//Log Large2 network: 15205861.0 //end
//Log Large3 network: 22410607.0 //end
//Log Large4 network: 15168149.0 //end
//Log Server network: 75423747.0 //end
//Log Total Actual Train Comm Cost: 143.70 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 0.7046554844379425
Average test accuracy, 0.755
//Log Theoretical Pretrain Comm Cost: 507.50 MB //end
//Log Theoretical Train Comm Cost: 123.09 MB //end
[36m(Trainer pid=118742, ip=192.168.39.156)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=118742, ip=192.168.39.156)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: pubmed, Trainers: 10, Distribution: average, IID Beta: 10.0, Hops: 1, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'pubmed', 'method': 'fedgcn', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 1, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/pubmed/raw/ind.pubmed.x
File already exists: ./data/pubmed/raw/ind.pubmed.tx
File already exists: ./data/pubmed/raw/ind.pubmed.allx
File already exists: ./data/pubmed/raw/ind.pubmed.y
File already exists: ./data/pubmed/raw/ind.pubmed.ty
File already exists: ./data/pubmed/raw/ind.pubmed.ally
File already exists: ./data/pubmed/raw/ind.pubmed.graph
File already exists: ./data/pubmed/raw/ind.pubmed.test.index
Initialization start: network data collected.
2025-05-15 03:08:23,980	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 03:08:23,981	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 03:08:23,987	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
[36m(Trainer pid=119511, ip=192.168.14.54)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=119511, ip=192.168.14.54)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5794.809 ms //end
//Log Large1 init network: 118143.0 //end
//Log Large2 init network: 139498.0 //end
//Log Large3 init network: 118502.0 //end
//Log Large4 init network: 152973.0 //end
//Log Server init network: 47903784.0 //end
//Log Initialization Communication Cost (MB): 46.19 //end
Pretrain start time recorded.
server aggregates all local neighbor feature sums
clients received feature aggregation from server
//pretrain_time: 1196.944 ms//end
//Log Max memory for Large1: 6179131392.0 //end
//Log Max memory for Large2: 6314803200.0 //end
//Log Max memory for Large3: 5922484224.0 //end
//Log Max memory for Large4: 6686367744.0 //end
//Log Max memory for Server: 18576809984.0 //end
//Log Large1 network: 80724623.0 //end
//Log Large2 network: 122959653.0 //end
//Log Large3 network: 80876143.0 //end
//Log Large4 network: 120939897.0 //end
//Log Server network: 139505912.0 //end
//Log Total Actual Pretrain Comm Cost: 519.76 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.2360
Round 2: Global Test Accuracy = 0.2140
Round 3: Global Test Accuracy = 0.2160
Round 4: Global Test Accuracy = 0.2090
Round 5: Global Test Accuracy = 0.2030
Round 6: Global Test Accuracy = 0.2000
Round 7: Global Test Accuracy = 0.2030
Round 8: Global Test Accuracy = 0.2070
Round 9: Global Test Accuracy = 0.2100
Round 10: Global Test Accuracy = 0.2130
Round 11: Global Test Accuracy = 0.2230
Round 12: Global Test Accuracy = 0.2350
Round 13: Global Test Accuracy = 0.2510
Round 14: Global Test Accuracy = 0.2790
Round 15: Global Test Accuracy = 0.3170
Round 16: Global Test Accuracy = 0.3570
Round 17: Global Test Accuracy = 0.3840
Round 18: Global Test Accuracy = 0.3980
Round 19: Global Test Accuracy = 0.4090
Round 20: Global Test Accuracy = 0.4260
Round 21: Global Test Accuracy = 0.4420
Round 22: Global Test Accuracy = 0.4560
Round 23: Global Test Accuracy = 0.4650
Round 24: Global Test Accuracy = 0.4710
Round 25: Global Test Accuracy = 0.4820
Round 26: Global Test Accuracy = 0.4850
Round 27: Global Test Accuracy = 0.4890
Round 28: Global Test Accuracy = 0.4930
Round 29: Global Test Accuracy = 0.5020
Round 30: Global Test Accuracy = 0.5030
Round 31: Global Test Accuracy = 0.5040
Round 32: Global Test Accuracy = 0.5060
Round 33: Global Test Accuracy = 0.5050
Round 34: Global Test Accuracy = 0.5080
Round 35: Global Test Accuracy = 0.5130
Round 36: Global Test Accuracy = 0.5120
Round 37: Global Test Accuracy = 0.5130
Round 38: Global Test Accuracy = 0.5210
Round 39: Global Test Accuracy = 0.5200
Round 40: Global Test Accuracy = 0.5200
Round 41: Global Test Accuracy = 0.5210
Round 42: Global Test Accuracy = 0.5220
Round 43: Global Test Accuracy = 0.5220
Round 44: Global Test Accuracy = 0.5200
Round 45: Global Test Accuracy = 0.5270
Round 46: Global Test Accuracy = 0.5270
Round 47: Global Test Accuracy = 0.5260
Round 48: Global Test Accuracy = 0.5270
Round 49: Global Test Accuracy = 0.5300
Round 50: Global Test Accuracy = 0.5320
Round 51: Global Test Accuracy = 0.5320
Round 52: Global Test Accuracy = 0.5320
Round 53: Global Test Accuracy = 0.5330
Round 54: Global Test Accuracy = 0.5330
Round 55: Global Test Accuracy = 0.5360
Round 56: Global Test Accuracy = 0.5380
Round 57: Global Test Accuracy = 0.5400
Round 58: Global Test Accuracy = 0.5390
Round 59: Global Test Accuracy = 0.5410
Round 60: Global Test Accuracy = 0.5450
Round 61: Global Test Accuracy = 0.5460
Round 62: Global Test Accuracy = 0.5500
Round 63: Global Test Accuracy = 0.5470
Round 64: Global Test Accuracy = 0.5550
Round 65: Global Test Accuracy = 0.5580
Round 66: Global Test Accuracy = 0.5590
Round 67: Global Test Accuracy = 0.5600
Round 68: Global Test Accuracy = 0.5610
Round 69: Global Test Accuracy = 0.5620
Round 70: Global Test Accuracy = 0.5650
Round 71: Global Test Accuracy = 0.5670
Round 72: Global Test Accuracy = 0.5700
Round 73: Global Test Accuracy = 0.5740
Round 74: Global Test Accuracy = 0.5720
Round 75: Global Test Accuracy = 0.5730
Round 76: Global Test Accuracy = 0.5730
Round 77: Global Test Accuracy = 0.5760
Round 78: Global Test Accuracy = 0.5780
Round 79: Global Test Accuracy = 0.5780
Round 80: Global Test Accuracy = 0.5840
Round 81: Global Test Accuracy = 0.5850
Round 82: Global Test Accuracy = 0.5840
Round 83: Global Test Accuracy = 0.5840
Round 84: Global Test Accuracy = 0.5850
Round 85: Global Test Accuracy = 0.5920
Round 86: Global Test Accuracy = 0.5950
Round 87: Global Test Accuracy = 0.6010
Round 88: Global Test Accuracy = 0.6040
Round 89: Global Test Accuracy = 0.6060
Round 90: Global Test Accuracy = 0.6080
Round 91: Global Test Accuracy = 0.6120
Round 92: Global Test Accuracy = 0.6160
Round 93: Global Test Accuracy = 0.6190
Round 94: Global Test Accuracy = 0.6250
Round 95: Global Test Accuracy = 0.6230
Round 96: Global Test Accuracy = 0.6230
Round 97: Global Test Accuracy = 0.6300
Round 98: Global Test Accuracy = 0.6300
Round 99: Global Test Accuracy = 0.6340
Round 100: Global Test Accuracy = 0.6330
Round 101: Global Test Accuracy = 0.6380
Round 102: Global Test Accuracy = 0.6430
Round 103: Global Test Accuracy = 0.6480
Round 104: Global Test Accuracy = 0.6490
Round 105: Global Test Accuracy = 0.6470
Round 106: Global Test Accuracy = 0.6430
Round 107: Global Test Accuracy = 0.6510
Round 108: Global Test Accuracy = 0.6530
Round 109: Global Test Accuracy = 0.6480
Round 110: Global Test Accuracy = 0.6520
Round 111: Global Test Accuracy = 0.6470
Round 112: Global Test Accuracy = 0.6490
Round 113: Global Test Accuracy = 0.6490
Round 114: Global Test Accuracy = 0.6510
Round 115: Global Test Accuracy = 0.6530
Round 116: Global Test Accuracy = 0.6550
Round 117: Global Test Accuracy = 0.6580
Round 118: Global Test Accuracy = 0.6580
Round 119: Global Test Accuracy = 0.6610
Round 120: Global Test Accuracy = 0.6590
Round 121: Global Test Accuracy = 0.6570
Round 122: Global Test Accuracy = 0.6600
Round 123: Global Test Accuracy = 0.6700
Round 124: Global Test Accuracy = 0.6620
Round 125: Global Test Accuracy = 0.6700
Round 126: Global Test Accuracy = 0.6760
Round 127: Global Test Accuracy = 0.6750
Round 128: Global Test Accuracy = 0.6760
Round 129: Global Test Accuracy = 0.6710
Round 130: Global Test Accuracy = 0.6700
Round 131: Global Test Accuracy = 0.6710
Round 132: Global Test Accuracy = 0.6750
Round 133: Global Test Accuracy = 0.6770
Round 134: Global Test Accuracy = 0.6830
Round 135: Global Test Accuracy = 0.6830
Round 136: Global Test Accuracy = 0.6810
Round 137: Global Test Accuracy = 0.6840
Round 138: Global Test Accuracy = 0.6870
Round 139: Global Test Accuracy = 0.6860
Round 140: Global Test Accuracy = 0.6870
Round 141: Global Test Accuracy = 0.6910
Round 142: Global Test Accuracy = 0.6910
Round 143: Global Test Accuracy = 0.6900
Round 144: Global Test Accuracy = 0.6940
Round 145: Global Test Accuracy = 0.6960
Round 146: Global Test Accuracy = 0.6960
Round 147: Global Test Accuracy = 0.6950
Round 148: Global Test Accuracy = 0.6980
Round 149: Global Test Accuracy = 0.7000
Round 150: Global Test Accuracy = 0.7020
Round 151: Global Test Accuracy = 0.7000
Round 152: Global Test Accuracy = 0.7000
Round 153: Global Test Accuracy = 0.7000
Round 154: Global Test Accuracy = 0.6960
Round 155: Global Test Accuracy = 0.7000
Round 156: Global Test Accuracy = 0.7010
Round 157: Global Test Accuracy = 0.7020
Round 158: Global Test Accuracy = 0.7010
Round 159: Global Test Accuracy = 0.6970
Round 160: Global Test Accuracy = 0.7000
Round 161: Global Test Accuracy = 0.7090
Round 162: Global Test Accuracy = 0.7100
Round 163: Global Test Accuracy = 0.7070
Round 164: Global Test Accuracy = 0.7040
Round 165: Global Test Accuracy = 0.7010
Round 166: Global Test Accuracy = 0.7020
Round 167: Global Test Accuracy = 0.7040
Round 168: Global Test Accuracy = 0.7030
Round 169: Global Test Accuracy = 0.7050
Round 170: Global Test Accuracy = 0.7060
Round 171: Global Test Accuracy = 0.7070
Round 172: Global Test Accuracy = 0.7060
Round 173: Global Test Accuracy = 0.7030
Round 174: Global Test Accuracy = 0.7040
Round 175: Global Test Accuracy = 0.7060
Round 176: Global Test Accuracy = 0.7140
Round 177: Global Test Accuracy = 0.7140
Round 178: Global Test Accuracy = 0.7130
Round 179: Global Test Accuracy = 0.7150
Round 180: Global Test Accuracy = 0.7080
Round 181: Global Test Accuracy = 0.7140
Round 182: Global Test Accuracy = 0.7160
Round 183: Global Test Accuracy = 0.7140
Round 184: Global Test Accuracy = 0.7150
Round 185: Global Test Accuracy = 0.7130
Round 186: Global Test Accuracy = 0.7130
Round 187: Global Test Accuracy = 0.7130
Round 188: Global Test Accuracy = 0.7140
Round 189: Global Test Accuracy = 0.7170
Round 190: Global Test Accuracy = 0.7160
Round 191: Global Test Accuracy = 0.7180
Round 192: Global Test Accuracy = 0.7200
Round 193: Global Test Accuracy = 0.7180
Round 194: Global Test Accuracy = 0.7240
Round 195: Global Test Accuracy = 0.7210
Round 196: Global Test Accuracy = 0.7210
Round 197: Global Test Accuracy = 0.7210
Round 198: Global Test Accuracy = 0.7200
Round 199: Global Test Accuracy = 0.7270
Round 200: Global Test Accuracy = 0.7280
//train_time: 7071.624 ms//end
//Log Max memory for Large1: 6204928000.0 //end
//Log Max memory for Large2: 6363721728.0 //end
//Log Max memory for Large3: 5956583424.0 //end
//Log Max memory for Large4: 6718435328.0 //end
//Log Max memory for Server: 18547769344.0 //end
//Log Large1 network: 15109330.0 //end
//Log Large2 network: 22505162.0 //end
//Log Large3 network: 15104330.0 //end
//Log Large4 network: 22421669.0 //end
//Log Server network: 75464417.0 //end
//Log Total Actual Train Comm Cost: 143.63 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 0.7230083376765252
Average test accuracy, 0.728
//Log Theoretical Pretrain Comm Cost: 506.85 MB //end
//Log Theoretical Train Comm Cost: 123.09 MB //end
[36m(Trainer pid=123397, ip=192.168.14.62)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=123397, ip=192.168.14.62)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: ogbn-arxiv, Trainers: 10, Distribution: average, IID Beta: 10000.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'ogbn-arxiv', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 10000.0, 'distribution_type': 'average', 'gpu': False}
ogbn-arxiv has been updated.
Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip

  0%|          | 0/81 [00:00<?, ?it/s]
Downloaded 0.00 GB:   0%|          | 0/81 [00:01<?, ?it/s]
Downloaded 0.00 GB:   1%|          | 1/81 [00:01<01:25,  1.07s/it]
Downloaded 0.00 GB:   1%|          | 1/81 [00:01<01:25,  1.07s/it]
Downloaded 0.00 GB:   2%|▏         | 2/81 [00:01<00:57,  1.36it/s]
Downloaded 0.00 GB:   2%|▏         | 2/81 [00:01<00:57,  1.36it/s]
Downloaded 0.00 GB:   4%|▎         | 3/81 [00:01<00:43,  1.78it/s]
Downloaded 0.00 GB:   4%|▎         | 3/81 [00:02<00:43,  1.78it/s]
Downloaded 0.00 GB:   5%|▍         | 4/81 [00:02<00:34,  2.21it/s]
Downloaded 0.00 GB:   5%|▍         | 4/81 [00:02<00:34,  2.21it/s]
Downloaded 0.00 GB:   6%|▌         | 5/81 [00:02<00:28,  2.64it/s]
Downloaded 0.01 GB:   6%|▌         | 5/81 [00:02<00:28,  2.64it/s]
Downloaded 0.01 GB:   7%|▋         | 6/81 [00:02<00:23,  3.22it/s]
Downloaded 0.01 GB:   7%|▋         | 6/81 [00:02<00:23,  3.22it/s]
Downloaded 0.01 GB:   9%|▊         | 7/81 [00:02<00:20,  3.60it/s]
Downloaded 0.01 GB:   9%|▊         | 7/81 [00:02<00:20,  3.60it/s]
Downloaded 0.01 GB:  10%|▉         | 8/81 [00:02<00:17,  4.25it/s]
Downloaded 0.01 GB:  10%|▉         | 8/81 [00:03<00:17,  4.25it/s]
Downloaded 0.01 GB:  11%|█         | 9/81 [00:03<00:14,  4.85it/s]
Downloaded 0.01 GB:  11%|█         | 9/81 [00:03<00:14,  4.85it/s]
Downloaded 0.01 GB:  12%|█▏        | 10/81 [00:03<00:13,  5.38it/s]
Downloaded 0.01 GB:  12%|█▏        | 10/81 [00:03<00:13,  5.38it/s]
Downloaded 0.01 GB:  14%|█▎        | 11/81 [00:03<00:12,  5.80it/s]
Downloaded 0.01 GB:  14%|█▎        | 11/81 [00:03<00:12,  5.80it/s]
Downloaded 0.01 GB:  15%|█▍        | 12/81 [00:03<00:11,  6.16it/s]
Downloaded 0.01 GB:  15%|█▍        | 12/81 [00:03<00:11,  6.16it/s]
Downloaded 0.01 GB:  15%|█▍        | 12/81 [00:03<00:11,  6.16it/s]
Downloaded 0.01 GB:  17%|█▋        | 14/81 [00:03<00:09,  7.32it/s]
Downloaded 0.01 GB:  17%|█▋        | 14/81 [00:03<00:09,  7.32it/s]
Downloaded 0.02 GB:  17%|█▋        | 14/81 [00:03<00:09,  7.32it/s]
Downloaded 0.02 GB:  20%|█▉        | 16/81 [00:03<00:07,  8.97it/s]
Downloaded 0.02 GB:  20%|█▉        | 16/81 [00:04<00:07,  8.97it/s]
Downloaded 0.02 GB:  21%|██        | 17/81 [00:04<00:07,  8.59it/s]
Downloaded 0.02 GB:  21%|██        | 17/81 [00:04<00:07,  8.59it/s]
Downloaded 0.02 GB:  21%|██        | 17/81 [00:04<00:07,  8.59it/s]
Downloaded 0.02 GB:  23%|██▎       | 19/81 [00:04<00:06, 10.06it/s]
Downloaded 0.02 GB:  23%|██▎       | 19/81 [00:04<00:06, 10.06it/s]
Downloaded 0.02 GB:  23%|██▎       | 19/81 [00:04<00:06, 10.06it/s]
Downloaded 0.02 GB:  26%|██▌       | 21/81 [00:04<00:05, 11.16it/s]
Downloaded 0.02 GB:  26%|██▌       | 21/81 [00:04<00:05, 11.16it/s]
Downloaded 0.02 GB:  26%|██▌       | 21/81 [00:04<00:05, 11.16it/s]
Downloaded 0.02 GB:  28%|██▊       | 23/81 [00:04<00:04, 12.03it/s]
Downloaded 0.02 GB:  28%|██▊       | 23/81 [00:04<00:04, 12.03it/s]
Downloaded 0.02 GB:  28%|██▊       | 23/81 [00:04<00:04, 12.03it/s]
Downloaded 0.02 GB:  31%|███       | 25/81 [00:04<00:04, 12.71it/s]
Downloaded 0.03 GB:  31%|███       | 25/81 [00:04<00:04, 12.71it/s]
Downloaded 0.03 GB:  31%|███       | 25/81 [00:04<00:04, 12.71it/s]
Downloaded 0.03 GB:  31%|███       | 25/81 [00:04<00:04, 12.71it/s]
Downloaded 0.03 GB:  35%|███▍      | 28/81 [00:04<00:03, 15.00it/s]
Downloaded 0.03 GB:  35%|███▍      | 28/81 [00:04<00:03, 15.00it/s]
Downloaded 0.03 GB:  35%|███▍      | 28/81 [00:04<00:03, 15.00it/s]
Downloaded 0.03 GB:  37%|███▋      | 30/81 [00:04<00:03, 14.98it/s]
Downloaded 0.03 GB:  37%|███▋      | 30/81 [00:04<00:03, 14.98it/s]
Downloaded 0.03 GB:  37%|███▋      | 30/81 [00:04<00:03, 14.98it/s]
Downloaded 0.03 GB:  37%|███▋      | 30/81 [00:05<00:03, 14.98it/s]
Downloaded 0.03 GB:  41%|████      | 33/81 [00:05<00:02, 16.75it/s]
Downloaded 0.03 GB:  41%|████      | 33/81 [00:05<00:02, 16.75it/s]
Downloaded 0.03 GB:  41%|████      | 33/81 [00:05<00:02, 16.75it/s]
Downloaded 0.04 GB:  41%|████      | 33/81 [00:05<00:02, 16.75it/s]
Downloaded 0.04 GB:  44%|████▍     | 36/81 [00:05<00:02, 18.19it/s]
Downloaded 0.04 GB:  44%|████▍     | 36/81 [00:05<00:02, 18.19it/s]
Downloaded 0.04 GB:  44%|████▍     | 36/81 [00:05<00:02, 18.19it/s]
Downloaded 0.04 GB:  44%|████▍     | 36/81 [00:05<00:02, 18.19it/s]
Downloaded 0.04 GB:  48%|████▊     | 39/81 [00:05<00:02, 20.77it/s]
Downloaded 0.04 GB:  48%|████▊     | 39/81 [00:05<00:02, 20.77it/s]
Downloaded 0.04 GB:  48%|████▊     | 39/81 [00:05<00:02, 20.77it/s]
Downloaded 0.04 GB:  48%|████▊     | 39/81 [00:05<00:02, 20.77it/s]
Downloaded 0.04 GB:  52%|█████▏    | 42/81 [00:05<00:01, 21.41it/s]
Downloaded 0.04 GB:  52%|█████▏    | 42/81 [00:05<00:01, 21.41it/s]
Downloaded 0.04 GB:  52%|█████▏    | 42/81 [00:05<00:01, 21.41it/s]
Downloaded 0.04 GB:  52%|█████▏    | 42/81 [00:05<00:01, 21.41it/s]
Downloaded 0.04 GB:  52%|█████▏    | 42/81 [00:05<00:01, 21.41it/s]
Downloaded 0.04 GB:  57%|█████▋    | 46/81 [00:05<00:01, 23.65it/s]
Downloaded 0.05 GB:  57%|█████▋    | 46/81 [00:05<00:01, 23.65it/s]
Downloaded 0.05 GB:  57%|█████▋    | 46/81 [00:05<00:01, 23.65it/s]
Downloaded 0.05 GB:  57%|█████▋    | 46/81 [00:05<00:01, 23.65it/s]
Downloaded 0.05 GB:  57%|█████▋    | 46/81 [00:05<00:01, 23.65it/s]
Downloaded 0.05 GB:  62%|██████▏   | 50/81 [00:05<00:01, 25.92it/s]
Downloaded 0.05 GB:  62%|██████▏   | 50/81 [00:05<00:01, 25.92it/s]
Downloaded 0.05 GB:  62%|██████▏   | 50/81 [00:05<00:01, 25.92it/s]
Downloaded 0.05 GB:  62%|██████▏   | 50/81 [00:05<00:01, 25.92it/s]
Downloaded 0.05 GB:  62%|██████▏   | 50/81 [00:05<00:01, 25.92it/s]
Downloaded 0.05 GB:  62%|██████▏   | 50/81 [00:05<00:01, 25.92it/s]
Downloaded 0.05 GB:  68%|██████▊   | 55/81 [00:05<00:00, 28.86it/s]
Downloaded 0.05 GB:  68%|██████▊   | 55/81 [00:05<00:00, 28.86it/s]
Downloaded 0.06 GB:  68%|██████▊   | 55/81 [00:05<00:00, 28.86it/s]
Downloaded 0.06 GB:  68%|██████▊   | 55/81 [00:05<00:00, 28.86it/s]
Downloaded 0.06 GB:  68%|██████▊   | 55/81 [00:05<00:00, 28.86it/s]
Downloaded 0.06 GB:  68%|██████▊   | 55/81 [00:05<00:00, 28.86it/s]
Downloaded 0.06 GB:  74%|███████▍  | 60/81 [00:05<00:00, 32.52it/s]
Downloaded 0.06 GB:  74%|███████▍  | 60/81 [00:05<00:00, 32.52it/s]
Downloaded 0.06 GB:  74%|███████▍  | 60/81 [00:05<00:00, 32.52it/s]
Downloaded 0.06 GB:  74%|███████▍  | 60/81 [00:06<00:00, 32.52it/s]
Downloaded 0.06 GB:  74%|███████▍  | 60/81 [00:06<00:00, 32.52it/s]
Downloaded 0.06 GB:  79%|███████▉  | 64/81 [00:06<00:00, 33.18it/s]
Downloaded 0.06 GB:  79%|███████▉  | 64/81 [00:06<00:00, 33.18it/s]
Downloaded 0.06 GB:  79%|███████▉  | 64/81 [00:06<00:00, 33.18it/s]
Downloaded 0.07 GB:  79%|███████▉  | 64/81 [00:06<00:00, 33.18it/s]
Downloaded 0.07 GB:  79%|███████▉  | 64/81 [00:06<00:00, 33.18it/s]
Downloaded 0.07 GB:  79%|███████▉  | 64/81 [00:06<00:00, 33.18it/s]
Downloaded 0.07 GB:  85%|████████▌ | 69/81 [00:06<00:00, 35.27it/s]
Downloaded 0.07 GB:  85%|████████▌ | 69/81 [00:06<00:00, 35.27it/s]
Downloaded 0.07 GB:  85%|████████▌ | 69/81 [00:06<00:00, 35.27it/s]
Downloaded 0.07 GB:  85%|████████▌ | 69/81 [00:06<00:00, 35.27it/s]
Downloaded 0.07 GB:  85%|████████▌ | 69/81 [00:06<00:00, 35.27it/s]
Downloaded 0.07 GB:  90%|█████████ | 73/81 [00:06<00:00, 35.03it/s]
Downloaded 0.07 GB:  90%|█████████ | 73/81 [00:06<00:00, 35.03it/s]
Downloaded 0.07 GB:  90%|█████████ | 73/81 [00:06<00:00, 35.03it/s]
Downloaded 0.07 GB:  90%|█████████ | 73/81 [00:06<00:00, 35.03it/s]
Downloaded 0.08 GB:  90%|█████████ | 73/81 [00:06<00:00, 35.03it/s]
Downloaded 0.08 GB:  95%|█████████▌| 77/81 [00:06<00:00, 34.94it/s]
Downloaded 0.08 GB:  95%|█████████▌| 77/81 [00:06<00:00, 34.94it/s]
Downloaded 0.08 GB:  95%|█████████▌| 77/81 [00:06<00:00, 34.94it/s]
Downloaded 0.08 GB:  95%|█████████▌| 77/81 [00:06<00:00, 34.94it/s]
Downloaded 0.08 GB:  95%|█████████▌| 77/81 [00:06<00:00, 34.94it/s]
Downloaded 0.08 GB: 100%|██████████| 81/81 [00:06<00:00, 12.56it/s]
Extracting dataset/arxiv.zip
Processing...
Loading necessary files...
This might take a while.
Processing graphs...

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 24105.20it/s]
Converting graphs into PyG objects...

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 6563.86it/s]
Saving...
Done!
/usr/local/lib/python3.11/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
Initialization start: network data collected.
2025-05-15 03:09:55,379	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 03:09:55,379	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 03:09:55,387	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
Changing method to FedAvg
/usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
[36m(Trainer pid=120063, ip=192.168.14.54)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=120063, ip=192.168.14.54)[0m   return torch.load(io.BytesIO(b))
[36m(Trainer pid=120063, ip=192.168.14.54)[0m Running GCN_arxiv
Running GCN_arxiv
//Log init_time: 5787.615 ms //end
//Log Large1 init network: 158480.0 //end
//Log Large2 init network: 476290.0 //end
//Log Large3 init network: 171768.0 //end
//Log Large4 init network: 156722.0 //end
//Log Server init network: 98305501.0 //end
//Log Initialization Communication Cost (MB): 94.67 //end
Pretrain start time recorded.
//pretrain_time: 4.872 ms//end
//Log Max memory for Large1: 6593396736.0 //end
//Log Max memory for Large2: 5861744640.0 //end
//Log Max memory for Large3: 6321549312.0 //end
//Log Max memory for Large4: 6210293760.0 //end
//Log Max memory for Server: 18547462144.0 //end
//Log Large1 network: 1055668.0 //end
//Log Large2 network: 611477.0 //end
//Log Large3 network: 1055005.0 //end
//Log Large4 network: 840420.0 //end
//Log Server network: 3125514.0 //end
//Log Total Actual Pretrain Comm Cost: 6.38 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.0882
Round 2: Global Test Accuracy = 0.0908
Round 3: Global Test Accuracy = 0.0691
Round 4: Global Test Accuracy = 0.0778
Round 5: Global Test Accuracy = 0.1386
Round 6: Global Test Accuracy = 0.2205
Round 7: Global Test Accuracy = 0.2519
Round 8: Global Test Accuracy = 0.2629
Round 9: Global Test Accuracy = 0.2710
Round 10: Global Test Accuracy = 0.2784
Round 11: Global Test Accuracy = 0.2857
Round 12: Global Test Accuracy = 0.2934
Round 13: Global Test Accuracy = 0.3006
Round 14: Global Test Accuracy = 0.3079
Round 15: Global Test Accuracy = 0.3145
Round 16: Global Test Accuracy = 0.3214
Round 17: Global Test Accuracy = 0.3294
Round 18: Global Test Accuracy = 0.3360
Round 19: Global Test Accuracy = 0.3425
Round 20: Global Test Accuracy = 0.3493
Round 21: Global Test Accuracy = 0.3554
Round 22: Global Test Accuracy = 0.3609
Round 23: Global Test Accuracy = 0.3668
Round 24: Global Test Accuracy = 0.3728
Round 25: Global Test Accuracy = 0.3788
Round 26: Global Test Accuracy = 0.3851
Round 27: Global Test Accuracy = 0.3924
Round 28: Global Test Accuracy = 0.3979
Round 29: Global Test Accuracy = 0.4028
Round 30: Global Test Accuracy = 0.4072
Round 31: Global Test Accuracy = 0.4149
Round 32: Global Test Accuracy = 0.4196
Round 33: Global Test Accuracy = 0.4230
Round 34: Global Test Accuracy = 0.4255
Round 35: Global Test Accuracy = 0.4314
Round 36: Global Test Accuracy = 0.4365
Round 37: Global Test Accuracy = 0.4388
Round 38: Global Test Accuracy = 0.4421
Round 39: Global Test Accuracy = 0.4452
Round 40: Global Test Accuracy = 0.4488
Round 41: Global Test Accuracy = 0.4522
Round 42: Global Test Accuracy = 0.4538
Round 43: Global Test Accuracy = 0.4577
Round 44: Global Test Accuracy = 0.4595
Round 45: Global Test Accuracy = 0.4610
Round 46: Global Test Accuracy = 0.4640
Round 47: Global Test Accuracy = 0.4662
Round 48: Global Test Accuracy = 0.4680
Round 49: Global Test Accuracy = 0.4685
Round 50: Global Test Accuracy = 0.4705
Round 51: Global Test Accuracy = 0.4717
Round 52: Global Test Accuracy = 0.4741
Round 53: Global Test Accuracy = 0.4753
Round 54: Global Test Accuracy = 0.4760
Round 55: Global Test Accuracy = 0.4777
Round 56: Global Test Accuracy = 0.4799
Round 57: Global Test Accuracy = 0.4816
Round 58: Global Test Accuracy = 0.4835
Round 59: Global Test Accuracy = 0.4845
Round 60: Global Test Accuracy = 0.4853
Round 61: Global Test Accuracy = 0.4865
Round 62: Global Test Accuracy = 0.4891
Round 63: Global Test Accuracy = 0.4902
Round 64: Global Test Accuracy = 0.4924
Round 65: Global Test Accuracy = 0.4939
Round 66: Global Test Accuracy = 0.4955
Round 67: Global Test Accuracy = 0.4962
Round 68: Global Test Accuracy = 0.4970
Round 69: Global Test Accuracy = 0.4976
Round 70: Global Test Accuracy = 0.4987
Round 71: Global Test Accuracy = 0.4995
Round 72: Global Test Accuracy = 0.5008
Round 73: Global Test Accuracy = 0.5025
Round 74: Global Test Accuracy = 0.5027
Round 75: Global Test Accuracy = 0.5031
Round 76: Global Test Accuracy = 0.5038
Round 77: Global Test Accuracy = 0.5038
Round 78: Global Test Accuracy = 0.5052
Round 79: Global Test Accuracy = 0.5046
Round 80: Global Test Accuracy = 0.5054
Round 81: Global Test Accuracy = 0.5061
Round 82: Global Test Accuracy = 0.5074
Round 83: Global Test Accuracy = 0.5078
Round 84: Global Test Accuracy = 0.5091
Round 85: Global Test Accuracy = 0.5083
Round 86: Global Test Accuracy = 0.5084
Round 87: Global Test Accuracy = 0.5087
Round 88: Global Test Accuracy = 0.5099
Round 89: Global Test Accuracy = 0.5098
Round 90: Global Test Accuracy = 0.5099
Round 91: Global Test Accuracy = 0.5109
Round 92: Global Test Accuracy = 0.5116
Round 93: Global Test Accuracy = 0.5127
Round 94: Global Test Accuracy = 0.5134
Round 95: Global Test Accuracy = 0.5142
Round 96: Global Test Accuracy = 0.5150
Round 97: Global Test Accuracy = 0.5150
Round 98: Global Test Accuracy = 0.5155
Round 99: Global Test Accuracy = 0.5166
Round 100: Global Test Accuracy = 0.5170
Round 101: Global Test Accuracy = 0.5173
Round 102: Global Test Accuracy = 0.5177
Round 103: Global Test Accuracy = 0.5179
Round 104: Global Test Accuracy = 0.5183
Round 105: Global Test Accuracy = 0.5194
Round 106: Global Test Accuracy = 0.5206
Round 107: Global Test Accuracy = 0.5212
Round 108: Global Test Accuracy = 0.5211
Round 109: Global Test Accuracy = 0.5210
Round 110: Global Test Accuracy = 0.5211
Round 111: Global Test Accuracy = 0.5218
Round 112: Global Test Accuracy = 0.5219
Round 113: Global Test Accuracy = 0.5221
Round 114: Global Test Accuracy = 0.5228
Round 115: Global Test Accuracy = 0.5236
Round 116: Global Test Accuracy = 0.5242
Round 117: Global Test Accuracy = 0.5247
Round 118: Global Test Accuracy = 0.5251
Round 119: Global Test Accuracy = 0.5253
Round 120: Global Test Accuracy = 0.5257
Round 121: Global Test Accuracy = 0.5258
Round 122: Global Test Accuracy = 0.5254
Round 123: Global Test Accuracy = 0.5257
Round 124: Global Test Accuracy = 0.5266
Round 125: Global Test Accuracy = 0.5263
Round 126: Global Test Accuracy = 0.5261
Round 127: Global Test Accuracy = 0.5264
Round 128: Global Test Accuracy = 0.5268
Round 129: Global Test Accuracy = 0.5267
Round 130: Global Test Accuracy = 0.5269
Round 131: Global Test Accuracy = 0.5278
Round 132: Global Test Accuracy = 0.5275
Round 133: Global Test Accuracy = 0.5282
Round 134: Global Test Accuracy = 0.5285
Round 135: Global Test Accuracy = 0.5282
Round 136: Global Test Accuracy = 0.5288
Round 137: Global Test Accuracy = 0.5292
Round 138: Global Test Accuracy = 0.5299
Round 139: Global Test Accuracy = 0.5301
Round 140: Global Test Accuracy = 0.5300
Round 141: Global Test Accuracy = 0.5307
Round 142: Global Test Accuracy = 0.5312
Round 143: Global Test Accuracy = 0.5313
Round 144: Global Test Accuracy = 0.5318
Round 145: Global Test Accuracy = 0.5312
Round 146: Global Test Accuracy = 0.5314
Round 147: Global Test Accuracy = 0.5320
Round 148: Global Test Accuracy = 0.5318
Round 149: Global Test Accuracy = 0.5320
Round 150: Global Test Accuracy = 0.5320
Round 151: Global Test Accuracy = 0.5315
Round 152: Global Test Accuracy = 0.5324
Round 153: Global Test Accuracy = 0.5321
Round 154: Global Test Accuracy = 0.5328
Round 155: Global Test Accuracy = 0.5339
Round 156: Global Test Accuracy = 0.5340
Round 157: Global Test Accuracy = 0.5344
Round 158: Global Test Accuracy = 0.5345
Round 159: Global Test Accuracy = 0.5345
Round 160: Global Test Accuracy = 0.5341
Round 161: Global Test Accuracy = 0.5343
Round 162: Global Test Accuracy = 0.5355
Round 163: Global Test Accuracy = 0.5348
Round 164: Global Test Accuracy = 0.5356
Round 165: Global Test Accuracy = 0.5363
Round 166: Global Test Accuracy = 0.5365
Round 167: Global Test Accuracy = 0.5364
Round 168: Global Test Accuracy = 0.5364
Round 169: Global Test Accuracy = 0.5368
Round 170: Global Test Accuracy = 0.5369
Round 171: Global Test Accuracy = 0.5370
Round 172: Global Test Accuracy = 0.5370
Round 173: Global Test Accuracy = 0.5381
Round 174: Global Test Accuracy = 0.5380
Round 175: Global Test Accuracy = 0.5382
Round 176: Global Test Accuracy = 0.5386
Round 177: Global Test Accuracy = 0.5392
Round 178: Global Test Accuracy = 0.5388
Round 179: Global Test Accuracy = 0.5393
Round 180: Global Test Accuracy = 0.5393
Round 181: Global Test Accuracy = 0.5395
Round 182: Global Test Accuracy = 0.5393
Round 183: Global Test Accuracy = 0.5396
Round 184: Global Test Accuracy = 0.5398
Round 185: Global Test Accuracy = 0.5396
Round 186: Global Test Accuracy = 0.5397
Round 187: Global Test Accuracy = 0.5391
Round 188: Global Test Accuracy = 0.5395
Round 189: Global Test Accuracy = 0.5398
Round 190: Global Test Accuracy = 0.5402
Round 191: Global Test Accuracy = 0.5404
Round 192: Global Test Accuracy = 0.5403
Round 193: Global Test Accuracy = 0.5407
Round 194: Global Test Accuracy = 0.5406
Round 195: Global Test Accuracy = 0.5400
Round 196: Global Test Accuracy = 0.5406
Round 197: Global Test Accuracy = 0.5411
Round 198: Global Test Accuracy = 0.5412
Round 199: Global Test Accuracy = 0.5413
Round 200: Global Test Accuracy = 0.5422
//train_time: 44721.489 ms//end
//Log Max memory for Large1: 7208312832.0 //end
//Log Max memory for Large2: 6089756672.0 //end
//Log Max memory for Large3: 6874587136.0 //end
//Log Max memory for Large4: 6730334208.0 //end
//Log Max memory for Server: 18502119424.0 //end
//Log Large1 network: 112039783.0 //end
//Log Large2 network: 75274293.0 //end
//Log Large3 network: 112130020.0 //end
//Log Large4 network: 75242656.0 //end
//Log Server network: 372464404.0 //end
//Log Total Actual Train Comm Cost: 712.54 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 1.6828728040554857
Average test accuracy, 0.5421681789189968
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 668.58 MB //end
[36m(Trainer pid=120028, ip=192.168.42.57)[0m Running GCN_arxiv[32m [repeated 9x across cluster][0m
[36m(Trainer pid=120028, ip=192.168.42.57)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=120028, ip=192.168.42.57)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: ogbn-arxiv, Trainers: 10, Distribution: average, IID Beta: 100.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'ogbn-arxiv', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 100.0, 'distribution_type': 'average', 'gpu': False}
/usr/local/lib/python3.11/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):
/usr/local/lib/python3.11/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):
/usr/local/lib/python3.11/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
Initialization start: network data collected.
2025-05-15 03:11:52,230	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 03:11:52,230	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 03:11:52,235	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=120801, ip=192.168.14.54)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=120801, ip=192.168.14.54)[0m   return torch.load(io.BytesIO(b))
/usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
[36m(Trainer pid=120801, ip=192.168.14.54)[0m Running GCN_arxiv
Running GCN_arxiv
//Log init_time: 6153.158 ms //end
//Log Large1 init network: 166026.0 //end
//Log Large2 init network: 175096.0 //end
//Log Large3 init network: 180816.0 //end
//Log Large4 init network: 165123.0 //end
//Log Server init network: 98373402.0 //end
//Log Initialization Communication Cost (MB): 94.47 //end
Pretrain start time recorded.
//pretrain_time: 5.122 ms//end
//Log Max memory for Large1: 6016761856.0 //end
//Log Max memory for Large2: 6181457920.0 //end
//Log Max memory for Large3: 5723688960.0 //end
//Log Max memory for Large4: 6558302208.0 //end
//Log Max memory for Server: 18536083456.0 //end
//Log Large1 network: 822697.0 //end
//Log Large2 network: 1143475.0 //end
//Log Large3 network: 826407.0 //end
//Log Large4 network: 1082627.0 //end
//Log Server network: 3062150.0 //end
//Log Total Actual Pretrain Comm Cost: 6.62 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.0891
Round 2: Global Test Accuracy = 0.0919
Round 3: Global Test Accuracy = 0.0689
Round 4: Global Test Accuracy = 0.0780
Round 5: Global Test Accuracy = 0.1340
Round 6: Global Test Accuracy = 0.2183
Round 7: Global Test Accuracy = 0.2511
Round 8: Global Test Accuracy = 0.2630
Round 9: Global Test Accuracy = 0.2712
Round 10: Global Test Accuracy = 0.2784
Round 11: Global Test Accuracy = 0.2869
Round 12: Global Test Accuracy = 0.2937
Round 13: Global Test Accuracy = 0.3016
Round 14: Global Test Accuracy = 0.3099
Round 15: Global Test Accuracy = 0.3168
Round 16: Global Test Accuracy = 0.3249
Round 17: Global Test Accuracy = 0.3321
Round 18: Global Test Accuracy = 0.3377
Round 19: Global Test Accuracy = 0.3441
Round 20: Global Test Accuracy = 0.3506
Round 21: Global Test Accuracy = 0.3566
Round 22: Global Test Accuracy = 0.3647
Round 23: Global Test Accuracy = 0.3712
Round 24: Global Test Accuracy = 0.3776
Round 25: Global Test Accuracy = 0.3835
Round 26: Global Test Accuracy = 0.3893
Round 27: Global Test Accuracy = 0.3951
Round 28: Global Test Accuracy = 0.4010
Round 29: Global Test Accuracy = 0.4076
Round 30: Global Test Accuracy = 0.4128
Round 31: Global Test Accuracy = 0.4168
Round 32: Global Test Accuracy = 0.4234
Round 33: Global Test Accuracy = 0.4267
Round 34: Global Test Accuracy = 0.4322
Round 35: Global Test Accuracy = 0.4368
Round 36: Global Test Accuracy = 0.4409
Round 37: Global Test Accuracy = 0.4436
Round 38: Global Test Accuracy = 0.4467
Round 39: Global Test Accuracy = 0.4508
Round 40: Global Test Accuracy = 0.4545
Round 41: Global Test Accuracy = 0.4591
Round 42: Global Test Accuracy = 0.4634
Round 43: Global Test Accuracy = 0.4650
Round 44: Global Test Accuracy = 0.4678
Round 45: Global Test Accuracy = 0.4696
Round 46: Global Test Accuracy = 0.4720
Round 47: Global Test Accuracy = 0.4737
Round 48: Global Test Accuracy = 0.4762
Round 49: Global Test Accuracy = 0.4782
Round 50: Global Test Accuracy = 0.4793
Round 51: Global Test Accuracy = 0.4813
Round 52: Global Test Accuracy = 0.4838
Round 53: Global Test Accuracy = 0.4848
Round 54: Global Test Accuracy = 0.4869
Round 55: Global Test Accuracy = 0.4878
Round 56: Global Test Accuracy = 0.4898
Round 57: Global Test Accuracy = 0.4910
Round 58: Global Test Accuracy = 0.4916
Round 59: Global Test Accuracy = 0.4929
Round 60: Global Test Accuracy = 0.4940
Round 61: Global Test Accuracy = 0.4951
Round 62: Global Test Accuracy = 0.4958
Round 63: Global Test Accuracy = 0.4976
Round 64: Global Test Accuracy = 0.4979
Round 65: Global Test Accuracy = 0.4998
Round 66: Global Test Accuracy = 0.5010
Round 67: Global Test Accuracy = 0.5021
Round 68: Global Test Accuracy = 0.5032
Round 69: Global Test Accuracy = 0.5036
Round 70: Global Test Accuracy = 0.5038
Round 71: Global Test Accuracy = 0.5041
Round 72: Global Test Accuracy = 0.5053
Round 73: Global Test Accuracy = 0.5054
Round 74: Global Test Accuracy = 0.5065
Round 75: Global Test Accuracy = 0.5073
Round 76: Global Test Accuracy = 0.5089
Round 77: Global Test Accuracy = 0.5098
Round 78: Global Test Accuracy = 0.5107
Round 79: Global Test Accuracy = 0.5128
Round 80: Global Test Accuracy = 0.5129
Round 81: Global Test Accuracy = 0.5130
Round 82: Global Test Accuracy = 0.5132
Round 83: Global Test Accuracy = 0.5142
Round 84: Global Test Accuracy = 0.5146
Round 85: Global Test Accuracy = 0.5149
Round 86: Global Test Accuracy = 0.5151
Round 87: Global Test Accuracy = 0.5162
Round 88: Global Test Accuracy = 0.5163
Round 89: Global Test Accuracy = 0.5169
Round 90: Global Test Accuracy = 0.5171
Round 91: Global Test Accuracy = 0.5176
Round 92: Global Test Accuracy = 0.5185
Round 93: Global Test Accuracy = 0.5186
Round 94: Global Test Accuracy = 0.5194
Round 95: Global Test Accuracy = 0.5196
Round 96: Global Test Accuracy = 0.5209
Round 97: Global Test Accuracy = 0.5223
Round 98: Global Test Accuracy = 0.5224
Round 99: Global Test Accuracy = 0.5219
Round 100: Global Test Accuracy = 0.5231
Round 101: Global Test Accuracy = 0.5232
Round 102: Global Test Accuracy = 0.5234
Round 103: Global Test Accuracy = 0.5241
Round 104: Global Test Accuracy = 0.5242
Round 105: Global Test Accuracy = 0.5247
Round 106: Global Test Accuracy = 0.5251
Round 107: Global Test Accuracy = 0.5254
Round 108: Global Test Accuracy = 0.5263
Round 109: Global Test Accuracy = 0.5267
Round 110: Global Test Accuracy = 0.5272
Round 111: Global Test Accuracy = 0.5281
Round 112: Global Test Accuracy = 0.5279
Round 113: Global Test Accuracy = 0.5274
Round 114: Global Test Accuracy = 0.5274
Round 115: Global Test Accuracy = 0.5284
Round 116: Global Test Accuracy = 0.5287
Round 117: Global Test Accuracy = 0.5296
Round 118: Global Test Accuracy = 0.5296
Round 119: Global Test Accuracy = 0.5301
Round 120: Global Test Accuracy = 0.5314
Round 121: Global Test Accuracy = 0.5321
Round 122: Global Test Accuracy = 0.5310
Round 123: Global Test Accuracy = 0.5317
Round 124: Global Test Accuracy = 0.5320
Round 125: Global Test Accuracy = 0.5311
Round 126: Global Test Accuracy = 0.5320
Round 127: Global Test Accuracy = 0.5318
Round 128: Global Test Accuracy = 0.5327
Round 129: Global Test Accuracy = 0.5340
Round 130: Global Test Accuracy = 0.5341
Round 131: Global Test Accuracy = 0.5343
Round 132: Global Test Accuracy = 0.5340
Round 133: Global Test Accuracy = 0.5342
Round 134: Global Test Accuracy = 0.5347
Round 135: Global Test Accuracy = 0.5351
Round 136: Global Test Accuracy = 0.5356
Round 137: Global Test Accuracy = 0.5354
Round 138: Global Test Accuracy = 0.5359
Round 139: Global Test Accuracy = 0.5360
Round 140: Global Test Accuracy = 0.5361
Round 141: Global Test Accuracy = 0.5367
Round 142: Global Test Accuracy = 0.5367
Round 143: Global Test Accuracy = 0.5369
Round 144: Global Test Accuracy = 0.5368
Round 145: Global Test Accuracy = 0.5371
Round 146: Global Test Accuracy = 0.5370
Round 147: Global Test Accuracy = 0.5372
Round 148: Global Test Accuracy = 0.5375
Round 149: Global Test Accuracy = 0.5375
Round 150: Global Test Accuracy = 0.5379
Round 151: Global Test Accuracy = 0.5379
Round 152: Global Test Accuracy = 0.5386
Round 153: Global Test Accuracy = 0.5379
Round 154: Global Test Accuracy = 0.5381
Round 155: Global Test Accuracy = 0.5385
Round 156: Global Test Accuracy = 0.5386
Round 157: Global Test Accuracy = 0.5394
Round 158: Global Test Accuracy = 0.5400
Round 159: Global Test Accuracy = 0.5397
Round 160: Global Test Accuracy = 0.5392
Round 161: Global Test Accuracy = 0.5396
Round 162: Global Test Accuracy = 0.5394
Round 163: Global Test Accuracy = 0.5397
Round 164: Global Test Accuracy = 0.5405
Round 165: Global Test Accuracy = 0.5411
Round 166: Global Test Accuracy = 0.5413
Round 167: Global Test Accuracy = 0.5421
Round 168: Global Test Accuracy = 0.5417
Round 169: Global Test Accuracy = 0.5421
Round 170: Global Test Accuracy = 0.5418
Round 171: Global Test Accuracy = 0.5416
Round 172: Global Test Accuracy = 0.5419
Round 173: Global Test Accuracy = 0.5422
Round 174: Global Test Accuracy = 0.5425
Round 175: Global Test Accuracy = 0.5423
Round 176: Global Test Accuracy = 0.5423
Round 177: Global Test Accuracy = 0.5428
Round 178: Global Test Accuracy = 0.5429
Round 179: Global Test Accuracy = 0.5437
Round 180: Global Test Accuracy = 0.5433
Round 181: Global Test Accuracy = 0.5436
Round 182: Global Test Accuracy = 0.5430
Round 183: Global Test Accuracy = 0.5435
Round 184: Global Test Accuracy = 0.5449
Round 185: Global Test Accuracy = 0.5442
Round 186: Global Test Accuracy = 0.5442
Round 187: Global Test Accuracy = 0.5444
Round 188: Global Test Accuracy = 0.5447
Round 189: Global Test Accuracy = 0.5444
Round 190: Global Test Accuracy = 0.5439
Round 191: Global Test Accuracy = 0.5444
Round 192: Global Test Accuracy = 0.5448
Round 193: Global Test Accuracy = 0.5453
Round 194: Global Test Accuracy = 0.5454
Round 195: Global Test Accuracy = 0.5454
Round 196: Global Test Accuracy = 0.5462
Round 197: Global Test Accuracy = 0.5455
Round 198: Global Test Accuracy = 0.5456
Round 199: Global Test Accuracy = 0.5455
Round 200: Global Test Accuracy = 0.5455
//train_time: 43235.388 ms//end
//Log Max memory for Large1: 6391156736.0 //end
//Log Max memory for Large2: 6681964544.0 //end
//Log Max memory for Large3: 6133170176.0 //end
//Log Max memory for Large4: 7152242688.0 //end
//Log Max memory for Server: 18525507584.0 //end
//Log Large1 network: 75090007.0 //end
//Log Large2 network: 112062961.0 //end
//Log Large3 network: 75138825.0 //end
//Log Large4 network: 111949393.0 //end
//Log Server network: 372165037.0 //end
//Log Total Actual Train Comm Cost: 711.83 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 1.6846265148028001
Average test accuracy, 0.5454601567804457
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 668.58 MB //end
[36m(Trainer pid=124674, ip=192.168.14.62)[0m Running GCN_arxiv[32m [repeated 9x across cluster][0m
[36m(Trainer pid=124673, ip=192.168.14.62)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=124673, ip=192.168.14.62)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: ogbn-arxiv, Trainers: 10, Distribution: average, IID Beta: 10.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'ogbn-arxiv', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
/usr/local/lib/python3.11/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):
/usr/local/lib/python3.11/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):
/usr/local/lib/python3.11/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
Initialization start: network data collected.
2025-05-15 03:13:47,916	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 03:13:47,916	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 03:13:47,922	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=121447, ip=192.168.14.54)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=121447, ip=192.168.14.54)[0m   return torch.load(io.BytesIO(b))
/usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
[36m(Trainer pid=121447, ip=192.168.14.54)[0m Running GCN_arxiv
Running GCN_arxiv
//Log init_time: 5777.555 ms //end
//Log Large1 init network: 169162.0 //end
//Log Large2 init network: 141049.0 //end
//Log Large3 init network: 170320.0 //end
//Log Large4 init network: 151202.0 //end
//Log Server init network: 95959172.0 //end
//Log Initialization Communication Cost (MB): 92.12 //end
Pretrain start time recorded.
//pretrain_time: 4.6 ms//end
//Log Max memory for Large1: 6422970368.0 //end
//Log Max memory for Large2: 5719502848.0 //end
//Log Max memory for Large3: 6155440128.0 //end
//Log Max memory for Large4: 6065487872.0 //end
//Log Max memory for Server: 18495651840.0 //end
//Log Large1 network: 1030683.0 //end
//Log Large2 network: 887388.0 //end
//Log Large3 network: 1041023.0 //end
//Log Large4 network: 811719.0 //end
//Log Server network: 5631823.0 //end
//Log Total Actual Pretrain Comm Cost: 8.97 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.0885
Round 2: Global Test Accuracy = 0.0928
Round 3: Global Test Accuracy = 0.0698
Round 4: Global Test Accuracy = 0.0811
Round 5: Global Test Accuracy = 0.1439
Round 6: Global Test Accuracy = 0.2211
Round 7: Global Test Accuracy = 0.2522
Round 8: Global Test Accuracy = 0.2635
Round 9: Global Test Accuracy = 0.2707
Round 10: Global Test Accuracy = 0.2779
Round 11: Global Test Accuracy = 0.2857
Round 12: Global Test Accuracy = 0.2929
Round 13: Global Test Accuracy = 0.3015
Round 14: Global Test Accuracy = 0.3097
Round 15: Global Test Accuracy = 0.3162
Round 16: Global Test Accuracy = 0.3229
Round 17: Global Test Accuracy = 0.3323
Round 18: Global Test Accuracy = 0.3395
Round 19: Global Test Accuracy = 0.3478
Round 20: Global Test Accuracy = 0.3529
Round 21: Global Test Accuracy = 0.3603
Round 22: Global Test Accuracy = 0.3662
Round 23: Global Test Accuracy = 0.3725
Round 24: Global Test Accuracy = 0.3795
Round 25: Global Test Accuracy = 0.3854
Round 26: Global Test Accuracy = 0.3929
Round 27: Global Test Accuracy = 0.3973
Round 28: Global Test Accuracy = 0.4010
Round 29: Global Test Accuracy = 0.4068
Round 30: Global Test Accuracy = 0.4119
Round 31: Global Test Accuracy = 0.4156
Round 32: Global Test Accuracy = 0.4202
Round 33: Global Test Accuracy = 0.4263
Round 34: Global Test Accuracy = 0.4301
Round 35: Global Test Accuracy = 0.4339
Round 36: Global Test Accuracy = 0.4378
Round 37: Global Test Accuracy = 0.4426
Round 38: Global Test Accuracy = 0.4463
Round 39: Global Test Accuracy = 0.4489
Round 40: Global Test Accuracy = 0.4531
Round 41: Global Test Accuracy = 0.4575
Round 42: Global Test Accuracy = 0.4608
Round 43: Global Test Accuracy = 0.4641
Round 44: Global Test Accuracy = 0.4648
Round 45: Global Test Accuracy = 0.4668
Round 46: Global Test Accuracy = 0.4702
Round 47: Global Test Accuracy = 0.4720
Round 48: Global Test Accuracy = 0.4723
Round 49: Global Test Accuracy = 0.4738
Round 50: Global Test Accuracy = 0.4758
Round 51: Global Test Accuracy = 0.4782
Round 52: Global Test Accuracy = 0.4808
Round 53: Global Test Accuracy = 0.4826
Round 54: Global Test Accuracy = 0.4844
Round 55: Global Test Accuracy = 0.4862
Round 56: Global Test Accuracy = 0.4870
Round 57: Global Test Accuracy = 0.4876
Round 58: Global Test Accuracy = 0.4897
Round 59: Global Test Accuracy = 0.4901
Round 60: Global Test Accuracy = 0.4913
Round 61: Global Test Accuracy = 0.4923
Round 62: Global Test Accuracy = 0.4943
Round 63: Global Test Accuracy = 0.4951
Round 64: Global Test Accuracy = 0.4966
Round 65: Global Test Accuracy = 0.4974
Round 66: Global Test Accuracy = 0.4988
Round 67: Global Test Accuracy = 0.5000
Round 68: Global Test Accuracy = 0.5015
Round 69: Global Test Accuracy = 0.5027
Round 70: Global Test Accuracy = 0.5038
Round 71: Global Test Accuracy = 0.5044
Round 72: Global Test Accuracy = 0.5044
Round 73: Global Test Accuracy = 0.5047
Round 74: Global Test Accuracy = 0.5055
Round 75: Global Test Accuracy = 0.5062
Round 76: Global Test Accuracy = 0.5076
Round 77: Global Test Accuracy = 0.5075
Round 78: Global Test Accuracy = 0.5092
Round 79: Global Test Accuracy = 0.5098
Round 80: Global Test Accuracy = 0.5113
Round 81: Global Test Accuracy = 0.5112
Round 82: Global Test Accuracy = 0.5124
Round 83: Global Test Accuracy = 0.5131
Round 84: Global Test Accuracy = 0.5133
Round 85: Global Test Accuracy = 0.5147
Round 86: Global Test Accuracy = 0.5151
Round 87: Global Test Accuracy = 0.5166
Round 88: Global Test Accuracy = 0.5167
Round 89: Global Test Accuracy = 0.5174
Round 90: Global Test Accuracy = 0.5176
Round 91: Global Test Accuracy = 0.5172
Round 92: Global Test Accuracy = 0.5187
Round 93: Global Test Accuracy = 0.5190
Round 94: Global Test Accuracy = 0.5197
Round 95: Global Test Accuracy = 0.5207
Round 96: Global Test Accuracy = 0.5211
Round 97: Global Test Accuracy = 0.5204
Round 98: Global Test Accuracy = 0.5207
Round 99: Global Test Accuracy = 0.5202
Round 100: Global Test Accuracy = 0.5217
Round 101: Global Test Accuracy = 0.5218
Round 102: Global Test Accuracy = 0.5225
Round 103: Global Test Accuracy = 0.5229
Round 104: Global Test Accuracy = 0.5237
Round 105: Global Test Accuracy = 0.5247
Round 106: Global Test Accuracy = 0.5261
Round 107: Global Test Accuracy = 0.5261
Round 108: Global Test Accuracy = 0.5254
Round 109: Global Test Accuracy = 0.5264
Round 110: Global Test Accuracy = 0.5261
Round 111: Global Test Accuracy = 0.5262
Round 112: Global Test Accuracy = 0.5270
Round 113: Global Test Accuracy = 0.5279
Round 114: Global Test Accuracy = 0.5277
Round 115: Global Test Accuracy = 0.5278
Round 116: Global Test Accuracy = 0.5286
Round 117: Global Test Accuracy = 0.5284
Round 118: Global Test Accuracy = 0.5290
Round 119: Global Test Accuracy = 0.5292
Round 120: Global Test Accuracy = 0.5299
Round 121: Global Test Accuracy = 0.5308
Round 122: Global Test Accuracy = 0.5300
Round 123: Global Test Accuracy = 0.5305
Round 124: Global Test Accuracy = 0.5307
Round 125: Global Test Accuracy = 0.5314
Round 126: Global Test Accuracy = 0.5318
Round 127: Global Test Accuracy = 0.5317
Round 128: Global Test Accuracy = 0.5316
Round 129: Global Test Accuracy = 0.5324
Round 130: Global Test Accuracy = 0.5327
Round 131: Global Test Accuracy = 0.5331
Round 132: Global Test Accuracy = 0.5336
Round 133: Global Test Accuracy = 0.5334
Round 134: Global Test Accuracy = 0.5331
Round 135: Global Test Accuracy = 0.5330
Round 136: Global Test Accuracy = 0.5338
Round 137: Global Test Accuracy = 0.5342
Round 138: Global Test Accuracy = 0.5348
Round 139: Global Test Accuracy = 0.5348
Round 140: Global Test Accuracy = 0.5352
Round 141: Global Test Accuracy = 0.5353
Round 142: Global Test Accuracy = 0.5352
Round 143: Global Test Accuracy = 0.5360
Round 144: Global Test Accuracy = 0.5366
Round 145: Global Test Accuracy = 0.5365
Round 146: Global Test Accuracy = 0.5361
Round 147: Global Test Accuracy = 0.5361
Round 148: Global Test Accuracy = 0.5367
Round 149: Global Test Accuracy = 0.5366
Round 150: Global Test Accuracy = 0.5376
Round 151: Global Test Accuracy = 0.5375
Round 152: Global Test Accuracy = 0.5378
Round 153: Global Test Accuracy = 0.5383
Round 154: Global Test Accuracy = 0.5380
Round 155: Global Test Accuracy = 0.5383
Round 156: Global Test Accuracy = 0.5383
Round 157: Global Test Accuracy = 0.5384
Round 158: Global Test Accuracy = 0.5384
Round 159: Global Test Accuracy = 0.5392
Round 160: Global Test Accuracy = 0.5394
Round 161: Global Test Accuracy = 0.5395
Round 162: Global Test Accuracy = 0.5401
Round 163: Global Test Accuracy = 0.5398
Round 164: Global Test Accuracy = 0.5396
Round 165: Global Test Accuracy = 0.5397
Round 166: Global Test Accuracy = 0.5400
Round 167: Global Test Accuracy = 0.5405
Round 168: Global Test Accuracy = 0.5404
Round 169: Global Test Accuracy = 0.5408
Round 170: Global Test Accuracy = 0.5408
Round 171: Global Test Accuracy = 0.5406
Round 172: Global Test Accuracy = 0.5408
Round 173: Global Test Accuracy = 0.5408
Round 174: Global Test Accuracy = 0.5412
Round 175: Global Test Accuracy = 0.5414
Round 176: Global Test Accuracy = 0.5417
Round 177: Global Test Accuracy = 0.5416
Round 178: Global Test Accuracy = 0.5420
Round 179: Global Test Accuracy = 0.5418
Round 180: Global Test Accuracy = 0.5417
Round 181: Global Test Accuracy = 0.5413
Round 182: Global Test Accuracy = 0.5418
Round 183: Global Test Accuracy = 0.5421
Round 184: Global Test Accuracy = 0.5430
Round 185: Global Test Accuracy = 0.5432
Round 186: Global Test Accuracy = 0.5434
Round 187: Global Test Accuracy = 0.5433
Round 188: Global Test Accuracy = 0.5430
Round 189: Global Test Accuracy = 0.5432
Round 190: Global Test Accuracy = 0.5428
Round 191: Global Test Accuracy = 0.5430
Round 192: Global Test Accuracy = 0.5429
Round 193: Global Test Accuracy = 0.5435
Round 194: Global Test Accuracy = 0.5438
Round 195: Global Test Accuracy = 0.5440
Round 196: Global Test Accuracy = 0.5439
Round 197: Global Test Accuracy = 0.5441
Round 198: Global Test Accuracy = 0.5439
Round 199: Global Test Accuracy = 0.5440
Round 200: Global Test Accuracy = 0.5444
//train_time: 47228.637 ms//end
//Log Max memory for Large1: 7080779776.0 //end
//Log Max memory for Large2: 6056484864.0 //end
//Log Max memory for Large3: 6827548672.0 //end
//Log Max memory for Large4: 6578225152.0 //end
//Log Max memory for Server: 18496917504.0 //end
//Log Large1 network: 112132217.0 //end
//Log Large2 network: 75427857.0 //end
//Log Large3 network: 112158016.0 //end
//Log Large4 network: 75289089.0 //end
//Log Server network: 372800780.0 //end
//Log Total Actual Train Comm Cost: 713.17 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 1.6895119210362664
Average test accuracy, 0.5443902639754747
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 668.58 MB //end
[36m(Trainer pid=121272, ip=192.168.39.156)[0m Running GCN_arxiv[32m [repeated 9x across cluster][0m
[36m(Trainer pid=121272, ip=192.168.39.156)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=121272, ip=192.168.39.156)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: ogbn-arxiv, Trainers: 10, Distribution: average, IID Beta: 10000.0, Hops: 1, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'ogbn-arxiv', 'method': 'fedgcn', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 1, 'iid_beta': 10000.0, 'distribution_type': 'average', 'gpu': False}
/usr/local/lib/python3.11/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):
/usr/local/lib/python3.11/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):
/usr/local/lib/python3.11/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
Initialization start: network data collected.
2025-05-15 03:15:47,858	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 03:15:47,858	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 03:15:47,865	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
/usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
[36m(Trainer pid=122072, ip=192.168.42.57)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=122072, ip=192.168.42.57)[0m   return torch.load(io.BytesIO(b))
[36m(Trainer pid=122015, ip=192.168.39.156)[0m running AggreGCN_Arxiv
Running AggreGCN_Arxiv
//Log init_time: 6484.373 ms //end
//Log Large1 init network: 259744.0 //end
//Log Large2 init network: 285717.0 //end
//Log Large3 init network: 211475.0 //end
//Log Large4 init network: 283365.0 //end
//Log Server init network: 379054946.0 //end
//Log Initialization Communication Cost (MB): 362.49 //end
Pretrain start time recorded.
server aggregates all local neighbor feature sums
clients received feature aggregation from server
//pretrain_time: 6493.782999999999 ms//end
//Log Max memory for Large1: 6435221504.0 //end
//Log Max memory for Large2: 6696882176.0 //end
//Log Max memory for Large3: 6165364736.0 //end
//Log Max memory for Large4: 7070957568.0 //end
//Log Max memory for Server: 19187908608.0 //end
//Log Large1 network: 178350974.0 //end
//Log Large2 network: 265294409.0 //end
//Log Large3 network: 178634696.0 //end
//Log Large4 network: 267770474.0 //end
//Log Server network: 491710209.0 //end
//Log Total Actual Pretrain Comm Cost: 1317.75 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.2135
Round 2: Global Test Accuracy = 0.2190
Round 3: Global Test Accuracy = 0.2336
Round 4: Global Test Accuracy = 0.2243
Round 5: Global Test Accuracy = 0.2328
Round 6: Global Test Accuracy = 0.2466
Round 7: Global Test Accuracy = 0.2444
Round 8: Global Test Accuracy = 0.2422
Round 9: Global Test Accuracy = 0.2491
Round 10: Global Test Accuracy = 0.2523
Round 11: Global Test Accuracy = 0.2543
Round 12: Global Test Accuracy = 0.2542
Round 13: Global Test Accuracy = 0.2544
Round 14: Global Test Accuracy = 0.2985
Round 15: Global Test Accuracy = 0.2727
Round 16: Global Test Accuracy = 0.2524
Round 17: Global Test Accuracy = 0.2482
Round 18: Global Test Accuracy = 0.2492
Round 19: Global Test Accuracy = 0.2514
Round 20: Global Test Accuracy = 0.2570
Round 21: Global Test Accuracy = 0.2992
Round 22: Global Test Accuracy = 0.3237
Round 23: Global Test Accuracy = 0.2926
Round 24: Global Test Accuracy = 0.2969
Round 25: Global Test Accuracy = 0.2953
Round 26: Global Test Accuracy = 0.2826
Round 27: Global Test Accuracy = 0.3086
Round 28: Global Test Accuracy = 0.2535
Round 29: Global Test Accuracy = 0.3013
Round 30: Global Test Accuracy = 0.2956
Round 31: Global Test Accuracy = 0.2740
Round 32: Global Test Accuracy = 0.3665
Round 33: Global Test Accuracy = 0.3537
Round 34: Global Test Accuracy = 0.3173
Round 35: Global Test Accuracy = 0.2993
Round 36: Global Test Accuracy = 0.3694
Round 37: Global Test Accuracy = 0.3603
Round 38: Global Test Accuracy = 0.2839
Round 39: Global Test Accuracy = 0.3637
Round 40: Global Test Accuracy = 0.2828
Round 41: Global Test Accuracy = 0.3552
Round 42: Global Test Accuracy = 0.3732
Round 43: Global Test Accuracy = 0.3445
Round 44: Global Test Accuracy = 0.3759
Round 45: Global Test Accuracy = 0.3301
Round 46: Global Test Accuracy = 0.3798
Round 47: Global Test Accuracy = 0.4093
Round 48: Global Test Accuracy = 0.3444
Round 49: Global Test Accuracy = 0.4035
Round 50: Global Test Accuracy = 0.3760
Round 51: Global Test Accuracy = 0.3197
Round 52: Global Test Accuracy = 0.4358
Round 53: Global Test Accuracy = 0.3862
Round 54: Global Test Accuracy = 0.4120
Round 55: Global Test Accuracy = 0.4022
Round 56: Global Test Accuracy = 0.4171
Round 57: Global Test Accuracy = 0.3684
Round 58: Global Test Accuracy = 0.4352
Round 59: Global Test Accuracy = 0.3918
Round 60: Global Test Accuracy = 0.4368
Round 61: Global Test Accuracy = 0.3858
Round 62: Global Test Accuracy = 0.3506
Round 63: Global Test Accuracy = 0.3215
Round 64: Global Test Accuracy = 0.3053
Round 65: Global Test Accuracy = 0.3171
Round 66: Global Test Accuracy = 0.4098
Round 67: Global Test Accuracy = 0.3904
Round 68: Global Test Accuracy = 0.3608
Round 69: Global Test Accuracy = 0.3186
Round 70: Global Test Accuracy = 0.4191
Round 71: Global Test Accuracy = 0.4549
Round 72: Global Test Accuracy = 0.4236
Round 73: Global Test Accuracy = 0.4524
Round 74: Global Test Accuracy = 0.3893
Round 75: Global Test Accuracy = 0.4001
Round 76: Global Test Accuracy = 0.4215
Round 77: Global Test Accuracy = 0.3829
Round 78: Global Test Accuracy = 0.3953
Round 79: Global Test Accuracy = 0.4579
Round 80: Global Test Accuracy = 0.4713
Round 81: Global Test Accuracy = 0.4088
Round 82: Global Test Accuracy = 0.4471
Round 83: Global Test Accuracy = 0.4634
Round 84: Global Test Accuracy = 0.3024
Round 85: Global Test Accuracy = 0.4575
Round 86: Global Test Accuracy = 0.4464
Round 87: Global Test Accuracy = 0.4232
Round 88: Global Test Accuracy = 0.4209
Round 89: Global Test Accuracy = 0.4516
Round 90: Global Test Accuracy = 0.4550
Round 91: Global Test Accuracy = 0.4303
Round 92: Global Test Accuracy = 0.4513
Round 93: Global Test Accuracy = 0.4165
Round 94: Global Test Accuracy = 0.4149
Round 95: Global Test Accuracy = 0.3679
Round 96: Global Test Accuracy = 0.3480
Round 97: Global Test Accuracy = 0.4445
Round 98: Global Test Accuracy = 0.4743
Round 99: Global Test Accuracy = 0.4543
Round 100: Global Test Accuracy = 0.4090
Round 101: Global Test Accuracy = 0.4383
Round 102: Global Test Accuracy = 0.4767
Round 103: Global Test Accuracy = 0.3987
Round 104: Global Test Accuracy = 0.4770
Round 105: Global Test Accuracy = 0.4725
Round 106: Global Test Accuracy = 0.4974
Round 107: Global Test Accuracy = 0.3896
Round 108: Global Test Accuracy = 0.4755
Round 109: Global Test Accuracy = 0.4865
Round 110: Global Test Accuracy = 0.4870
Round 111: Global Test Accuracy = 0.4694
Round 112: Global Test Accuracy = 0.4721
Round 113: Global Test Accuracy = 0.4438
Round 114: Global Test Accuracy = 0.4940
Round 115: Global Test Accuracy = 0.4445
Round 116: Global Test Accuracy = 0.4824
Round 117: Global Test Accuracy = 0.4194
Round 118: Global Test Accuracy = 0.4720
Round 119: Global Test Accuracy = 0.4798
Round 120: Global Test Accuracy = 0.4788
Round 121: Global Test Accuracy = 0.4855
Round 122: Global Test Accuracy = 0.4764
Round 123: Global Test Accuracy = 0.4805
Round 124: Global Test Accuracy = 0.4784
Round 125: Global Test Accuracy = 0.4395
Round 126: Global Test Accuracy = 0.4965
Round 127: Global Test Accuracy = 0.4845
Round 128: Global Test Accuracy = 0.4755
Round 129: Global Test Accuracy = 0.4341
Round 130: Global Test Accuracy = 0.4837
Round 131: Global Test Accuracy = 0.4236
Round 132: Global Test Accuracy = 0.4474
Round 133: Global Test Accuracy = 0.4706
Round 134: Global Test Accuracy = 0.4716
Round 135: Global Test Accuracy = 0.4726
Round 136: Global Test Accuracy = 0.4480
Round 137: Global Test Accuracy = 0.4956
Round 138: Global Test Accuracy = 0.4892
Round 139: Global Test Accuracy = 0.5030
Round 140: Global Test Accuracy = 0.4756
Round 141: Global Test Accuracy = 0.4701
Round 142: Global Test Accuracy = 0.4785
Round 143: Global Test Accuracy = 0.4977
Round 144: Global Test Accuracy = 0.4697
Round 145: Global Test Accuracy = 0.5025
Round 146: Global Test Accuracy = 0.4494
Round 147: Global Test Accuracy = 0.4865
Round 148: Global Test Accuracy = 0.4956
Round 149: Global Test Accuracy = 0.4533
Round 150: Global Test Accuracy = 0.5026
Round 151: Global Test Accuracy = 0.4932
Round 152: Global Test Accuracy = 0.4522
Round 153: Global Test Accuracy = 0.4964
Round 154: Global Test Accuracy = 0.4828
Round 155: Global Test Accuracy = 0.5123
Round 156: Global Test Accuracy = 0.4758
Round 157: Global Test Accuracy = 0.4828
Round 158: Global Test Accuracy = 0.4285
Round 159: Global Test Accuracy = 0.5127
Round 160: Global Test Accuracy = 0.4922
Round 161: Global Test Accuracy = 0.5108
Round 162: Global Test Accuracy = 0.5169
Round 163: Global Test Accuracy = 0.5210
Round 164: Global Test Accuracy = 0.4662
Round 165: Global Test Accuracy = 0.4692
Round 166: Global Test Accuracy = 0.4700
Round 167: Global Test Accuracy = 0.4987
Round 168: Global Test Accuracy = 0.4145
Round 169: Global Test Accuracy = 0.5144
Round 170: Global Test Accuracy = 0.5121
Round 171: Global Test Accuracy = 0.4424
Round 172: Global Test Accuracy = 0.5130
Round 173: Global Test Accuracy = 0.5245
Round 174: Global Test Accuracy = 0.4548
Round 175: Global Test Accuracy = 0.4927
Round 176: Global Test Accuracy = 0.4061
Round 177: Global Test Accuracy = 0.5237
Round 178: Global Test Accuracy = 0.4880
Round 179: Global Test Accuracy = 0.4606
Round 180: Global Test Accuracy = 0.4772
Round 181: Global Test Accuracy = 0.5202
Round 182: Global Test Accuracy = 0.5230
Round 183: Global Test Accuracy = 0.4984
Round 184: Global Test Accuracy = 0.4950
Round 185: Global Test Accuracy = 0.4751
Round 186: Global Test Accuracy = 0.3828
Round 187: Global Test Accuracy = 0.5109
Round 188: Global Test Accuracy = 0.5034
Round 189: Global Test Accuracy = 0.4807
Round 190: Global Test Accuracy = 0.4287
Round 191: Global Test Accuracy = 0.5099
Round 192: Global Test Accuracy = 0.5007
Round 193: Global Test Accuracy = 0.4728
Round 194: Global Test Accuracy = 0.5166
Round 195: Global Test Accuracy = 0.4963
Round 196: Global Test Accuracy = 0.5002
Round 197: Global Test Accuracy = 0.5215
Round 198: Global Test Accuracy = 0.5106
Round 199: Global Test Accuracy = 0.4611
Round 200: Global Test Accuracy = 0.5063
//train_time: 319060.721 ms//end
//Log Max memory for Large1: 8153255936.0 //end
//Log Max memory for Large2: 9916465152.0 //end
//Log Max memory for Large3: 8420851712.0 //end
//Log Max memory for Large4: 9616732160.0 //end
//Log Max memory for Server: 19105017856.0 //end
//Log Large1 network: 80750181.0 //end
//Log Large2 network: 119261426.0 //end
//Log Large3 network: 80806163.0 //end
//Log Large4 network: 118670151.0 //end
//Log Server network: 384612108.0 //end
//Log Total Actual Train Comm Cost: 747.78 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 2.02640028951335
Average test accuracy, 0.506306195090838
//Log Theoretical Pretrain Comm Cost: 1290.84 MB //end
//Log Theoretical Train Comm Cost: 668.58 MB //end
[36m(Trainer pid=122131, ip=192.168.42.57)[0m running AggreGCN_Arxiv[32m [repeated 9x across cluster][0m
[36m(Trainer pid=126128, ip=192.168.14.62)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=126128, ip=192.168.14.62)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: ogbn-arxiv, Trainers: 10, Distribution: average, IID Beta: 100.0, Hops: 1, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'ogbn-arxiv', 'method': 'fedgcn', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 1, 'iid_beta': 100.0, 'distribution_type': 'average', 'gpu': False}
/usr/local/lib/python3.11/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):
/usr/local/lib/python3.11/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):
/usr/local/lib/python3.11/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
Initialization start: network data collected.
2025-05-15 03:22:27,154	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 03:22:27,154	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 03:22:27,161	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
[36m(Trainer pid=124038, ip=192.168.14.54)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=124038, ip=192.168.14.54)[0m   return torch.load(io.BytesIO(b))
/usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
[36m(Trainer pid=124038, ip=192.168.14.54)[0m running AggreGCN_Arxiv
Running AggreGCN_Arxiv
//Log init_time: 6271.259 ms //end
//Log Large1 init network: 297366.0 //end
//Log Large2 init network: 1021435.0 //end
//Log Large3 init network: 271184.0 //end
//Log Large4 init network: 217785.0 //end
//Log Server init network: 378574585.0 //end
//Log Initialization Communication Cost (MB): 362.76 //end
Pretrain start time recorded.
server aggregates all local neighbor feature sums
clients received feature aggregation from server
//pretrain_time: 5988.739 ms//end
//Log Max memory for Large1: 7007977472.0 //end
//Log Max memory for Large2: 6165184512.0 //end
//Log Max memory for Large3: 6725722112.0 //end
//Log Max memory for Large4: 6508314624.0 //end
//Log Max memory for Server: 19240599552.0 //end
//Log Large1 network: 266345225.0 //end
//Log Large2 network: 177194546.0 //end
//Log Large3 network: 265842560.0 //end
//Log Large4 network: 178032257.0 //end
//Log Server network: 490685066.0 //end
//Log Total Actual Pretrain Comm Cost: 1314.26 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.2128
Round 2: Global Test Accuracy = 0.2184
Round 3: Global Test Accuracy = 0.2278
Round 4: Global Test Accuracy = 0.2263
Round 5: Global Test Accuracy = 0.2305
Round 6: Global Test Accuracy = 0.2439
Round 7: Global Test Accuracy = 0.2527
Round 8: Global Test Accuracy = 0.2531
Round 9: Global Test Accuracy = 0.2645
Round 10: Global Test Accuracy = 0.2707
Round 11: Global Test Accuracy = 0.2561
Round 12: Global Test Accuracy = 0.2539
Round 13: Global Test Accuracy = 0.2559
Round 14: Global Test Accuracy = 0.2542
Round 15: Global Test Accuracy = 0.2530
Round 16: Global Test Accuracy = 0.2563
Round 17: Global Test Accuracy = 0.2537
Round 18: Global Test Accuracy = 0.3365
Round 19: Global Test Accuracy = 0.2570
Round 20: Global Test Accuracy = 0.2554
Round 21: Global Test Accuracy = 0.2539
Round 22: Global Test Accuracy = 0.3141
Round 23: Global Test Accuracy = 0.2493
Round 24: Global Test Accuracy = 0.3179
Round 25: Global Test Accuracy = 0.2940
Round 26: Global Test Accuracy = 0.2490
Round 27: Global Test Accuracy = 0.2833
Round 28: Global Test Accuracy = 0.2965
Round 29: Global Test Accuracy = 0.2549
Round 30: Global Test Accuracy = 0.3237
Round 31: Global Test Accuracy = 0.3382
Round 32: Global Test Accuracy = 0.2635
Round 33: Global Test Accuracy = 0.2541
Round 34: Global Test Accuracy = 0.3634
Round 35: Global Test Accuracy = 0.2902
Round 36: Global Test Accuracy = 0.3684
Round 37: Global Test Accuracy = 0.3835
Round 38: Global Test Accuracy = 0.3804
Round 39: Global Test Accuracy = 0.3660
Round 40: Global Test Accuracy = 0.3003
Round 41: Global Test Accuracy = 0.3504
Round 42: Global Test Accuracy = 0.4043
Round 43: Global Test Accuracy = 0.3315
Round 44: Global Test Accuracy = 0.3586
Round 45: Global Test Accuracy = 0.3494
Round 46: Global Test Accuracy = 0.3410
Round 47: Global Test Accuracy = 0.3626
Round 48: Global Test Accuracy = 0.3415
Round 49: Global Test Accuracy = 0.3378
Round 50: Global Test Accuracy = 0.3773
Round 51: Global Test Accuracy = 0.4219
Round 52: Global Test Accuracy = 0.4007
Round 53: Global Test Accuracy = 0.3949
Round 54: Global Test Accuracy = 0.4119
Round 55: Global Test Accuracy = 0.4181
Round 56: Global Test Accuracy = 0.3596
Round 57: Global Test Accuracy = 0.3748
Round 58: Global Test Accuracy = 0.4452
Round 59: Global Test Accuracy = 0.3902
Round 60: Global Test Accuracy = 0.4123
Round 61: Global Test Accuracy = 0.3523
Round 62: Global Test Accuracy = 0.4475
Round 63: Global Test Accuracy = 0.3661
Round 64: Global Test Accuracy = 0.4155
Round 65: Global Test Accuracy = 0.4261
Round 66: Global Test Accuracy = 0.4470
Round 67: Global Test Accuracy = 0.3558
Round 68: Global Test Accuracy = 0.4173
Round 69: Global Test Accuracy = 0.4100
Round 70: Global Test Accuracy = 0.3771
Round 71: Global Test Accuracy = 0.4360
Round 72: Global Test Accuracy = 0.3629
Round 73: Global Test Accuracy = 0.3724
Round 74: Global Test Accuracy = 0.4027
Round 75: Global Test Accuracy = 0.4535
Round 76: Global Test Accuracy = 0.4363
Round 77: Global Test Accuracy = 0.4051
Round 78: Global Test Accuracy = 0.4183
Round 79: Global Test Accuracy = 0.4682
Round 80: Global Test Accuracy = 0.4450
Round 81: Global Test Accuracy = 0.4035
Round 82: Global Test Accuracy = 0.4552
Round 83: Global Test Accuracy = 0.4633
Round 84: Global Test Accuracy = 0.4417
Round 85: Global Test Accuracy = 0.3841
Round 86: Global Test Accuracy = 0.4334
Round 87: Global Test Accuracy = 0.4277
Round 88: Global Test Accuracy = 0.4601
Round 89: Global Test Accuracy = 0.4589
Round 90: Global Test Accuracy = 0.3514
Round 91: Global Test Accuracy = 0.4230
Round 92: Global Test Accuracy = 0.4706
Round 93: Global Test Accuracy = 0.4251
Round 94: Global Test Accuracy = 0.4243
Round 95: Global Test Accuracy = 0.4104
Round 96: Global Test Accuracy = 0.4142
Round 97: Global Test Accuracy = 0.3870
Round 98: Global Test Accuracy = 0.4723
Round 99: Global Test Accuracy = 0.4157
Round 100: Global Test Accuracy = 0.4818
Round 101: Global Test Accuracy = 0.4404
Round 102: Global Test Accuracy = 0.4540
Round 103: Global Test Accuracy = 0.4309
Round 104: Global Test Accuracy = 0.4772
Round 105: Global Test Accuracy = 0.4848
Round 106: Global Test Accuracy = 0.4807
Round 107: Global Test Accuracy = 0.4164
Round 108: Global Test Accuracy = 0.4753
Round 109: Global Test Accuracy = 0.4883
Round 110: Global Test Accuracy = 0.4591
Round 111: Global Test Accuracy = 0.4504
Round 112: Global Test Accuracy = 0.4629
Round 113: Global Test Accuracy = 0.4927
Round 114: Global Test Accuracy = 0.4771
Round 115: Global Test Accuracy = 0.4348
Round 116: Global Test Accuracy = 0.4783
Round 117: Global Test Accuracy = 0.4713
Round 118: Global Test Accuracy = 0.4695
Round 119: Global Test Accuracy = 0.4783
Round 120: Global Test Accuracy = 0.4226
Round 121: Global Test Accuracy = 0.4383
Round 122: Global Test Accuracy = 0.4826
Round 123: Global Test Accuracy = 0.4374
Round 124: Global Test Accuracy = 0.4895
Round 125: Global Test Accuracy = 0.5010
Round 126: Global Test Accuracy = 0.4765
Round 127: Global Test Accuracy = 0.4722
Round 128: Global Test Accuracy = 0.4869
Round 129: Global Test Accuracy = 0.4768
Round 130: Global Test Accuracy = 0.4467
Round 131: Global Test Accuracy = 0.3184
Round 132: Global Test Accuracy = 0.4409
Round 133: Global Test Accuracy = 0.4934
Round 134: Global Test Accuracy = 0.4670
Round 135: Global Test Accuracy = 0.4298
Round 136: Global Test Accuracy = 0.4460
Round 137: Global Test Accuracy = 0.4962
Round 138: Global Test Accuracy = 0.4847
Round 139: Global Test Accuracy = 0.4359
Round 140: Global Test Accuracy = 0.4790
Round 141: Global Test Accuracy = 0.4899
Round 142: Global Test Accuracy = 0.4853
Round 143: Global Test Accuracy = 0.4911
Round 144: Global Test Accuracy = 0.4663
Round 145: Global Test Accuracy = 0.4864
Round 146: Global Test Accuracy = 0.4975
Round 147: Global Test Accuracy = 0.4742
Round 148: Global Test Accuracy = 0.4855
Round 149: Global Test Accuracy = 0.4937
Round 150: Global Test Accuracy = 0.4890
Round 151: Global Test Accuracy = 0.4583
Round 152: Global Test Accuracy = 0.5158
Round 153: Global Test Accuracy = 0.4839
Round 154: Global Test Accuracy = 0.4763
Round 155: Global Test Accuracy = 0.5024
Round 156: Global Test Accuracy = 0.5228
Round 157: Global Test Accuracy = 0.4890
Round 158: Global Test Accuracy = 0.5218
Round 159: Global Test Accuracy = 0.3858
Round 160: Global Test Accuracy = 0.4976
Round 161: Global Test Accuracy = 0.5034
Round 162: Global Test Accuracy = 0.5041
Round 163: Global Test Accuracy = 0.4773
Round 164: Global Test Accuracy = 0.4994
Round 165: Global Test Accuracy = 0.4723
Round 166: Global Test Accuracy = 0.4767
Round 167: Global Test Accuracy = 0.5255
Round 168: Global Test Accuracy = 0.4739
Round 169: Global Test Accuracy = 0.5274
Round 170: Global Test Accuracy = 0.5028
Round 171: Global Test Accuracy = 0.5119
Round 172: Global Test Accuracy = 0.5306
Round 173: Global Test Accuracy = 0.5073
Round 174: Global Test Accuracy = 0.4861
Round 175: Global Test Accuracy = 0.5071
Round 176: Global Test Accuracy = 0.4951
Round 177: Global Test Accuracy = 0.5149
Round 178: Global Test Accuracy = 0.5152
Round 179: Global Test Accuracy = 0.5139
Round 180: Global Test Accuracy = 0.5133
Round 181: Global Test Accuracy = 0.4749
Round 182: Global Test Accuracy = 0.4948
Round 183: Global Test Accuracy = 0.3638
Round 184: Global Test Accuracy = 0.5094
Round 185: Global Test Accuracy = 0.4749
Round 186: Global Test Accuracy = 0.5138
Round 187: Global Test Accuracy = 0.5278
Round 188: Global Test Accuracy = 0.4979
Round 189: Global Test Accuracy = 0.5156
Round 190: Global Test Accuracy = 0.4722
Round 191: Global Test Accuracy = 0.3855
Round 192: Global Test Accuracy = 0.4627
Round 193: Global Test Accuracy = 0.4557
Round 194: Global Test Accuracy = 0.4553
Round 195: Global Test Accuracy = 0.4868
Round 196: Global Test Accuracy = 0.4815
Round 197: Global Test Accuracy = 0.4868
Round 198: Global Test Accuracy = 0.5348
Round 199: Global Test Accuracy = 0.5234
Round 200: Global Test Accuracy = 0.4799
//train_time: 305836.602 ms//end
//Log Max memory for Large1: 10171858944.0 //end
//Log Max memory for Large2: 8323153920.0 //end
//Log Max memory for Large3: 10024415232.0 //end
//Log Max memory for Large4: 8466972672.0 //end
//Log Max memory for Server: 19163971584.0 //end
//Log Large1 network: 118316059.0 //end
//Log Large2 network: 81264809.0 //end
//Log Large3 network: 118354468.0 //end
//Log Large4 network: 80552187.0 //end
//Log Server network: 383982746.0 //end
//Log Total Actual Train Comm Cost: 746.22 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 2.14062848347287
Average test accuracy, 0.4798880727527107
//Log Theoretical Pretrain Comm Cost: 1289.93 MB //end
//Log Theoretical Train Comm Cost: 668.58 MB //end
[36m(Trainer pid=127996, ip=192.168.14.62)[0m running AggreGCN_Arxiv[32m [repeated 9x across cluster][0m
[36m(Trainer pid=123907, ip=192.168.39.156)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=123907, ip=192.168.39.156)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: ogbn-arxiv, Trainers: 10, Distribution: average, IID Beta: 10.0, Hops: 1, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 4, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'ogbn-arxiv', 'method': 'fedgcn', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 1, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
/usr/local/lib/python3.11/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):
/usr/local/lib/python3.11/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):
/usr/local/lib/python3.11/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
Initialization start: network data collected.
2025-05-15 03:28:52,404	INFO worker.py:1429 -- Using address 192.168.45.172:6379 set in the environment variable RAY_ADDRESS
2025-05-15 03:28:52,404	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.45.172:6379...
2025-05-15 03:28:52,411	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.45.172:8265 [39m[22m
[36m(Trainer pid=125779, ip=192.168.42.57)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=125779, ip=192.168.42.57)[0m   return torch.load(io.BytesIO(b))
/usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
[36m(Trainer pid=129777, ip=192.168.14.62)[0m running AggreGCN_Arxiv
Running AggreGCN_Arxiv
//Log init_time: 6781.144 ms //end
//Log Large1 init network: 181652.0 //end
//Log Large2 init network: 358199.0 //end
//Log Large3 init network: 220284.0 //end
//Log Large4 init network: 251288.0 //end
//Log Server init network: 375127051.0 //end
//Log Initialization Communication Cost (MB): 358.71 //end
Pretrain start time recorded.
server aggregates all local neighbor feature sums
clients received feature aggregation from server
//pretrain_time: 6160.019 ms//end
//Log Max memory for Large1: 6448713728.0 //end
//Log Max memory for Large2: 6716321792.0 //end
//Log Max memory for Large3: 6156681216.0 //end
//Log Max memory for Large4: 7033253888.0 //end
//Log Max memory for Server: 19185709056.0 //end
//Log Large1 network: 177621210.0 //end
//Log Large2 network: 265566559.0 //end
//Log Large3 network: 177637800.0 //end
//Log Large4 network: 266983193.0 //end
//Log Server network: 485517457.0 //end
//Log Total Actual Pretrain Comm Cost: 1309.71 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.2137
Round 2: Global Test Accuracy = 0.2180
Round 3: Global Test Accuracy = 0.2256
Round 4: Global Test Accuracy = 0.2249
Round 5: Global Test Accuracy = 0.2361
Round 6: Global Test Accuracy = 0.2540
Round 7: Global Test Accuracy = 0.2550
Round 8: Global Test Accuracy = 0.2533
Round 9: Global Test Accuracy = 0.2823
Round 10: Global Test Accuracy = 0.2551
Round 11: Global Test Accuracy = 0.2534
Round 12: Global Test Accuracy = 0.2482
Round 13: Global Test Accuracy = 0.2557
Round 14: Global Test Accuracy = 0.2524
Round 15: Global Test Accuracy = 0.2568
Round 16: Global Test Accuracy = 0.2712
Round 17: Global Test Accuracy = 0.2542
Round 18: Global Test Accuracy = 0.2524
Round 19: Global Test Accuracy = 0.2503
Round 20: Global Test Accuracy = 0.2525
Round 21: Global Test Accuracy = 0.2833
Round 22: Global Test Accuracy = 0.2806
Round 23: Global Test Accuracy = 0.2994
Round 24: Global Test Accuracy = 0.2536
Round 25: Global Test Accuracy = 0.2570
Round 26: Global Test Accuracy = 0.3024
Round 27: Global Test Accuracy = 0.3282
Round 28: Global Test Accuracy = 0.3089
Round 29: Global Test Accuracy = 0.3336
Round 30: Global Test Accuracy = 0.3726
Round 31: Global Test Accuracy = 0.3220
Round 32: Global Test Accuracy = 0.2816
Round 33: Global Test Accuracy = 0.2576
Round 34: Global Test Accuracy = 0.3304
Round 35: Global Test Accuracy = 0.3553
Round 36: Global Test Accuracy = 0.3486
Round 37: Global Test Accuracy = 0.3357
Round 38: Global Test Accuracy = 0.3354
Round 39: Global Test Accuracy = 0.3781
Round 40: Global Test Accuracy = 0.3463
Round 41: Global Test Accuracy = 0.3770
Round 42: Global Test Accuracy = 0.3600
Round 43: Global Test Accuracy = 0.3140
Round 44: Global Test Accuracy = 0.3533
Round 45: Global Test Accuracy = 0.3891
Round 46: Global Test Accuracy = 0.3337
Round 47: Global Test Accuracy = 0.4044
Round 48: Global Test Accuracy = 0.4200
Round 49: Global Test Accuracy = 0.4074
Round 50: Global Test Accuracy = 0.3694
Round 51: Global Test Accuracy = 0.4128
Round 52: Global Test Accuracy = 0.3735
Round 53: Global Test Accuracy = 0.3615
Round 54: Global Test Accuracy = 0.4109
Round 55: Global Test Accuracy = 0.4091
Round 56: Global Test Accuracy = 0.4395
Round 57: Global Test Accuracy = 0.3698
Round 58: Global Test Accuracy = 0.4049
Round 59: Global Test Accuracy = 0.4324
Round 60: Global Test Accuracy = 0.3500
Round 61: Global Test Accuracy = 0.4412
Round 62: Global Test Accuracy = 0.4037
Round 63: Global Test Accuracy = 0.4212
Round 64: Global Test Accuracy = 0.4085
Round 65: Global Test Accuracy = 0.4010
Round 66: Global Test Accuracy = 0.4226
Round 67: Global Test Accuracy = 0.4204
Round 68: Global Test Accuracy = 0.4060
Round 69: Global Test Accuracy = 0.4527
Round 70: Global Test Accuracy = 0.4398
Round 71: Global Test Accuracy = 0.3365
Round 72: Global Test Accuracy = 0.3018
Round 73: Global Test Accuracy = 0.4641
Round 74: Global Test Accuracy = 0.4304
Round 75: Global Test Accuracy = 0.3822
Round 76: Global Test Accuracy = 0.3820
Round 77: Global Test Accuracy = 0.4466
Round 78: Global Test Accuracy = 0.3743
Round 79: Global Test Accuracy = 0.4597
Round 80: Global Test Accuracy = 0.4718
Round 81: Global Test Accuracy = 0.4610
Round 82: Global Test Accuracy = 0.4237
Round 83: Global Test Accuracy = 0.4164
Round 84: Global Test Accuracy = 0.3313
Round 85: Global Test Accuracy = 0.4005
Round 86: Global Test Accuracy = 0.4257
Round 87: Global Test Accuracy = 0.4165
Round 88: Global Test Accuracy = 0.3622
Round 89: Global Test Accuracy = 0.4516
Round 90: Global Test Accuracy = 0.4068
Round 91: Global Test Accuracy = 0.4649
Round 92: Global Test Accuracy = 0.4375
Round 93: Global Test Accuracy = 0.4203
Round 94: Global Test Accuracy = 0.4509
Round 95: Global Test Accuracy = 0.4306
Round 96: Global Test Accuracy = 0.4637
Round 97: Global Test Accuracy = 0.4569
Round 98: Global Test Accuracy = 0.4519
Round 99: Global Test Accuracy = 0.4611
Round 100: Global Test Accuracy = 0.4733
Round 101: Global Test Accuracy = 0.4874
Round 102: Global Test Accuracy = 0.4568
Round 103: Global Test Accuracy = 0.4608
Round 104: Global Test Accuracy = 0.4863
Round 105: Global Test Accuracy = 0.4477
Round 106: Global Test Accuracy = 0.4416
Round 107: Global Test Accuracy = 0.4827
Round 108: Global Test Accuracy = 0.3683
Round 109: Global Test Accuracy = 0.4736
Round 110: Global Test Accuracy = 0.4390
Round 111: Global Test Accuracy = 0.4999
Round 112: Global Test Accuracy = 0.4464
Round 113: Global Test Accuracy = 0.4823
Round 114: Global Test Accuracy = 0.4815
Round 115: Global Test Accuracy = 0.4715
Round 116: Global Test Accuracy = 0.4319
Round 117: Global Test Accuracy = 0.4853
Round 118: Global Test Accuracy = 0.4126
Round 119: Global Test Accuracy = 0.4873
Round 120: Global Test Accuracy = 0.4201
Round 121: Global Test Accuracy = 0.4764
Round 122: Global Test Accuracy = 0.4604
Round 123: Global Test Accuracy = 0.4989
Round 124: Global Test Accuracy = 0.4732
Round 125: Global Test Accuracy = 0.4754
Round 126: Global Test Accuracy = 0.4819
Round 127: Global Test Accuracy = 0.4862
Round 128: Global Test Accuracy = 0.4587
Round 129: Global Test Accuracy = 0.4053
Round 130: Global Test Accuracy = 0.4639
Round 131: Global Test Accuracy = 0.4679
Round 132: Global Test Accuracy = 0.4264
Round 133: Global Test Accuracy = 0.4858
Round 134: Global Test Accuracy = 0.5050
Round 135: Global Test Accuracy = 0.4944
Round 136: Global Test Accuracy = 0.4495
Round 137: Global Test Accuracy = 0.4959
Round 138: Global Test Accuracy = 0.4220
Round 139: Global Test Accuracy = 0.4933
Round 140: Global Test Accuracy = 0.4655
Round 141: Global Test Accuracy = 0.4359
Round 142: Global Test Accuracy = 0.4925
Round 143: Global Test Accuracy = 0.4938
Round 144: Global Test Accuracy = 0.4475
Round 145: Global Test Accuracy = 0.4918
Round 146: Global Test Accuracy = 0.4625
Round 147: Global Test Accuracy = 0.4554
Round 148: Global Test Accuracy = 0.4856
Round 149: Global Test Accuracy = 0.4336
Round 150: Global Test Accuracy = 0.4993
Round 151: Global Test Accuracy = 0.4915
Round 152: Global Test Accuracy = 0.4915
Round 153: Global Test Accuracy = 0.4849
Round 154: Global Test Accuracy = 0.5124
Round 155: Global Test Accuracy = 0.5059
Round 156: Global Test Accuracy = 0.4595
Round 157: Global Test Accuracy = 0.4798
Round 158: Global Test Accuracy = 0.3766
Round 159: Global Test Accuracy = 0.4988
Round 160: Global Test Accuracy = 0.4594
Round 161: Global Test Accuracy = 0.4904
Round 162: Global Test Accuracy = 0.5090
Round 163: Global Test Accuracy = 0.4981
Round 164: Global Test Accuracy = 0.4793
Round 165: Global Test Accuracy = 0.5024
Round 166: Global Test Accuracy = 0.4126
Round 167: Global Test Accuracy = 0.5162
Round 168: Global Test Accuracy = 0.4981
Round 169: Global Test Accuracy = 0.5082
Round 170: Global Test Accuracy = 0.5283
Round 171: Global Test Accuracy = 0.4613
Round 172: Global Test Accuracy = 0.5207
Round 173: Global Test Accuracy = 0.5022
Round 174: Global Test Accuracy = 0.5190
Round 175: Global Test Accuracy = 0.5252
Round 176: Global Test Accuracy = 0.4873
Round 177: Global Test Accuracy = 0.4261
Round 178: Global Test Accuracy = 0.2907
Round 179: Global Test Accuracy = 0.4005
Round 180: Global Test Accuracy = 0.4970
Round 181: Global Test Accuracy = 0.4851
Round 182: Global Test Accuracy = 0.5054
Round 183: Global Test Accuracy = 0.4908
Round 184: Global Test Accuracy = 0.4627
Round 185: Global Test Accuracy = 0.5167
Round 186: Global Test Accuracy = 0.5073
Round 187: Global Test Accuracy = 0.4655
Round 188: Global Test Accuracy = 0.5221
Round 189: Global Test Accuracy = 0.4741
Round 190: Global Test Accuracy = 0.5230
Round 191: Global Test Accuracy = 0.5058
Round 192: Global Test Accuracy = 0.5094
Round 193: Global Test Accuracy = 0.5235
Round 194: Global Test Accuracy = 0.4769
Round 195: Global Test Accuracy = 0.5137
Round 196: Global Test Accuracy = 0.5210
Round 197: Global Test Accuracy = 0.4374
Round 198: Global Test Accuracy = 0.4909
Round 199: Global Test Accuracy = 0.5175
Round 200: Global Test Accuracy = 0.4873
//train_time: 302037.934 ms//end
//Log Max memory for Large1: 8512446464.0 //end
//Log Max memory for Large2: 9766805504.0 //end
//Log Max memory for Large3: 8315936768.0 //end
//Log Max memory for Large4: 9597517824.0 //end
//Log Max memory for Server: 19127435264.0 //end
//Log Large1 network: 80456129.0 //end
//Log Large2 network: 118846453.0 //end
//Log Large3 network: 80511856.0 //end
//Log Large4 network: 118445920.0 //end
//Log Server network: 384263382.0 //end
//Log Total Actual Train Comm Cost: 746.27 MB //end
Train end time recorded and duration set to gauge.
average_final_test_loss, 2.0866519310120077
Average test accuracy, 0.4872950229409707
//Log Theoretical Pretrain Comm Cost: 1284.67 MB //end
//Log Theoretical Train Comm Cost: 668.58 MB //end
[36m(Trainer pid=125903, ip=192.168.14.54)[0m running AggreGCN_Arxiv[32m [repeated 9x across cluster][0m
[36m(Trainer pid=129836, ip=192.168.14.62)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=129836, ip=192.168.14.62)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m
Benchmark completed.
Traceback (most recent call last):
  File "/Users/yuyang/miniconda3/envs/fedgraph-env-py310/bin/ray", line 8, in <module>
    sys.exit(main())
  File "/Users/yuyang/miniconda3/envs/fedgraph-env-py310/lib/python3.10/site-packages/ray/scripts/scripts.py", line 2691, in main
    return cli()
  File "/Users/yuyang/miniconda3/envs/fedgraph-env-py310/lib/python3.10/site-packages/click/core.py", line 1161, in __call__
    return self.main(*args, **kwargs)
  File "/Users/yuyang/miniconda3/envs/fedgraph-env-py310/lib/python3.10/site-packages/click/core.py", line 1082, in main
    rv = self.invoke(ctx)
  File "/Users/yuyang/miniconda3/envs/fedgraph-env-py310/lib/python3.10/site-packages/click/core.py", line 1697, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/Users/yuyang/miniconda3/envs/fedgraph-env-py310/lib/python3.10/site-packages/click/core.py", line 1697, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/Users/yuyang/miniconda3/envs/fedgraph-env-py310/lib/python3.10/site-packages/click/core.py", line 1443, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/Users/yuyang/miniconda3/envs/fedgraph-env-py310/lib/python3.10/site-packages/click/core.py", line 788, in invoke
    return __callback(*args, **kwargs)
  File "/Users/yuyang/miniconda3/envs/fedgraph-env-py310/lib/python3.10/site-packages/ray/dashboard/modules/job/cli_utils.py", line 54, in wrapper
    return func(*args, **kwargs)
  File "/Users/yuyang/miniconda3/envs/fedgraph-env-py310/lib/python3.10/site-packages/ray/autoscaler/_private/cli_logger.py", line 823, in wrapper
    return f(*args, **kwargs)
  File "/Users/yuyang/miniconda3/envs/fedgraph-env-py310/lib/python3.10/site-packages/ray/dashboard/modules/job/cli.py", line 310, in submit
    job_status = get_or_create_event_loop().run_until_complete(
  File "/Users/yuyang/miniconda3/envs/fedgraph-env-py310/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/Users/yuyang/miniconda3/envs/fedgraph-env-py310/lib/python3.10/site-packages/ray/dashboard/modules/job/cli.py", line 99, in _tail_logs
    return _log_job_status(client, job_id)
  File "/Users/yuyang/miniconda3/envs/fedgraph-env-py310/lib/python3.10/site-packages/ray/dashboard/modules/job/cli.py", line 78, in _log_job_status
    info = client.get_job_info(job_id)
  File "/Users/yuyang/miniconda3/envs/fedgraph-env-py310/lib/python3.10/site-packages/ray/dashboard/modules/job/sdk.py", line 355, in get_job_info
    return JobDetails(**r.json())
TypeError: 'NoneType' object is not callable
