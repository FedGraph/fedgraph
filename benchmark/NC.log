2025-07-29 09:19:08,792	INFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_7b993ab290439a98.zip.
2025-07-29 09:19:08,793	INFO packaging.py:575 -- Creating a file package for local module '.'.
Job submission server address: http://localhost:8265

-------------------------------------------------------
Job 'raysubmit_QXevCUFTcSACnJti' submitted successfully
-------------------------------------------------------

Next steps
  Query the logs of the job:
    ray job logs raysubmit_QXevCUFTcSACnJti
  Query the status of the job:
    ray job status raysubmit_QXevCUFTcSACnJti
  Request the job to be stopped:
    ray job stop raysubmit_QXevCUFTcSACnJti

Tailing logs until the job exits (disable with --no-wait):
INFO:matplotlib.font_manager:generated new fontManager

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: cora, Trainers: 10, Distribution: average, IID Beta: 10000.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'cora', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 10000.0, 'distribution_type': 'average', 'gpu': False}
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x to ./data/cora/raw/ind.cora.x...
Downloaded ./data/cora/raw/ind.cora.x
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx to ./data/cora/raw/ind.cora.tx...
Downloaded ./data/cora/raw/ind.cora.tx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx to ./data/cora/raw/ind.cora.allx...
Downloaded ./data/cora/raw/ind.cora.allx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y to ./data/cora/raw/ind.cora.y...
Downloaded ./data/cora/raw/ind.cora.y
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty to ./data/cora/raw/ind.cora.ty...
Downloaded ./data/cora/raw/ind.cora.ty
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally to ./data/cora/raw/ind.cora.ally...
Downloaded ./data/cora/raw/ind.cora.ally
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph to ./data/cora/raw/ind.cora.graph...
Downloaded ./data/cora/raw/ind.cora.graph
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index to ./data/cora/raw/ind.cora.test.index...
Downloaded ./data/cora/raw/ind.cora.test.index
Initialization start: network data collected.
2025-07-29 16:19:29,190	INFO worker.py:1429 -- Using address 192.168.59.106:6379 set in the environment variable RAY_ADDRESS
2025-07-29 16:19:29,190	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.59.106:6379...
2025-07-29 16:19:29,199	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.59.106:8265 [39m[22m
Changing method to FedAvg
[36m(pid=1668, ip=192.168.28.30)[0m INFO:matplotlib.font_manager:generated new fontManager
[36m(Trainer pid=1668, ip=192.168.28.30)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=1668, ip=192.168.28.30)[0m   return torch.load(io.BytesIO(b))
/usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
//Log init_time: 11132.465 ms //end
//Log Large1 init network: 412258.0 //end
//Log Large2 init network: 374747.0 //end
//Log Large3 init network: 272770.0 //end
//Log Large4 init network: 269561.0 //end
//Log Server init network: 219706793.0 //end
//Log Initialization Communication Cost (MB): 210.80 //end
Pretrain start time recorded.
//pretrain_time: 6.737 ms//end
//Log Max memory for Large1: 1613520896.0 //end
//Log Max memory for Large2: 1189023744.0 //end
//Log Max memory for Large3: 1612574720.0 //end
//Log Max memory for Large4: 1187659776.0 //end
//Log Max memory for Server: 1849184256.0 //end
//Log Large1 network: 652704.0 //end
//Log Large2 network: 556681.0 //end
//Log Large3 network: 697474.0 //end
//Log Large4 network: 578429.0 //end
//Log Server network: 1816148.0 //end
//Log Total Actual Pretrain Comm Cost: 4.10 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1590
Round 2: Global Test Accuracy = 0.1600
Round 3: Global Test Accuracy = 0.1590
Round 4: Global Test Accuracy = 0.1610
Round 5: Global Test Accuracy = 0.1630
Round 6: Global Test Accuracy = 0.1650
Round 7: Global Test Accuracy = 0.1590
Round 8: Global Test Accuracy = 0.1650
Round 9: Global Test Accuracy = 0.1700
Round 10: Global Test Accuracy = 0.1710
Round 11: Global Test Accuracy = 0.1760
Round 12: Global Test Accuracy = 0.1840
Round 13: Global Test Accuracy = 0.1850
Round 14: Global Test Accuracy = 0.1890
Round 15: Global Test Accuracy = 0.1880
Round 16: Global Test Accuracy = 0.1860
Round 17: Global Test Accuracy = 0.1930
Round 18: Global Test Accuracy = 0.2000
Round 19: Global Test Accuracy = 0.2010
Round 20: Global Test Accuracy = 0.2060
Round 21: Global Test Accuracy = 0.2100
Round 22: Global Test Accuracy = 0.2150
Round 23: Global Test Accuracy = 0.2210
Round 24: Global Test Accuracy = 0.2310
Round 25: Global Test Accuracy = 0.2340
Round 26: Global Test Accuracy = 0.2330
Round 27: Global Test Accuracy = 0.2380
Round 28: Global Test Accuracy = 0.2430
Round 29: Global Test Accuracy = 0.2520
Round 30: Global Test Accuracy = 0.2510
Round 31: Global Test Accuracy = 0.2600
Round 32: Global Test Accuracy = 0.2620
Round 33: Global Test Accuracy = 0.2720
Round 34: Global Test Accuracy = 0.2760
Round 35: Global Test Accuracy = 0.2790
Round 36: Global Test Accuracy = 0.2840
Round 37: Global Test Accuracy = 0.2890
Round 38: Global Test Accuracy = 0.2900
Round 39: Global Test Accuracy = 0.2880
Round 40: Global Test Accuracy = 0.2890
Round 41: Global Test Accuracy = 0.2910
Round 42: Global Test Accuracy = 0.2950
Round 43: Global Test Accuracy = 0.3000
Round 44: Global Test Accuracy = 0.3090
Round 45: Global Test Accuracy = 0.3060
Round 46: Global Test Accuracy = 0.3180
Round 47: Global Test Accuracy = 0.3270
Round 48: Global Test Accuracy = 0.3250
Round 49: Global Test Accuracy = 0.3310
Round 50: Global Test Accuracy = 0.3390
Round 51: Global Test Accuracy = 0.3440
Round 52: Global Test Accuracy = 0.3530
Round 53: Global Test Accuracy = 0.3570
Round 54: Global Test Accuracy = 0.3620
Round 55: Global Test Accuracy = 0.3750
Round 56: Global Test Accuracy = 0.3800
Round 57: Global Test Accuracy = 0.3830
Round 58: Global Test Accuracy = 0.3820
Round 59: Global Test Accuracy = 0.3880
Round 60: Global Test Accuracy = 0.3860
Round 61: Global Test Accuracy = 0.3930
Round 62: Global Test Accuracy = 0.3910
Round 63: Global Test Accuracy = 0.3940
Round 64: Global Test Accuracy = 0.4050
Round 65: Global Test Accuracy = 0.4070
Round 66: Global Test Accuracy = 0.4110
Round 67: Global Test Accuracy = 0.4170
Round 68: Global Test Accuracy = 0.4240
Round 69: Global Test Accuracy = 0.4280
Round 70: Global Test Accuracy = 0.4320
Round 71: Global Test Accuracy = 0.4390
Round 72: Global Test Accuracy = 0.4380
Round 73: Global Test Accuracy = 0.4420
Round 74: Global Test Accuracy = 0.4460
Round 75: Global Test Accuracy = 0.4550
Round 76: Global Test Accuracy = 0.4610
Round 77: Global Test Accuracy = 0.4600
Round 78: Global Test Accuracy = 0.4660
Round 79: Global Test Accuracy = 0.4690
Round 80: Global Test Accuracy = 0.4770
Round 81: Global Test Accuracy = 0.4780
Round 82: Global Test Accuracy = 0.4790
Round 83: Global Test Accuracy = 0.4820
Round 84: Global Test Accuracy = 0.4840
Round 85: Global Test Accuracy = 0.4870
Round 86: Global Test Accuracy = 0.4960
Round 87: Global Test Accuracy = 0.4970
Round 88: Global Test Accuracy = 0.4980
Round 89: Global Test Accuracy = 0.5080
Round 90: Global Test Accuracy = 0.5070
Round 91: Global Test Accuracy = 0.5080
Round 92: Global Test Accuracy = 0.5140
Round 93: Global Test Accuracy = 0.5120
Round 94: Global Test Accuracy = 0.5150
Round 95: Global Test Accuracy = 0.5180
Round 96: Global Test Accuracy = 0.5230
Round 97: Global Test Accuracy = 0.5200
Round 98: Global Test Accuracy = 0.5230
Round 99: Global Test Accuracy = 0.5350
Round 100: Global Test Accuracy = 0.5400
Round 101: Global Test Accuracy = 0.5400
Round 102: Global Test Accuracy = 0.5420
Round 103: Global Test Accuracy = 0.5420
Round 104: Global Test Accuracy = 0.5440
Round 105: Global Test Accuracy = 0.5450
Round 106: Global Test Accuracy = 0.5470
Round 107: Global Test Accuracy = 0.5490
Round 108: Global Test Accuracy = 0.5470
Round 109: Global Test Accuracy = 0.5450
Round 110: Global Test Accuracy = 0.5490
Round 111: Global Test Accuracy = 0.5480
Round 112: Global Test Accuracy = 0.5500
Round 113: Global Test Accuracy = 0.5490
Round 114: Global Test Accuracy = 0.5500
Round 115: Global Test Accuracy = 0.5550
Round 116: Global Test Accuracy = 0.5570
Round 117: Global Test Accuracy = 0.5630
Round 118: Global Test Accuracy = 0.5630
Round 119: Global Test Accuracy = 0.5610
Round 120: Global Test Accuracy = 0.5610
Round 121: Global Test Accuracy = 0.5680
Round 122: Global Test Accuracy = 0.5660
Round 123: Global Test Accuracy = 0.5710
Round 124: Global Test Accuracy = 0.5730
Round 125: Global Test Accuracy = 0.5710
Round 126: Global Test Accuracy = 0.5720
Round 127: Global Test Accuracy = 0.5760
Round 128: Global Test Accuracy = 0.5780
Round 129: Global Test Accuracy = 0.5800
Round 130: Global Test Accuracy = 0.5790
Round 131: Global Test Accuracy = 0.5780
Round 132: Global Test Accuracy = 0.5820
Round 133: Global Test Accuracy = 0.5810
Round 134: Global Test Accuracy = 0.5780
Round 135: Global Test Accuracy = 0.5810
Round 136: Global Test Accuracy = 0.5810
Round 137: Global Test Accuracy = 0.5790
Round 138: Global Test Accuracy = 0.5820
Round 139: Global Test Accuracy = 0.5830
Round 140: Global Test Accuracy = 0.5850
Round 141: Global Test Accuracy = 0.5830
Round 142: Global Test Accuracy = 0.5880
Round 143: Global Test Accuracy = 0.5870
Round 144: Global Test Accuracy = 0.5860
Round 145: Global Test Accuracy = 0.5900
Round 146: Global Test Accuracy = 0.5890
Round 147: Global Test Accuracy = 0.5920
Round 148: Global Test Accuracy = 0.5890
Round 149: Global Test Accuracy = 0.5910
Round 150: Global Test Accuracy = 0.5930
Round 151: Global Test Accuracy = 0.5970
Round 152: Global Test Accuracy = 0.5970
Round 153: Global Test Accuracy = 0.5980
Round 154: Global Test Accuracy = 0.6000
Round 155: Global Test Accuracy = 0.6010
Round 156: Global Test Accuracy = 0.6000
Round 157: Global Test Accuracy = 0.6020
Round 158: Global Test Accuracy = 0.6010
Round 159: Global Test Accuracy = 0.6000
Round 160: Global Test Accuracy = 0.6030
Round 161: Global Test Accuracy = 0.6040
Round 162: Global Test Accuracy = 0.6060
Round 163: Global Test Accuracy = 0.6040
Round 164: Global Test Accuracy = 0.6030
Round 165: Global Test Accuracy = 0.6050
Round 166: Global Test Accuracy = 0.6070
Round 167: Global Test Accuracy = 0.6080
Round 168: Global Test Accuracy = 0.6030
Round 169: Global Test Accuracy = 0.6050
Round 170: Global Test Accuracy = 0.6090
Round 171: Global Test Accuracy = 0.6100
Round 172: Global Test Accuracy = 0.6100
Round 173: Global Test Accuracy = 0.6100
Round 174: Global Test Accuracy = 0.6050
Round 175: Global Test Accuracy = 0.6080
Round 176: Global Test Accuracy = 0.6030
Round 177: Global Test Accuracy = 0.6080
Round 178: Global Test Accuracy = 0.6110
Round 179: Global Test Accuracy = 0.6100
Round 180: Global Test Accuracy = 0.6090
Round 181: Global Test Accuracy = 0.6100
Round 182: Global Test Accuracy = 0.6050
Round 183: Global Test Accuracy = 0.6100
Round 184: Global Test Accuracy = 0.6100
Round 185: Global Test Accuracy = 0.6090
Round 186: Global Test Accuracy = 0.6120
Round 187: Global Test Accuracy = 0.6130
Round 188: Global Test Accuracy = 0.6120
Round 189: Global Test Accuracy = 0.6120
Round 190: Global Test Accuracy = 0.6130
Round 191: Global Test Accuracy = 0.6140
Round 192: Global Test Accuracy = 0.6140
Round 193: Global Test Accuracy = 0.6160
Round 194: Global Test Accuracy = 0.6150
Round 195: Global Test Accuracy = 0.6150
Round 196: Global Test Accuracy = 0.6140
Round 197: Global Test Accuracy = 0.6160
Round 198: Global Test Accuracy = 0.6140
Round 199: Global Test Accuracy = 0.6140
Round 200: Global Test Accuracy = 0.6150
//train_time: 4716.523999999999 ms//end
//Log Max memory for Large1: 1645342720.0 //end
//Log Max memory for Large2: 1208832000.0 //end
//Log Max memory for Large3: 1641218048.0 //end
//Log Max memory for Large4: 1210654720.0 //end
//Log Max memory for Server: 1979133952.0 //end
//Log Large1 network: 58489110.0 //end
//Log Large2 network: 39142144.0 //end
//Log Large3 network: 58444988.0 //end
//Log Large4 network: 39116204.0 //end
//Log Server network: 195076233.0 //end
//Log Total Actual Train Comm Cost: 372.19 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: cora, Batch Size: -1, Trainers: 10, Hops: 0, IID Beta: 10000.0 => Training Time = 34.72 seconds
average_final_test_loss, 1.271689626097679
Average test accuracy, 0.615

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge
----------------------------------------------------------------------------------------------------
0        663.8        267      92       2.486        7.215
1        660.7        270      106      2.447        6.233
2        663.4        270      156      2.457        4.253
3        662.3        269      100      2.462        6.623
4        663.0        273      90       2.428        7.366
5        662.2        270      118      2.453        5.612
6        662.4        272      134      2.435        4.943
7        660.0        270      108      2.444        6.111
8        661.2        272      90       2.431        7.347
9        662.6        275      102      2.410        6.497
====================================================================================================
Total Memory Usage: 6621.5 MB (6.47 GB)
Total Nodes: 2708, Total Edges: 1096
Average Memory per Trainer: 662.2 MB
Average Nodes per Trainer: 270.8
Average Edges per Trainer: 109.6
Max Memory: 663.8 MB (Trainer 0)
Min Memory: 660.0 MB (Trainer 7)
Overall Memory/Node Ratio: 2.445 MB/node
Overall Memory/Edge Ratio: 6.042 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 351.91 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
cora,10000.0,-1,75.8,0.61,34.7,351.9,663.8,0.174,0.088,0
================================================================================
[36m(pid=1705, ip=192.168.54.57)[0m INFO:matplotlib.font_manager:generated new fontManager[32m [repeated 9x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(Trainer pid=1705, ip=192.168.54.57)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=1705, ip=192.168.54.57)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: cora, Trainers: 10, Distribution: average, IID Beta: 100.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'cora', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 100.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/cora/raw/ind.cora.x
File already exists: ./data/cora/raw/ind.cora.tx
File already exists: ./data/cora/raw/ind.cora.allx
File already exists: ./data/cora/raw/ind.cora.y
File already exists: ./data/cora/raw/ind.cora.ty
File already exists: ./data/cora/raw/ind.cora.ally
File already exists: ./data/cora/raw/ind.cora.graph
File already exists: ./data/cora/raw/ind.cora.test.index
Initialization start: network data collected.
2025-07-29 16:20:50,477	INFO worker.py:1429 -- Using address 192.168.59.106:6379 set in the environment variable RAY_ADDRESS
2025-07-29 16:20:50,477	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.59.106:6379...
2025-07-29 16:20:50,484	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.59.106:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=2234, ip=192.168.28.30)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=2234, ip=192.168.28.30)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5393.783 ms //end
//Log Large1 init network: 99041.0 //end
//Log Large2 init network: 159564.0 //end
//Log Large3 init network: 138771.0 //end
//Log Large4 init network: 123175.0 //end
//Log Server init network: 16259243.0 //end
//Log Initialization Communication Cost (MB): 16.00 //end
Pretrain start time recorded.
//pretrain_time: 6.84 ms//end
//Log Max memory for Large1: 1195778048.0 //end
//Log Max memory for Large2: 1617477632.0 //end
//Log Max memory for Large3: 1196802048.0 //end
//Log Max memory for Large4: 1615110144.0 //end
//Log Max memory for Server: 2024628224.0 //end
//Log Large1 network: 616868.0 //end
//Log Large2 network: 685957.0 //end
//Log Large3 network: 622803.0 //end
//Log Large4 network: 725037.0 //end
//Log Server network: 1798655.0 //end
//Log Total Actual Pretrain Comm Cost: 4.24 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1520
Round 2: Global Test Accuracy = 0.1590
Round 3: Global Test Accuracy = 0.1660
Round 4: Global Test Accuracy = 0.1680
Round 5: Global Test Accuracy = 0.1660
Round 6: Global Test Accuracy = 0.1680
Round 7: Global Test Accuracy = 0.1760
Round 8: Global Test Accuracy = 0.1820
Round 9: Global Test Accuracy = 0.1880
Round 10: Global Test Accuracy = 0.1910
Round 11: Global Test Accuracy = 0.1970
Round 12: Global Test Accuracy = 0.2030
Round 13: Global Test Accuracy = 0.2120
Round 14: Global Test Accuracy = 0.2100
Round 15: Global Test Accuracy = 0.2110
Round 16: Global Test Accuracy = 0.2170
Round 17: Global Test Accuracy = 0.2250
Round 18: Global Test Accuracy = 0.2270
Round 19: Global Test Accuracy = 0.2310
Round 20: Global Test Accuracy = 0.2320
Round 21: Global Test Accuracy = 0.2390
Round 22: Global Test Accuracy = 0.2440
Round 23: Global Test Accuracy = 0.2510
Round 24: Global Test Accuracy = 0.2660
Round 25: Global Test Accuracy = 0.2780
Round 26: Global Test Accuracy = 0.2810
Round 27: Global Test Accuracy = 0.2790
Round 28: Global Test Accuracy = 0.2940
Round 29: Global Test Accuracy = 0.2970
Round 30: Global Test Accuracy = 0.3070
Round 31: Global Test Accuracy = 0.3140
Round 32: Global Test Accuracy = 0.3250
Round 33: Global Test Accuracy = 0.3300
Round 34: Global Test Accuracy = 0.3360
Round 35: Global Test Accuracy = 0.3420
Round 36: Global Test Accuracy = 0.3470
Round 37: Global Test Accuracy = 0.3580
Round 38: Global Test Accuracy = 0.3630
Round 39: Global Test Accuracy = 0.3700
Round 40: Global Test Accuracy = 0.3680
Round 41: Global Test Accuracy = 0.3710
Round 42: Global Test Accuracy = 0.3750
Round 43: Global Test Accuracy = 0.3780
Round 44: Global Test Accuracy = 0.3800
Round 45: Global Test Accuracy = 0.3860
Round 46: Global Test Accuracy = 0.3870
Round 47: Global Test Accuracy = 0.3960
Round 48: Global Test Accuracy = 0.4050
Round 49: Global Test Accuracy = 0.4070
Round 50: Global Test Accuracy = 0.4110
Round 51: Global Test Accuracy = 0.4210
Round 52: Global Test Accuracy = 0.4240
Round 53: Global Test Accuracy = 0.4280
Round 54: Global Test Accuracy = 0.4310
Round 55: Global Test Accuracy = 0.4370
Round 56: Global Test Accuracy = 0.4420
Round 57: Global Test Accuracy = 0.4470
Round 58: Global Test Accuracy = 0.4520
Round 59: Global Test Accuracy = 0.4540
Round 60: Global Test Accuracy = 0.4600
Round 61: Global Test Accuracy = 0.4630
Round 62: Global Test Accuracy = 0.4670
Round 63: Global Test Accuracy = 0.4710
Round 64: Global Test Accuracy = 0.4700
Round 65: Global Test Accuracy = 0.4710
Round 66: Global Test Accuracy = 0.4790
Round 67: Global Test Accuracy = 0.4780
Round 68: Global Test Accuracy = 0.4800
Round 69: Global Test Accuracy = 0.4800
Round 70: Global Test Accuracy = 0.4780
Round 71: Global Test Accuracy = 0.4800
Round 72: Global Test Accuracy = 0.4790
Round 73: Global Test Accuracy = 0.4800
Round 74: Global Test Accuracy = 0.4830
Round 75: Global Test Accuracy = 0.4840
Round 76: Global Test Accuracy = 0.4840
Round 77: Global Test Accuracy = 0.4860
Round 78: Global Test Accuracy = 0.4860
Round 79: Global Test Accuracy = 0.4940
Round 80: Global Test Accuracy = 0.4950
Round 81: Global Test Accuracy = 0.4990
Round 82: Global Test Accuracy = 0.4980
Round 83: Global Test Accuracy = 0.5080
Round 84: Global Test Accuracy = 0.5090
Round 85: Global Test Accuracy = 0.5110
Round 86: Global Test Accuracy = 0.5140
Round 87: Global Test Accuracy = 0.5150
Round 88: Global Test Accuracy = 0.5160
Round 89: Global Test Accuracy = 0.5150
Round 90: Global Test Accuracy = 0.5190
Round 91: Global Test Accuracy = 0.5190
Round 92: Global Test Accuracy = 0.5260
Round 93: Global Test Accuracy = 0.5280
Round 94: Global Test Accuracy = 0.5280
Round 95: Global Test Accuracy = 0.5320
Round 96: Global Test Accuracy = 0.5370
Round 97: Global Test Accuracy = 0.5350
Round 98: Global Test Accuracy = 0.5420
Round 99: Global Test Accuracy = 0.5480
Round 100: Global Test Accuracy = 0.5520
Round 101: Global Test Accuracy = 0.5540
Round 102: Global Test Accuracy = 0.5570
Round 103: Global Test Accuracy = 0.5580
Round 104: Global Test Accuracy = 0.5580
Round 105: Global Test Accuracy = 0.5610
Round 106: Global Test Accuracy = 0.5610
Round 107: Global Test Accuracy = 0.5610
Round 108: Global Test Accuracy = 0.5600
Round 109: Global Test Accuracy = 0.5610
Round 110: Global Test Accuracy = 0.5610
Round 111: Global Test Accuracy = 0.5600
Round 112: Global Test Accuracy = 0.5640
Round 113: Global Test Accuracy = 0.5650
Round 114: Global Test Accuracy = 0.5660
Round 115: Global Test Accuracy = 0.5670
Round 116: Global Test Accuracy = 0.5660
Round 117: Global Test Accuracy = 0.5660
Round 118: Global Test Accuracy = 0.5680
Round 119: Global Test Accuracy = 0.5770
Round 120: Global Test Accuracy = 0.5730
Round 121: Global Test Accuracy = 0.5750
Round 122: Global Test Accuracy = 0.5780
Round 123: Global Test Accuracy = 0.5800
Round 124: Global Test Accuracy = 0.5820
Round 125: Global Test Accuracy = 0.5850
Round 126: Global Test Accuracy = 0.5850
Round 127: Global Test Accuracy = 0.5840
Round 128: Global Test Accuracy = 0.5860
Round 129: Global Test Accuracy = 0.5890
Round 130: Global Test Accuracy = 0.5880
Round 131: Global Test Accuracy = 0.5900
Round 132: Global Test Accuracy = 0.5910
Round 133: Global Test Accuracy = 0.5890
Round 134: Global Test Accuracy = 0.5900
Round 135: Global Test Accuracy = 0.5940
Round 136: Global Test Accuracy = 0.5920
Round 137: Global Test Accuracy = 0.5950
Round 138: Global Test Accuracy = 0.5950
Round 139: Global Test Accuracy = 0.5970
Round 140: Global Test Accuracy = 0.5970
Round 141: Global Test Accuracy = 0.5950
Round 142: Global Test Accuracy = 0.5990
Round 143: Global Test Accuracy = 0.5970
Round 144: Global Test Accuracy = 0.5980
Round 145: Global Test Accuracy = 0.5980
Round 146: Global Test Accuracy = 0.5980
Round 147: Global Test Accuracy = 0.5990
Round 148: Global Test Accuracy = 0.5970
Round 149: Global Test Accuracy = 0.5950
Round 150: Global Test Accuracy = 0.5950
Round 151: Global Test Accuracy = 0.5950
Round 152: Global Test Accuracy = 0.5960
Round 153: Global Test Accuracy = 0.5960
Round 154: Global Test Accuracy = 0.5980
Round 155: Global Test Accuracy = 0.6010
Round 156: Global Test Accuracy = 0.6000
Round 157: Global Test Accuracy = 0.6010
Round 158: Global Test Accuracy = 0.5990
Round 159: Global Test Accuracy = 0.6000
Round 160: Global Test Accuracy = 0.6000
Round 161: Global Test Accuracy = 0.5990
Round 162: Global Test Accuracy = 0.6010
Round 163: Global Test Accuracy = 0.5990
Round 164: Global Test Accuracy = 0.6000
Round 165: Global Test Accuracy = 0.6000
Round 166: Global Test Accuracy = 0.6010
Round 167: Global Test Accuracy = 0.6030
Round 168: Global Test Accuracy = 0.6020
Round 169: Global Test Accuracy = 0.6040
Round 170: Global Test Accuracy = 0.6050
Round 171: Global Test Accuracy = 0.6050
Round 172: Global Test Accuracy = 0.6070
Round 173: Global Test Accuracy = 0.6070
Round 174: Global Test Accuracy = 0.6070
Round 175: Global Test Accuracy = 0.6110
Round 176: Global Test Accuracy = 0.6120
Round 177: Global Test Accuracy = 0.6150
Round 178: Global Test Accuracy = 0.6110
Round 179: Global Test Accuracy = 0.6160
Round 180: Global Test Accuracy = 0.6140
Round 181: Global Test Accuracy = 0.6170
Round 182: Global Test Accuracy = 0.6160
Round 183: Global Test Accuracy = 0.6160
Round 184: Global Test Accuracy = 0.6160
Round 185: Global Test Accuracy = 0.6160
Round 186: Global Test Accuracy = 0.6150
Round 187: Global Test Accuracy = 0.6160
Round 188: Global Test Accuracy = 0.6160
Round 189: Global Test Accuracy = 0.6160
Round 190: Global Test Accuracy = 0.6160
Round 191: Global Test Accuracy = 0.6170
Round 192: Global Test Accuracy = 0.6160
Round 193: Global Test Accuracy = 0.6170
Round 194: Global Test Accuracy = 0.6160
Round 195: Global Test Accuracy = 0.6180
Round 196: Global Test Accuracy = 0.6190
Round 197: Global Test Accuracy = 0.6190
Round 198: Global Test Accuracy = 0.6190
Round 199: Global Test Accuracy = 0.6180
Round 200: Global Test Accuracy = 0.6190
//train_time: 4506.111 ms//end
//Log Max memory for Large1: 1219895296.0 //end
//Log Max memory for Large2: 1652723712.0 //end
//Log Max memory for Large3: 1220718592.0 //end
//Log Max memory for Large4: 1652252672.0 //end
//Log Max memory for Server: 2067525632.0 //end
//Log Large1 network: 39169199.0 //end
//Log Large2 network: 58510994.0 //end
//Log Large3 network: 39122972.0 //end
//Log Large4 network: 58457054.0 //end
//Log Server network: 195072996.0 //end
//Log Total Actual Train Comm Cost: 372.25 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: cora, Batch Size: -1, Trainers: 10, Hops: 0, IID Beta: 100.0 => Training Time = 34.51 seconds
average_final_test_loss, 1.2449069901704788
Average test accuracy, 0.619

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge
----------------------------------------------------------------------------------------------------
0        661.8        258      126      2.565        5.252
1        661.4        272      116      2.432        5.701
2        661.6        279      96       2.371        6.891
3        661.3        267      102      2.477        6.483
4        662.8        273      106      2.428        6.253
5        661.8        257      98       2.575        6.753
6        663.1        279      130      2.377        5.101
7        661.1        269      100      2.458        6.611
8        663.0        279      102      2.376        6.500
9        664.2        275      132      2.415        5.032
====================================================================================================
Total Memory Usage: 6622.0 MB (6.47 GB)
Total Nodes: 2708, Total Edges: 1108
Average Memory per Trainer: 662.2 MB
Average Nodes per Trainer: 270.8
Average Edges per Trainer: 110.8
Max Memory: 664.2 MB (Trainer 9)
Min Memory: 661.1 MB (Trainer 7)
Overall Memory/Node Ratio: 2.445 MB/node
Overall Memory/Edge Ratio: 5.977 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 351.91 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
cora,100.0,-1,69.9,0.62,34.5,351.9,664.2,0.173,0.088,0
================================================================================
[36m(Trainer pid=2192, ip=192.168.54.57)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=2192, ip=192.168.54.57)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: cora, Trainers: 10, Distribution: average, IID Beta: 10.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'cora', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/cora/raw/ind.cora.x
File already exists: ./data/cora/raw/ind.cora.tx
File already exists: ./data/cora/raw/ind.cora.allx
File already exists: ./data/cora/raw/ind.cora.y
File already exists: ./data/cora/raw/ind.cora.ty
File already exists: ./data/cora/raw/ind.cora.ally
File already exists: ./data/cora/raw/ind.cora.graph
File already exists: ./data/cora/raw/ind.cora.test.index
Initialization start: network data collected.
2025-07-29 16:22:05,906	INFO worker.py:1429 -- Using address 192.168.59.106:6379 set in the environment variable RAY_ADDRESS
2025-07-29 16:22:05,906	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.59.106:6379...
2025-07-29 16:22:05,913	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.59.106:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=2703, ip=192.168.28.30)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=2703, ip=192.168.28.30)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5302.328 ms //end
//Log Large1 init network: 123613.0 //end
//Log Large2 init network: 107761.0 //end
//Log Large3 init network: 113755.0 //end
//Log Large4 init network: 142348.0 //end
//Log Server init network: 16261034.0 //end
//Log Initialization Communication Cost (MB): 15.97 //end
Pretrain start time recorded.
//pretrain_time: 6.926 ms//end
//Log Max memory for Large1: 1621127168.0 //end
//Log Max memory for Large2: 1201700864.0 //end
//Log Max memory for Large3: 1623597056.0 //end
//Log Max memory for Large4: 1204641792.0 //end
//Log Max memory for Server: 2063904768.0 //end
//Log Large1 network: 764056.0 //end
//Log Large2 network: 641722.0 //end
//Log Large3 network: 774566.0 //end
//Log Large4 network: 626074.0 //end
//Log Server network: 1755689.0 //end
//Log Total Actual Pretrain Comm Cost: 4.35 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1530
Round 2: Global Test Accuracy = 0.1590
Round 3: Global Test Accuracy = 0.1570
Round 4: Global Test Accuracy = 0.1540
Round 5: Global Test Accuracy = 0.1570
Round 6: Global Test Accuracy = 0.1570
Round 7: Global Test Accuracy = 0.1550
Round 8: Global Test Accuracy = 0.1580
Round 9: Global Test Accuracy = 0.1620
Round 10: Global Test Accuracy = 0.1670
Round 11: Global Test Accuracy = 0.1750
Round 12: Global Test Accuracy = 0.1770
Round 13: Global Test Accuracy = 0.1760
Round 14: Global Test Accuracy = 0.1790
Round 15: Global Test Accuracy = 0.1810
Round 16: Global Test Accuracy = 0.1840
Round 17: Global Test Accuracy = 0.1900
Round 18: Global Test Accuracy = 0.1970
Round 19: Global Test Accuracy = 0.2020
Round 20: Global Test Accuracy = 0.2100
Round 21: Global Test Accuracy = 0.2130
Round 22: Global Test Accuracy = 0.2190
Round 23: Global Test Accuracy = 0.2300
Round 24: Global Test Accuracy = 0.2340
Round 25: Global Test Accuracy = 0.2360
Round 26: Global Test Accuracy = 0.2460
Round 27: Global Test Accuracy = 0.2460
Round 28: Global Test Accuracy = 0.2530
Round 29: Global Test Accuracy = 0.2610
Round 30: Global Test Accuracy = 0.2650
Round 31: Global Test Accuracy = 0.2630
Round 32: Global Test Accuracy = 0.2640
Round 33: Global Test Accuracy = 0.2750
Round 34: Global Test Accuracy = 0.2750
Round 35: Global Test Accuracy = 0.2780
Round 36: Global Test Accuracy = 0.2860
Round 37: Global Test Accuracy = 0.2890
Round 38: Global Test Accuracy = 0.2980
Round 39: Global Test Accuracy = 0.2970
Round 40: Global Test Accuracy = 0.3020
Round 41: Global Test Accuracy = 0.3080
Round 42: Global Test Accuracy = 0.3060
Round 43: Global Test Accuracy = 0.3080
Round 44: Global Test Accuracy = 0.3160
Round 45: Global Test Accuracy = 0.3240
Round 46: Global Test Accuracy = 0.3260
Round 47: Global Test Accuracy = 0.3250
Round 48: Global Test Accuracy = 0.3300
Round 49: Global Test Accuracy = 0.3310
Round 50: Global Test Accuracy = 0.3330
Round 51: Global Test Accuracy = 0.3380
Round 52: Global Test Accuracy = 0.3400
Round 53: Global Test Accuracy = 0.3430
Round 54: Global Test Accuracy = 0.3470
Round 55: Global Test Accuracy = 0.3560
Round 56: Global Test Accuracy = 0.3550
Round 57: Global Test Accuracy = 0.3580
Round 58: Global Test Accuracy = 0.3580
Round 59: Global Test Accuracy = 0.3670
Round 60: Global Test Accuracy = 0.3690
Round 61: Global Test Accuracy = 0.3720
Round 62: Global Test Accuracy = 0.3720
Round 63: Global Test Accuracy = 0.3780
Round 64: Global Test Accuracy = 0.3810
Round 65: Global Test Accuracy = 0.3790
Round 66: Global Test Accuracy = 0.3820
Round 67: Global Test Accuracy = 0.3830
Round 68: Global Test Accuracy = 0.3840
Round 69: Global Test Accuracy = 0.3860
Round 70: Global Test Accuracy = 0.3850
Round 71: Global Test Accuracy = 0.3900
Round 72: Global Test Accuracy = 0.3980
Round 73: Global Test Accuracy = 0.3950
Round 74: Global Test Accuracy = 0.4020
Round 75: Global Test Accuracy = 0.3990
Round 76: Global Test Accuracy = 0.4060
Round 77: Global Test Accuracy = 0.4110
Round 78: Global Test Accuracy = 0.4120
Round 79: Global Test Accuracy = 0.4120
Round 80: Global Test Accuracy = 0.4180
Round 81: Global Test Accuracy = 0.4200
Round 82: Global Test Accuracy = 0.4300
Round 83: Global Test Accuracy = 0.4340
Round 84: Global Test Accuracy = 0.4400
Round 85: Global Test Accuracy = 0.4400
Round 86: Global Test Accuracy = 0.4450
Round 87: Global Test Accuracy = 0.4450
Round 88: Global Test Accuracy = 0.4460
Round 89: Global Test Accuracy = 0.4480
Round 90: Global Test Accuracy = 0.4550
Round 91: Global Test Accuracy = 0.4620
Round 92: Global Test Accuracy = 0.4600
Round 93: Global Test Accuracy = 0.4620
Round 94: Global Test Accuracy = 0.4680
Round 95: Global Test Accuracy = 0.4690
Round 96: Global Test Accuracy = 0.4720
Round 97: Global Test Accuracy = 0.4770
Round 98: Global Test Accuracy = 0.4770
Round 99: Global Test Accuracy = 0.4810
Round 100: Global Test Accuracy = 0.4790
Round 101: Global Test Accuracy = 0.4760
Round 102: Global Test Accuracy = 0.4810
Round 103: Global Test Accuracy = 0.4860
Round 104: Global Test Accuracy = 0.4860
Round 105: Global Test Accuracy = 0.4910
Round 106: Global Test Accuracy = 0.4900
Round 107: Global Test Accuracy = 0.4940
Round 108: Global Test Accuracy = 0.4960
Round 109: Global Test Accuracy = 0.4990
Round 110: Global Test Accuracy = 0.4990
Round 111: Global Test Accuracy = 0.5080
Round 112: Global Test Accuracy = 0.5110
Round 113: Global Test Accuracy = 0.5110
Round 114: Global Test Accuracy = 0.5130
Round 115: Global Test Accuracy = 0.5190
Round 116: Global Test Accuracy = 0.5240
Round 117: Global Test Accuracy = 0.5230
Round 118: Global Test Accuracy = 0.5230
Round 119: Global Test Accuracy = 0.5250
Round 120: Global Test Accuracy = 0.5300
Round 121: Global Test Accuracy = 0.5280
Round 122: Global Test Accuracy = 0.5320
Round 123: Global Test Accuracy = 0.5310
Round 124: Global Test Accuracy = 0.5320
Round 125: Global Test Accuracy = 0.5410
Round 126: Global Test Accuracy = 0.5390
Round 127: Global Test Accuracy = 0.5370
Round 128: Global Test Accuracy = 0.5430
Round 129: Global Test Accuracy = 0.5430
Round 130: Global Test Accuracy = 0.5420
Round 131: Global Test Accuracy = 0.5450
Round 132: Global Test Accuracy = 0.5460
Round 133: Global Test Accuracy = 0.5460
Round 134: Global Test Accuracy = 0.5460
Round 135: Global Test Accuracy = 0.5480
Round 136: Global Test Accuracy = 0.5460
Round 137: Global Test Accuracy = 0.5490
Round 138: Global Test Accuracy = 0.5490
Round 139: Global Test Accuracy = 0.5480
Round 140: Global Test Accuracy = 0.5510
Round 141: Global Test Accuracy = 0.5490
Round 142: Global Test Accuracy = 0.5520
Round 143: Global Test Accuracy = 0.5500
Round 144: Global Test Accuracy = 0.5540
Round 145: Global Test Accuracy = 0.5580
Round 146: Global Test Accuracy = 0.5560
Round 147: Global Test Accuracy = 0.5550
Round 148: Global Test Accuracy = 0.5590
Round 149: Global Test Accuracy = 0.5570
Round 150: Global Test Accuracy = 0.5580
Round 151: Global Test Accuracy = 0.5600
Round 152: Global Test Accuracy = 0.5620
Round 153: Global Test Accuracy = 0.5630
Round 154: Global Test Accuracy = 0.5640
Round 155: Global Test Accuracy = 0.5630
Round 156: Global Test Accuracy = 0.5660
Round 157: Global Test Accuracy = 0.5690
Round 158: Global Test Accuracy = 0.5670
Round 159: Global Test Accuracy = 0.5710
Round 160: Global Test Accuracy = 0.5690
Round 161: Global Test Accuracy = 0.5690
Round 162: Global Test Accuracy = 0.5710
Round 163: Global Test Accuracy = 0.5710
Round 164: Global Test Accuracy = 0.5740
Round 165: Global Test Accuracy = 0.5720
Round 166: Global Test Accuracy = 0.5790
Round 167: Global Test Accuracy = 0.5800
Round 168: Global Test Accuracy = 0.5800
Round 169: Global Test Accuracy = 0.5800
Round 170: Global Test Accuracy = 0.5800
Round 171: Global Test Accuracy = 0.5790
Round 172: Global Test Accuracy = 0.5800
Round 173: Global Test Accuracy = 0.5800
Round 174: Global Test Accuracy = 0.5800
Round 175: Global Test Accuracy = 0.5800
Round 176: Global Test Accuracy = 0.5800
Round 177: Global Test Accuracy = 0.5800
Round 178: Global Test Accuracy = 0.5820
Round 179: Global Test Accuracy = 0.5790
Round 180: Global Test Accuracy = 0.5820
Round 181: Global Test Accuracy = 0.5840
Round 182: Global Test Accuracy = 0.5830
Round 183: Global Test Accuracy = 0.5840
Round 184: Global Test Accuracy = 0.5850
Round 185: Global Test Accuracy = 0.5860
Round 186: Global Test Accuracy = 0.5890
Round 187: Global Test Accuracy = 0.5890
Round 188: Global Test Accuracy = 0.5850
Round 189: Global Test Accuracy = 0.5860
Round 190: Global Test Accuracy = 0.5900
Round 191: Global Test Accuracy = 0.5890
Round 192: Global Test Accuracy = 0.5880
Round 193: Global Test Accuracy = 0.5880
Round 194: Global Test Accuracy = 0.5880
Round 195: Global Test Accuracy = 0.5930
Round 196: Global Test Accuracy = 0.5930
Round 197: Global Test Accuracy = 0.5920
Round 198: Global Test Accuracy = 0.5930
Round 199: Global Test Accuracy = 0.5940
Round 200: Global Test Accuracy = 0.5950
//train_time: 4583.0830000000005 ms//end
//Log Max memory for Large1: 1653346304.0 //end
//Log Max memory for Large2: 1224028160.0 //end
//Log Max memory for Large3: 1657888768.0 //end
//Log Max memory for Large4: 1228275712.0 //end
//Log Max memory for Server: 2101215232.0 //end
//Log Large1 network: 58462104.0 //end
//Log Large2 network: 39174556.0 //end
//Log Large3 network: 58511935.0 //end
//Log Large4 network: 39114064.0 //end
//Log Server network: 195194444.0 //end
//Log Total Actual Train Comm Cost: 372.37 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: cora, Batch Size: -1, Trainers: 10, Hops: 0, IID Beta: 10.0 => Training Time = 34.59 seconds
average_final_test_loss, 1.2877148967981338
Average test accuracy, 0.595

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge
----------------------------------------------------------------------------------------------------
0        664.2        276      112      2.407        5.931
1        661.5        267      78       2.478        8.481
2        661.3        250      98       2.645        6.748
3        661.6        242      80       2.734        8.270
4        663.7        309      154      2.148        4.310
5        662.7        274      110      2.418        6.024
6        661.9        267      122      2.479        5.426
7        662.5        283      114      2.341        5.811
8        664.1        271      126      2.450        5.270
9        663.4        269      98       2.466        6.769
====================================================================================================
Total Memory Usage: 6626.9 MB (6.47 GB)
Total Nodes: 2708, Total Edges: 1092
Average Memory per Trainer: 662.7 MB
Average Nodes per Trainer: 270.8
Average Edges per Trainer: 109.2
Max Memory: 664.2 MB (Trainer 0)
Min Memory: 661.3 MB (Trainer 2)
Overall Memory/Node Ratio: 2.447 MB/node
Overall Memory/Edge Ratio: 6.069 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 351.91 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
cora,10.0,-1,69.9,0.59,34.6,351.9,664.2,0.173,0.088,0
================================================================================
[36m(Trainer pid=2739, ip=192.168.54.57)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=2739, ip=192.168.54.57)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: citeseer, Trainers: 10, Distribution: average, IID Beta: 10000.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'citeseer', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 10000.0, 'distribution_type': 'average', 'gpu': False}
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x to ./data/citeseer/raw/ind.citeseer.x...
Downloaded ./data/citeseer/raw/ind.citeseer.x
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx to ./data/citeseer/raw/ind.citeseer.tx...
Downloaded ./data/citeseer/raw/ind.citeseer.tx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx to ./data/citeseer/raw/ind.citeseer.allx...
Downloaded ./data/citeseer/raw/ind.citeseer.allx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y to ./data/citeseer/raw/ind.citeseer.y...
Downloaded ./data/citeseer/raw/ind.citeseer.y
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty to ./data/citeseer/raw/ind.citeseer.ty...
Downloaded ./data/citeseer/raw/ind.citeseer.ty
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally to ./data/citeseer/raw/ind.citeseer.ally...
Downloaded ./data/citeseer/raw/ind.citeseer.ally
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph to ./data/citeseer/raw/ind.citeseer.graph...
Downloaded ./data/citeseer/raw/ind.citeseer.graph
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index to ./data/citeseer/raw/ind.citeseer.test.index...
Downloaded ./data/citeseer/raw/ind.citeseer.test.index
Initialization start: network data collected.
2025-07-29 16:23:23,814	INFO worker.py:1429 -- Using address 192.168.59.106:6379 set in the environment variable RAY_ADDRESS
2025-07-29 16:23:23,814	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.59.106:6379...
2025-07-29 16:23:23,821	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.59.106:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=3261, ip=192.168.28.30)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=3261, ip=192.168.28.30)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5423.397 ms //end
//Log Large1 init network: 153573.0 //end
//Log Large2 init network: 142530.0 //end
//Log Large3 init network: 112229.0 //end
//Log Large4 init network: 136247.0 //end
//Log Server init network: 50053889.0 //end
//Log Initialization Communication Cost (MB): 48.25 //end
Pretrain start time recorded.
//pretrain_time: 6.783 ms//end
//Log Max memory for Large1: 1231958016.0 //end
//Log Max memory for Large2: 1662840832.0 //end
//Log Max memory for Large3: 1237323776.0 //end
//Log Max memory for Large4: 1667686400.0 //end
//Log Max memory for Server: 2139172864.0 //end
//Log Large1 network: 649041.0 //end
//Log Large2 network: 801347.0 //end
//Log Large3 network: 666612.0 //end
//Log Large4 network: 789105.0 //end
//Log Server network: 3360743.0 //end
//Log Total Actual Pretrain Comm Cost: 5.98 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1690
Round 2: Global Test Accuracy = 0.1770
Round 3: Global Test Accuracy = 0.1930
Round 4: Global Test Accuracy = 0.2040
Round 5: Global Test Accuracy = 0.2150
Round 6: Global Test Accuracy = 0.2140
Round 7: Global Test Accuracy = 0.2270
Round 8: Global Test Accuracy = 0.2330
Round 9: Global Test Accuracy = 0.2410
Round 10: Global Test Accuracy = 0.2470
Round 11: Global Test Accuracy = 0.2520
Round 12: Global Test Accuracy = 0.2640
Round 13: Global Test Accuracy = 0.2660
Round 14: Global Test Accuracy = 0.2710
Round 15: Global Test Accuracy = 0.2790
Round 16: Global Test Accuracy = 0.2850
Round 17: Global Test Accuracy = 0.2870
Round 18: Global Test Accuracy = 0.2940
Round 19: Global Test Accuracy = 0.2910
Round 20: Global Test Accuracy = 0.2920
Round 21: Global Test Accuracy = 0.2980
Round 22: Global Test Accuracy = 0.3070
Round 23: Global Test Accuracy = 0.3090
Round 24: Global Test Accuracy = 0.3170
Round 25: Global Test Accuracy = 0.3140
Round 26: Global Test Accuracy = 0.3180
Round 27: Global Test Accuracy = 0.3300
Round 28: Global Test Accuracy = 0.3390
Round 29: Global Test Accuracy = 0.3490
Round 30: Global Test Accuracy = 0.3420
Round 31: Global Test Accuracy = 0.3630
Round 32: Global Test Accuracy = 0.3700
Round 33: Global Test Accuracy = 0.3740
Round 34: Global Test Accuracy = 0.3860
Round 35: Global Test Accuracy = 0.3980
Round 36: Global Test Accuracy = 0.4040
Round 37: Global Test Accuracy = 0.4020
Round 38: Global Test Accuracy = 0.4170
Round 39: Global Test Accuracy = 0.4180
Round 40: Global Test Accuracy = 0.4250
Round 41: Global Test Accuracy = 0.4240
Round 42: Global Test Accuracy = 0.4280
Round 43: Global Test Accuracy = 0.4390
Round 44: Global Test Accuracy = 0.4520
Round 45: Global Test Accuracy = 0.4540
Round 46: Global Test Accuracy = 0.4610
Round 47: Global Test Accuracy = 0.4640
Round 48: Global Test Accuracy = 0.4730
Round 49: Global Test Accuracy = 0.4750
Round 50: Global Test Accuracy = 0.4790
Round 51: Global Test Accuracy = 0.4830
Round 52: Global Test Accuracy = 0.4830
Round 53: Global Test Accuracy = 0.4900
Round 54: Global Test Accuracy = 0.4960
Round 55: Global Test Accuracy = 0.4970
Round 56: Global Test Accuracy = 0.5010
Round 57: Global Test Accuracy = 0.5070
Round 58: Global Test Accuracy = 0.5080
Round 59: Global Test Accuracy = 0.5110
Round 60: Global Test Accuracy = 0.5150
Round 61: Global Test Accuracy = 0.5170
Round 62: Global Test Accuracy = 0.5250
Round 63: Global Test Accuracy = 0.5260
Round 64: Global Test Accuracy = 0.5320
Round 65: Global Test Accuracy = 0.5320
Round 66: Global Test Accuracy = 0.5320
Round 67: Global Test Accuracy = 0.5370
Round 68: Global Test Accuracy = 0.5370
Round 69: Global Test Accuracy = 0.5370
Round 70: Global Test Accuracy = 0.5350
Round 71: Global Test Accuracy = 0.5360
Round 72: Global Test Accuracy = 0.5410
Round 73: Global Test Accuracy = 0.5450
Round 74: Global Test Accuracy = 0.5450
Round 75: Global Test Accuracy = 0.5460
Round 76: Global Test Accuracy = 0.5470
Round 77: Global Test Accuracy = 0.5480
Round 78: Global Test Accuracy = 0.5490
Round 79: Global Test Accuracy = 0.5440
Round 80: Global Test Accuracy = 0.5490
Round 81: Global Test Accuracy = 0.5480
Round 82: Global Test Accuracy = 0.5490
Round 83: Global Test Accuracy = 0.5560
Round 84: Global Test Accuracy = 0.5550
Round 85: Global Test Accuracy = 0.5570
Round 86: Global Test Accuracy = 0.5620
Round 87: Global Test Accuracy = 0.5590
Round 88: Global Test Accuracy = 0.5590
Round 89: Global Test Accuracy = 0.5610
Round 90: Global Test Accuracy = 0.5630
Round 91: Global Test Accuracy = 0.5650
Round 92: Global Test Accuracy = 0.5630
Round 93: Global Test Accuracy = 0.5660
Round 94: Global Test Accuracy = 0.5680
Round 95: Global Test Accuracy = 0.5700
Round 96: Global Test Accuracy = 0.5690
Round 97: Global Test Accuracy = 0.5690
Round 98: Global Test Accuracy = 0.5720
Round 99: Global Test Accuracy = 0.5730
Round 100: Global Test Accuracy = 0.5740
Round 101: Global Test Accuracy = 0.5720
Round 102: Global Test Accuracy = 0.5710
Round 103: Global Test Accuracy = 0.5720
Round 104: Global Test Accuracy = 0.5720
Round 105: Global Test Accuracy = 0.5730
Round 106: Global Test Accuracy = 0.5710
Round 107: Global Test Accuracy = 0.5730
Round 108: Global Test Accuracy = 0.5750
Round 109: Global Test Accuracy = 0.5730
Round 110: Global Test Accuracy = 0.5710
Round 111: Global Test Accuracy = 0.5760
Round 112: Global Test Accuracy = 0.5750
Round 113: Global Test Accuracy = 0.5750
Round 114: Global Test Accuracy = 0.5820
Round 115: Global Test Accuracy = 0.5790
Round 116: Global Test Accuracy = 0.5790
Round 117: Global Test Accuracy = 0.5790
Round 118: Global Test Accuracy = 0.5860
Round 119: Global Test Accuracy = 0.5840
Round 120: Global Test Accuracy = 0.5860
Round 121: Global Test Accuracy = 0.5820
Round 122: Global Test Accuracy = 0.5860
Round 123: Global Test Accuracy = 0.5880
Round 124: Global Test Accuracy = 0.5850
Round 125: Global Test Accuracy = 0.5860
Round 126: Global Test Accuracy = 0.5890
Round 127: Global Test Accuracy = 0.5890
Round 128: Global Test Accuracy = 0.5860
Round 129: Global Test Accuracy = 0.5840
Round 130: Global Test Accuracy = 0.5830
Round 131: Global Test Accuracy = 0.5810
Round 132: Global Test Accuracy = 0.5850
Round 133: Global Test Accuracy = 0.5810
Round 134: Global Test Accuracy = 0.5810
Round 135: Global Test Accuracy = 0.5820
Round 136: Global Test Accuracy = 0.5840
Round 137: Global Test Accuracy = 0.5830
Round 138: Global Test Accuracy = 0.5840
Round 139: Global Test Accuracy = 0.5830
Round 140: Global Test Accuracy = 0.5820
Round 141: Global Test Accuracy = 0.5840
Round 142: Global Test Accuracy = 0.5840
Round 143: Global Test Accuracy = 0.5800
Round 144: Global Test Accuracy = 0.5830
Round 145: Global Test Accuracy = 0.5820
Round 146: Global Test Accuracy = 0.5840
Round 147: Global Test Accuracy = 0.5850
Round 148: Global Test Accuracy = 0.5870
Round 149: Global Test Accuracy = 0.5850
Round 150: Global Test Accuracy = 0.5810
Round 151: Global Test Accuracy = 0.5810
Round 152: Global Test Accuracy = 0.5820
Round 153: Global Test Accuracy = 0.5820
Round 154: Global Test Accuracy = 0.5800
Round 155: Global Test Accuracy = 0.5790
Round 156: Global Test Accuracy = 0.5840
Round 157: Global Test Accuracy = 0.5780
Round 158: Global Test Accuracy = 0.5810
Round 159: Global Test Accuracy = 0.5840
Round 160: Global Test Accuracy = 0.5840
Round 161: Global Test Accuracy = 0.5860
Round 162: Global Test Accuracy = 0.5890
Round 163: Global Test Accuracy = 0.5830
Round 164: Global Test Accuracy = 0.5810
Round 165: Global Test Accuracy = 0.5810
Round 166: Global Test Accuracy = 0.5840
Round 167: Global Test Accuracy = 0.5840
Round 168: Global Test Accuracy = 0.5880
Round 169: Global Test Accuracy = 0.5900
Round 170: Global Test Accuracy = 0.5930
Round 171: Global Test Accuracy = 0.5900
Round 172: Global Test Accuracy = 0.5900
Round 173: Global Test Accuracy = 0.5900
Round 174: Global Test Accuracy = 0.5900
Round 175: Global Test Accuracy = 0.5900
Round 176: Global Test Accuracy = 0.5930
Round 177: Global Test Accuracy = 0.5910
Round 178: Global Test Accuracy = 0.5910
Round 179: Global Test Accuracy = 0.5900
Round 180: Global Test Accuracy = 0.5930
Round 181: Global Test Accuracy = 0.5880
Round 182: Global Test Accuracy = 0.5870
Round 183: Global Test Accuracy = 0.5870
Round 184: Global Test Accuracy = 0.5860
Round 185: Global Test Accuracy = 0.5880
Round 186: Global Test Accuracy = 0.5910
Round 187: Global Test Accuracy = 0.5880
Round 188: Global Test Accuracy = 0.5860
Round 189: Global Test Accuracy = 0.5900
Round 190: Global Test Accuracy = 0.5900
Round 191: Global Test Accuracy = 0.5890
Round 192: Global Test Accuracy = 0.5920
Round 193: Global Test Accuracy = 0.5910
Round 194: Global Test Accuracy = 0.5900
Round 195: Global Test Accuracy = 0.5880
Round 196: Global Test Accuracy = 0.5900
Round 197: Global Test Accuracy = 0.5900
Round 198: Global Test Accuracy = 0.5880
Round 199: Global Test Accuracy = 0.5860
Round 200: Global Test Accuracy = 0.5840
//train_time: 12519.687 ms//end
//Log Max memory for Large1: 1250705408.0 //end
//Log Max memory for Large2: 1688952832.0 //end
//Log Max memory for Large3: 1257791488.0 //end
//Log Max memory for Large4: 1686093824.0 //end
//Log Max memory for Server: 2206916608.0 //end
//Log Large1 network: 99147381.0 //end
//Log Large2 network: 148371804.0 //end
//Log Large3 network: 99213157.0 //end
//Log Large4 network: 148417918.0 //end
//Log Server network: 493645141.0 //end
//Log Total Actual Train Comm Cost: 942.99 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: citeseer, Batch Size: -1, Trainers: 10, Hops: 0, IID Beta: 10000.0 => Training Time = 42.52 seconds
average_final_test_loss, 1.1902133359909057
Average test accuracy, 0.584

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge
----------------------------------------------------------------------------------------------------
0        681.7        329      122      2.072        5.587
1        679.6        334      80       2.035        8.495
2        676.9        333      116      2.033        5.836
3        675.2        331      80       2.040        8.440
4        679.8        333      88       2.042        7.725
5        678.4        331      99       2.050        6.853
6        678.9        334      95       2.033        7.147
7        676.6        331      89       2.044        7.602
8        681.5        334      132      2.041        5.163
9        679.8        337      135      2.017        5.035
====================================================================================================
Total Memory Usage: 6788.5 MB (6.63 GB)
Total Nodes: 3327, Total Edges: 1036
Average Memory per Trainer: 678.9 MB
Average Nodes per Trainer: 332.7
Average Edges per Trainer: 103.6
Max Memory: 681.7 MB (Trainer 0)
Min Memory: 675.2 MB (Trainer 3)
Overall Memory/Node Ratio: 2.040 MB/node
Overall Memory/Edge Ratio: 6.553 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 905.85 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
citeseer,10000.0,-1,78.0,0.58,42.5,905.9,681.7,0.213,0.226,0
================================================================================
[36m(Trainer pid=3223, ip=192.168.54.57)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=3223, ip=192.168.54.57)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: citeseer, Trainers: 10, Distribution: average, IID Beta: 100.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'citeseer', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 100.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/citeseer/raw/ind.citeseer.x
File already exists: ./data/citeseer/raw/ind.citeseer.tx
File already exists: ./data/citeseer/raw/ind.citeseer.allx
File already exists: ./data/citeseer/raw/ind.citeseer.y
File already exists: ./data/citeseer/raw/ind.citeseer.ty
File already exists: ./data/citeseer/raw/ind.citeseer.ally
File already exists: ./data/citeseer/raw/ind.citeseer.graph
File already exists: ./data/citeseer/raw/ind.citeseer.test.index
Initialization start: network data collected.
2025-07-29 16:24:47,774	INFO worker.py:1429 -- Using address 192.168.59.106:6379 set in the environment variable RAY_ADDRESS
2025-07-29 16:24:47,774	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.59.106:6379...
2025-07-29 16:24:47,780	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.59.106:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=3806, ip=192.168.31.174)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=3806, ip=192.168.31.174)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5280.8640000000005 ms //end
//Log Large1 init network: 145446.0 //end
//Log Large2 init network: 115614.0 //end
//Log Large3 init network: 129962.0 //end
//Log Large4 init network: 118145.0 //end
//Log Server init network: 50091358.0 //end
//Log Initialization Communication Cost (MB): 48.26 //end
Pretrain start time recorded.
//pretrain_time: 7.713 ms//end
//Log Max memory for Large1: 1683066880.0 //end
//Log Max memory for Large2: 1261309952.0 //end
//Log Max memory for Large3: 1685049344.0 //end
//Log Max memory for Large4: 1261756416.0 //end
//Log Max memory for Server: 2223357952.0 //end
//Log Large1 network: 789043.0 //end
//Log Large2 network: 665698.0 //end
//Log Large3 network: 788300.0 //end
//Log Large4 network: 638808.0 //end
//Log Server network: 3317884.0 //end
//Log Total Actual Pretrain Comm Cost: 5.91 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1720
Round 2: Global Test Accuracy = 0.1790
Round 3: Global Test Accuracy = 0.1900
Round 4: Global Test Accuracy = 0.2000
Round 5: Global Test Accuracy = 0.2050
Round 6: Global Test Accuracy = 0.2100
Round 7: Global Test Accuracy = 0.2190
Round 8: Global Test Accuracy = 0.2220
Round 9: Global Test Accuracy = 0.2280
Round 10: Global Test Accuracy = 0.2360
Round 11: Global Test Accuracy = 0.2420
Round 12: Global Test Accuracy = 0.2470
Round 13: Global Test Accuracy = 0.2500
Round 14: Global Test Accuracy = 0.2520
Round 15: Global Test Accuracy = 0.2540
Round 16: Global Test Accuracy = 0.2550
Round 17: Global Test Accuracy = 0.2630
Round 18: Global Test Accuracy = 0.2640
Round 19: Global Test Accuracy = 0.2730
Round 20: Global Test Accuracy = 0.2800
Round 21: Global Test Accuracy = 0.2840
Round 22: Global Test Accuracy = 0.2920
Round 23: Global Test Accuracy = 0.2990
Round 24: Global Test Accuracy = 0.3080
Round 25: Global Test Accuracy = 0.3100
Round 26: Global Test Accuracy = 0.3240
Round 27: Global Test Accuracy = 0.3290
Round 28: Global Test Accuracy = 0.3320
Round 29: Global Test Accuracy = 0.3420
Round 30: Global Test Accuracy = 0.3470
Round 31: Global Test Accuracy = 0.3510
Round 32: Global Test Accuracy = 0.3570
Round 33: Global Test Accuracy = 0.3630
Round 34: Global Test Accuracy = 0.3730
Round 35: Global Test Accuracy = 0.3730
Round 36: Global Test Accuracy = 0.3820
Round 37: Global Test Accuracy = 0.3840
Round 38: Global Test Accuracy = 0.3940
Round 39: Global Test Accuracy = 0.4000
Round 40: Global Test Accuracy = 0.4010
Round 41: Global Test Accuracy = 0.4090
Round 42: Global Test Accuracy = 0.4130
Round 43: Global Test Accuracy = 0.4250
Round 44: Global Test Accuracy = 0.4320
Round 45: Global Test Accuracy = 0.4330
Round 46: Global Test Accuracy = 0.4410
Round 47: Global Test Accuracy = 0.4430
Round 48: Global Test Accuracy = 0.4470
Round 49: Global Test Accuracy = 0.4500
Round 50: Global Test Accuracy = 0.4630
Round 51: Global Test Accuracy = 0.4670
Round 52: Global Test Accuracy = 0.4700
Round 53: Global Test Accuracy = 0.4750
Round 54: Global Test Accuracy = 0.4700
Round 55: Global Test Accuracy = 0.4740
Round 56: Global Test Accuracy = 0.4820
Round 57: Global Test Accuracy = 0.4820
Round 58: Global Test Accuracy = 0.4870
Round 59: Global Test Accuracy = 0.4920
Round 60: Global Test Accuracy = 0.4910
Round 61: Global Test Accuracy = 0.4920
Round 62: Global Test Accuracy = 0.4980
Round 63: Global Test Accuracy = 0.4930
Round 64: Global Test Accuracy = 0.4990
Round 65: Global Test Accuracy = 0.5100
Round 66: Global Test Accuracy = 0.5020
Round 67: Global Test Accuracy = 0.5110
Round 68: Global Test Accuracy = 0.5170
Round 69: Global Test Accuracy = 0.5210
Round 70: Global Test Accuracy = 0.5200
Round 71: Global Test Accuracy = 0.5250
Round 72: Global Test Accuracy = 0.5260
Round 73: Global Test Accuracy = 0.5290
Round 74: Global Test Accuracy = 0.5300
Round 75: Global Test Accuracy = 0.5290
Round 76: Global Test Accuracy = 0.5270
Round 77: Global Test Accuracy = 0.5260
Round 78: Global Test Accuracy = 0.5310
Round 79: Global Test Accuracy = 0.5300
Round 80: Global Test Accuracy = 0.5330
Round 81: Global Test Accuracy = 0.5310
Round 82: Global Test Accuracy = 0.5330
Round 83: Global Test Accuracy = 0.5320
Round 84: Global Test Accuracy = 0.5310
Round 85: Global Test Accuracy = 0.5310
Round 86: Global Test Accuracy = 0.5320
Round 87: Global Test Accuracy = 0.5360
Round 88: Global Test Accuracy = 0.5380
Round 89: Global Test Accuracy = 0.5350
Round 90: Global Test Accuracy = 0.5330
Round 91: Global Test Accuracy = 0.5370
Round 92: Global Test Accuracy = 0.5350
Round 93: Global Test Accuracy = 0.5390
Round 94: Global Test Accuracy = 0.5370
Round 95: Global Test Accuracy = 0.5360
Round 96: Global Test Accuracy = 0.5410
Round 97: Global Test Accuracy = 0.5380
Round 98: Global Test Accuracy = 0.5390
Round 99: Global Test Accuracy = 0.5430
Round 100: Global Test Accuracy = 0.5400
Round 101: Global Test Accuracy = 0.5400
Round 102: Global Test Accuracy = 0.5400
Round 103: Global Test Accuracy = 0.5440
Round 104: Global Test Accuracy = 0.5370
Round 105: Global Test Accuracy = 0.5400
Round 106: Global Test Accuracy = 0.5400
Round 107: Global Test Accuracy = 0.5440
Round 108: Global Test Accuracy = 0.5460
Round 109: Global Test Accuracy = 0.5450
Round 110: Global Test Accuracy = 0.5450
Round 111: Global Test Accuracy = 0.5450
Round 112: Global Test Accuracy = 0.5460
Round 113: Global Test Accuracy = 0.5460
Round 114: Global Test Accuracy = 0.5470
Round 115: Global Test Accuracy = 0.5500
Round 116: Global Test Accuracy = 0.5490
Round 117: Global Test Accuracy = 0.5480
Round 118: Global Test Accuracy = 0.5480
Round 119: Global Test Accuracy = 0.5490
Round 120: Global Test Accuracy = 0.5460
Round 121: Global Test Accuracy = 0.5490
Round 122: Global Test Accuracy = 0.5490
Round 123: Global Test Accuracy = 0.5510
Round 124: Global Test Accuracy = 0.5500
Round 125: Global Test Accuracy = 0.5560
Round 126: Global Test Accuracy = 0.5530
Round 127: Global Test Accuracy = 0.5530
Round 128: Global Test Accuracy = 0.5540
Round 129: Global Test Accuracy = 0.5540
Round 130: Global Test Accuracy = 0.5520
Round 131: Global Test Accuracy = 0.5540
Round 132: Global Test Accuracy = 0.5540
Round 133: Global Test Accuracy = 0.5540
Round 134: Global Test Accuracy = 0.5550
Round 135: Global Test Accuracy = 0.5550
Round 136: Global Test Accuracy = 0.5530
Round 137: Global Test Accuracy = 0.5530
Round 138: Global Test Accuracy = 0.5550
Round 139: Global Test Accuracy = 0.5550
Round 140: Global Test Accuracy = 0.5520
Round 141: Global Test Accuracy = 0.5530
Round 142: Global Test Accuracy = 0.5560
Round 143: Global Test Accuracy = 0.5590
Round 144: Global Test Accuracy = 0.5550
Round 145: Global Test Accuracy = 0.5580
Round 146: Global Test Accuracy = 0.5630
Round 147: Global Test Accuracy = 0.5570
Round 148: Global Test Accuracy = 0.5600
Round 149: Global Test Accuracy = 0.5600
Round 150: Global Test Accuracy = 0.5600
Round 151: Global Test Accuracy = 0.5610
Round 152: Global Test Accuracy = 0.5580
Round 153: Global Test Accuracy = 0.5590
Round 154: Global Test Accuracy = 0.5610
Round 155: Global Test Accuracy = 0.5610
Round 156: Global Test Accuracy = 0.5600
Round 157: Global Test Accuracy = 0.5610
Round 158: Global Test Accuracy = 0.5610
Round 159: Global Test Accuracy = 0.5610
Round 160: Global Test Accuracy = 0.5640
Round 161: Global Test Accuracy = 0.5600
Round 162: Global Test Accuracy = 0.5640
Round 163: Global Test Accuracy = 0.5650
Round 164: Global Test Accuracy = 0.5640
Round 165: Global Test Accuracy = 0.5650
Round 166: Global Test Accuracy = 0.5650
Round 167: Global Test Accuracy = 0.5640
Round 168: Global Test Accuracy = 0.5660
Round 169: Global Test Accuracy = 0.5650
Round 170: Global Test Accuracy = 0.5680
Round 171: Global Test Accuracy = 0.5660
Round 172: Global Test Accuracy = 0.5650
Round 173: Global Test Accuracy = 0.5660
Round 174: Global Test Accuracy = 0.5650
Round 175: Global Test Accuracy = 0.5650
Round 176: Global Test Accuracy = 0.5660
Round 177: Global Test Accuracy = 0.5660
Round 178: Global Test Accuracy = 0.5680
Round 179: Global Test Accuracy = 0.5660
Round 180: Global Test Accuracy = 0.5650
Round 181: Global Test Accuracy = 0.5640
Round 182: Global Test Accuracy = 0.5660
Round 183: Global Test Accuracy = 0.5670
Round 184: Global Test Accuracy = 0.5660
Round 185: Global Test Accuracy = 0.5680
Round 186: Global Test Accuracy = 0.5710
Round 187: Global Test Accuracy = 0.5720
Round 188: Global Test Accuracy = 0.5710
Round 189: Global Test Accuracy = 0.5700
Round 190: Global Test Accuracy = 0.5680
Round 191: Global Test Accuracy = 0.5680
Round 192: Global Test Accuracy = 0.5680
Round 193: Global Test Accuracy = 0.5710
Round 194: Global Test Accuracy = 0.5700
Round 195: Global Test Accuracy = 0.5680
Round 196: Global Test Accuracy = 0.5680
Round 197: Global Test Accuracy = 0.5730
Round 198: Global Test Accuracy = 0.5710
Round 199: Global Test Accuracy = 0.5730
Round 200: Global Test Accuracy = 0.5690
//train_time: 12413.439 ms//end
//Log Max memory for Large1: 1688616960.0 //end
//Log Max memory for Large2: 1266483200.0 //end
//Log Max memory for Large3: 1692270592.0 //end
//Log Max memory for Large4: 1264197632.0 //end
//Log Max memory for Server: 2209234944.0 //end
//Log Large1 network: 148211920.0 //end
//Log Large2 network: 99081419.0 //end
//Log Large3 network: 148268279.0 //end
//Log Large4 network: 99193801.0 //end
//Log Server network: 493493029.0 //end
//Log Total Actual Train Comm Cost: 942.47 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: citeseer, Batch Size: -1, Trainers: 10, Hops: 0, IID Beta: 100.0 => Training Time = 42.42 seconds
average_final_test_loss, 1.223918135523796
Average test accuracy, 0.569

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge
----------------------------------------------------------------------------------------------------
0        683.0        331      114      2.064        5.991
1        679.8        346      103      1.965        6.600
2        677.1        310      89       2.184        7.608
3        675.1        347      123      1.945        5.488
4        684.2        335      101      2.043        6.775
5        681.8        330      74       2.066        9.214
6        676.3        321      82       2.107        8.248
7        673.8        320      93       2.105        7.245
8        683.0        323      100      2.114        6.830
9        681.3        364      165      1.872        4.129
====================================================================================================
Total Memory Usage: 6795.4 MB (6.64 GB)
Total Nodes: 3327, Total Edges: 1044
Average Memory per Trainer: 679.5 MB
Average Nodes per Trainer: 332.7
Average Edges per Trainer: 104.4
Max Memory: 684.2 MB (Trainer 4)
Min Memory: 673.8 MB (Trainer 7)
Overall Memory/Node Ratio: 2.042 MB/node
Overall Memory/Edge Ratio: 6.509 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 905.85 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
citeseer,100.0,-1,77.7,0.57,42.4,905.9,684.2,0.212,0.226,0
================================================================================
[36m(Trainer pid=3793, ip=192.168.52.89)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=3793, ip=192.168.52.89)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: citeseer, Trainers: 10, Distribution: average, IID Beta: 10.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'citeseer', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/citeseer/raw/ind.citeseer.x
File already exists: ./data/citeseer/raw/ind.citeseer.tx
File already exists: ./data/citeseer/raw/ind.citeseer.allx
File already exists: ./data/citeseer/raw/ind.citeseer.y
File already exists: ./data/citeseer/raw/ind.citeseer.ty
File already exists: ./data/citeseer/raw/ind.citeseer.ally
File already exists: ./data/citeseer/raw/ind.citeseer.graph
File already exists: ./data/citeseer/raw/ind.citeseer.test.index
Initialization start: network data collected.
2025-07-29 16:26:11,673	INFO worker.py:1429 -- Using address 192.168.59.106:6379 set in the environment variable RAY_ADDRESS
2025-07-29 16:26:11,674	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.59.106:6379...
2025-07-29 16:26:11,680	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.59.106:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=4352, ip=192.168.28.30)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=4352, ip=192.168.28.30)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5395.241 ms //end
//Log Large1 init network: 151294.0 //end
//Log Large2 init network: 135174.0 //end
//Log Large3 init network: 109103.0 //end
//Log Large4 init network: 177872.0 //end
//Log Server init network: 50023166.0 //end
//Log Initialization Communication Cost (MB): 48.25 //end
Pretrain start time recorded.
//pretrain_time: 7.622 ms//end
//Log Max memory for Large1: 1267437568.0 //end
//Log Max memory for Large2: 1696477184.0 //end
//Log Max memory for Large3: 1267023872.0 //end
//Log Max memory for Large4: 1692721152.0 //end
//Log Max memory for Server: 2239156224.0 //end
//Log Large1 network: 661952.0 //end
//Log Large2 network: 778901.0 //end
//Log Large3 network: 639656.0 //end
//Log Large4 network: 710796.0 //end
//Log Server network: 3407254.0 //end
//Log Total Actual Pretrain Comm Cost: 5.91 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.1600
Round 2: Global Test Accuracy = 0.1650
Round 3: Global Test Accuracy = 0.1800
Round 4: Global Test Accuracy = 0.1880
Round 5: Global Test Accuracy = 0.1960
Round 6: Global Test Accuracy = 0.1980
Round 7: Global Test Accuracy = 0.2050
Round 8: Global Test Accuracy = 0.2040
Round 9: Global Test Accuracy = 0.2090
Round 10: Global Test Accuracy = 0.2170
Round 11: Global Test Accuracy = 0.2280
Round 12: Global Test Accuracy = 0.2350
Round 13: Global Test Accuracy = 0.2410
Round 14: Global Test Accuracy = 0.2440
Round 15: Global Test Accuracy = 0.2420
Round 16: Global Test Accuracy = 0.2470
Round 17: Global Test Accuracy = 0.2590
Round 18: Global Test Accuracy = 0.2690
Round 19: Global Test Accuracy = 0.2710
Round 20: Global Test Accuracy = 0.2780
Round 21: Global Test Accuracy = 0.2790
Round 22: Global Test Accuracy = 0.2850
Round 23: Global Test Accuracy = 0.2910
Round 24: Global Test Accuracy = 0.2960
Round 25: Global Test Accuracy = 0.2980
Round 26: Global Test Accuracy = 0.3110
Round 27: Global Test Accuracy = 0.3220
Round 28: Global Test Accuracy = 0.3240
Round 29: Global Test Accuracy = 0.3300
Round 30: Global Test Accuracy = 0.3350
Round 31: Global Test Accuracy = 0.3410
Round 32: Global Test Accuracy = 0.3480
Round 33: Global Test Accuracy = 0.3660
Round 34: Global Test Accuracy = 0.3680
Round 35: Global Test Accuracy = 0.3730
Round 36: Global Test Accuracy = 0.3780
Round 37: Global Test Accuracy = 0.3850
Round 38: Global Test Accuracy = 0.3970
Round 39: Global Test Accuracy = 0.4080
Round 40: Global Test Accuracy = 0.4110
Round 41: Global Test Accuracy = 0.4190
Round 42: Global Test Accuracy = 0.4320
Round 43: Global Test Accuracy = 0.4290
Round 44: Global Test Accuracy = 0.4430
Round 45: Global Test Accuracy = 0.4470
Round 46: Global Test Accuracy = 0.4560
Round 47: Global Test Accuracy = 0.4600
Round 48: Global Test Accuracy = 0.4580
Round 49: Global Test Accuracy = 0.4710
Round 50: Global Test Accuracy = 0.4730
Round 51: Global Test Accuracy = 0.4750
Round 52: Global Test Accuracy = 0.4770
Round 53: Global Test Accuracy = 0.4890
Round 54: Global Test Accuracy = 0.4940
Round 55: Global Test Accuracy = 0.4990
Round 56: Global Test Accuracy = 0.4980
Round 57: Global Test Accuracy = 0.5050
Round 58: Global Test Accuracy = 0.5060
Round 59: Global Test Accuracy = 0.5070
Round 60: Global Test Accuracy = 0.5080
Round 61: Global Test Accuracy = 0.5100
Round 62: Global Test Accuracy = 0.5190
Round 63: Global Test Accuracy = 0.5190
Round 64: Global Test Accuracy = 0.5180
Round 65: Global Test Accuracy = 0.5180
Round 66: Global Test Accuracy = 0.5250
Round 67: Global Test Accuracy = 0.5270
Round 68: Global Test Accuracy = 0.5270
Round 69: Global Test Accuracy = 0.5240
Round 70: Global Test Accuracy = 0.5230
Round 71: Global Test Accuracy = 0.5230
Round 72: Global Test Accuracy = 0.5250
Round 73: Global Test Accuracy = 0.5260
Round 74: Global Test Accuracy = 0.5260
Round 75: Global Test Accuracy = 0.5320
Round 76: Global Test Accuracy = 0.5340
Round 77: Global Test Accuracy = 0.5330
Round 78: Global Test Accuracy = 0.5320
Round 79: Global Test Accuracy = 0.5320
Round 80: Global Test Accuracy = 0.5300
Round 81: Global Test Accuracy = 0.5320
Round 82: Global Test Accuracy = 0.5300
Round 83: Global Test Accuracy = 0.5310
Round 84: Global Test Accuracy = 0.5330
Round 85: Global Test Accuracy = 0.5370
Round 86: Global Test Accuracy = 0.5330
Round 87: Global Test Accuracy = 0.5320
Round 88: Global Test Accuracy = 0.5460
Round 89: Global Test Accuracy = 0.5490
Round 90: Global Test Accuracy = 0.5430
Round 91: Global Test Accuracy = 0.5450
Round 92: Global Test Accuracy = 0.5440
Round 93: Global Test Accuracy = 0.5510
Round 94: Global Test Accuracy = 0.5550
Round 95: Global Test Accuracy = 0.5570
Round 96: Global Test Accuracy = 0.5530
Round 97: Global Test Accuracy = 0.5530
Round 98: Global Test Accuracy = 0.5530
Round 99: Global Test Accuracy = 0.5520
Round 100: Global Test Accuracy = 0.5600
Round 101: Global Test Accuracy = 0.5570
Round 102: Global Test Accuracy = 0.5550
Round 103: Global Test Accuracy = 0.5540
Round 104: Global Test Accuracy = 0.5540
Round 105: Global Test Accuracy = 0.5500
Round 106: Global Test Accuracy = 0.5530
Round 107: Global Test Accuracy = 0.5560
Round 108: Global Test Accuracy = 0.5570
Round 109: Global Test Accuracy = 0.5600
Round 110: Global Test Accuracy = 0.5640
Round 111: Global Test Accuracy = 0.5610
Round 112: Global Test Accuracy = 0.5550
Round 113: Global Test Accuracy = 0.5570
Round 114: Global Test Accuracy = 0.5580
Round 115: Global Test Accuracy = 0.5620
Round 116: Global Test Accuracy = 0.5620
Round 117: Global Test Accuracy = 0.5620
Round 118: Global Test Accuracy = 0.5610
Round 119: Global Test Accuracy = 0.5610
Round 120: Global Test Accuracy = 0.5620
Round 121: Global Test Accuracy = 0.5670
Round 122: Global Test Accuracy = 0.5650
Round 123: Global Test Accuracy = 0.5670
Round 124: Global Test Accuracy = 0.5680
Round 125: Global Test Accuracy = 0.5690
Round 126: Global Test Accuracy = 0.5710
Round 127: Global Test Accuracy = 0.5670
Round 128: Global Test Accuracy = 0.5690
Round 129: Global Test Accuracy = 0.5680
Round 130: Global Test Accuracy = 0.5710
Round 131: Global Test Accuracy = 0.5700
Round 132: Global Test Accuracy = 0.5660
Round 133: Global Test Accuracy = 0.5670
Round 134: Global Test Accuracy = 0.5670
Round 135: Global Test Accuracy = 0.5680
Round 136: Global Test Accuracy = 0.5680
Round 137: Global Test Accuracy = 0.5670
Round 138: Global Test Accuracy = 0.5670
Round 139: Global Test Accuracy = 0.5660
Round 140: Global Test Accuracy = 0.5700
Round 141: Global Test Accuracy = 0.5630
Round 142: Global Test Accuracy = 0.5670
Round 143: Global Test Accuracy = 0.5700
Round 144: Global Test Accuracy = 0.5720
Round 145: Global Test Accuracy = 0.5720
Round 146: Global Test Accuracy = 0.5720
Round 147: Global Test Accuracy = 0.5740
Round 148: Global Test Accuracy = 0.5720
Round 149: Global Test Accuracy = 0.5720
Round 150: Global Test Accuracy = 0.5740
Round 151: Global Test Accuracy = 0.5790
Round 152: Global Test Accuracy = 0.5780
Round 153: Global Test Accuracy = 0.5780
Round 154: Global Test Accuracy = 0.5810
Round 155: Global Test Accuracy = 0.5820
Round 156: Global Test Accuracy = 0.5820
Round 157: Global Test Accuracy = 0.5810
Round 158: Global Test Accuracy = 0.5820
Round 159: Global Test Accuracy = 0.5830
Round 160: Global Test Accuracy = 0.5810
Round 161: Global Test Accuracy = 0.5810
Round 162: Global Test Accuracy = 0.5800
Round 163: Global Test Accuracy = 0.5840
Round 164: Global Test Accuracy = 0.5840
Round 165: Global Test Accuracy = 0.5820
Round 166: Global Test Accuracy = 0.5820
Round 167: Global Test Accuracy = 0.5830
Round 168: Global Test Accuracy = 0.5820
Round 169: Global Test Accuracy = 0.5820
Round 170: Global Test Accuracy = 0.5820
Round 171: Global Test Accuracy = 0.5870
Round 172: Global Test Accuracy = 0.5920
Round 173: Global Test Accuracy = 0.5850
Round 174: Global Test Accuracy = 0.5860
Round 175: Global Test Accuracy = 0.5850
Round 176: Global Test Accuracy = 0.5880
Round 177: Global Test Accuracy = 0.5900
Round 178: Global Test Accuracy = 0.5870
Round 179: Global Test Accuracy = 0.5860
Round 180: Global Test Accuracy = 0.5880
Round 181: Global Test Accuracy = 0.5880
Round 182: Global Test Accuracy = 0.5860
Round 183: Global Test Accuracy = 0.5830
Round 184: Global Test Accuracy = 0.5870
Round 185: Global Test Accuracy = 0.5900
Round 186: Global Test Accuracy = 0.5880
Round 187: Global Test Accuracy = 0.5850
Round 188: Global Test Accuracy = 0.5860
Round 189: Global Test Accuracy = 0.5860
Round 190: Global Test Accuracy = 0.5870
Round 191: Global Test Accuracy = 0.5880
Round 192: Global Test Accuracy = 0.5880
Round 193: Global Test Accuracy = 0.5880
Round 194: Global Test Accuracy = 0.5860
Round 195: Global Test Accuracy = 0.5840
Round 196: Global Test Accuracy = 0.5820
Round 197: Global Test Accuracy = 0.5840
Round 198: Global Test Accuracy = 0.5830
Round 199: Global Test Accuracy = 0.5830
Round 200: Global Test Accuracy = 0.5820
//train_time: 12541.435 ms//end
//Log Max memory for Large1: 1267437568.0 //end
//Log Max memory for Large2: 1693237248.0 //end
//Log Max memory for Large3: 1268174848.0 //end
//Log Max memory for Large4: 1691693056.0 //end
//Log Max memory for Server: 2253438976.0 //end
//Log Large1 network: 99094671.0 //end
//Log Large2 network: 148217917.0 //end
//Log Large3 network: 99189515.0 //end
//Log Large4 network: 148388089.0 //end
//Log Server network: 493501372.0 //end
//Log Total Actual Train Comm Cost: 942.60 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: citeseer, Batch Size: -1, Trainers: 10, Hops: 0, IID Beta: 10.0 => Training Time = 42.54 seconds
average_final_test_loss, 1.2040593657493592
Average test accuracy, 0.582

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge
----------------------------------------------------------------------------------------------------
0        679.2        339      79       2.003        8.597
1        679.1        306      122      2.219        5.567
2        674.6        301      87       2.241        7.754
3        675.8        350      140      1.931        4.827
4        680.9        353      138      1.929        4.934
5        681.5        352      110      1.936        6.195
6        675.4        324      114      2.085        5.924
7        675.2        330      109      2.046        6.194
8        681.1        320      119      2.128        5.723
9        680.2        352      116      1.932        5.864
====================================================================================================
Total Memory Usage: 6782.8 MB (6.62 GB)
Total Nodes: 3327, Total Edges: 1134
Average Memory per Trainer: 678.3 MB
Average Nodes per Trainer: 332.7
Average Edges per Trainer: 113.4
Max Memory: 681.5 MB (Trainer 5)
Min Memory: 674.6 MB (Trainer 2)
Overall Memory/Node Ratio: 2.039 MB/node
Overall Memory/Edge Ratio: 5.981 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 905.85 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
citeseer,10.0,-1,78.0,0.58,42.6,905.9,681.5,0.213,0.226,0
================================================================================
[36m(Trainer pid=4310, ip=192.168.31.174)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=4310, ip=192.168.31.174)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: pubmed, Trainers: 10, Distribution: average, IID Beta: 10000.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'pubmed', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 10000.0, 'distribution_type': 'average', 'gpu': False}
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x to ./data/pubmed/raw/ind.pubmed.x...
Error running experiment: Failed to download https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x. HTTP Status Code: 429
Configuration: {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'pubmed', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 10000.0, 'distribution_type': 'average', 'gpu': False}

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: pubmed, Trainers: 10, Distribution: average, IID Beta: 100.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'pubmed', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 100.0, 'distribution_type': 'average', 'gpu': False}
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x to ./data/pubmed/raw/ind.pubmed.x...
Downloaded ./data/pubmed/raw/ind.pubmed.x
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx to ./data/pubmed/raw/ind.pubmed.tx...
Downloaded ./data/pubmed/raw/ind.pubmed.tx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx to ./data/pubmed/raw/ind.pubmed.allx...
Downloaded ./data/pubmed/raw/ind.pubmed.allx
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y to ./data/pubmed/raw/ind.pubmed.y...
Downloaded ./data/pubmed/raw/ind.pubmed.y
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty to ./data/pubmed/raw/ind.pubmed.ty...
Downloaded ./data/pubmed/raw/ind.pubmed.ty
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally to ./data/pubmed/raw/ind.pubmed.ally...
Downloaded ./data/pubmed/raw/ind.pubmed.ally
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph to ./data/pubmed/raw/ind.pubmed.graph...
Downloaded ./data/pubmed/raw/ind.pubmed.graph
Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index to ./data/pubmed/raw/ind.pubmed.test.index...
Downloaded ./data/pubmed/raw/ind.pubmed.test.index
Initialization start: network data collected.
2025-07-29 16:27:51,829	INFO worker.py:1429 -- Using address 192.168.59.106:6379 set in the environment variable RAY_ADDRESS
2025-07-29 16:27:51,829	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.59.106:6379...
2025-07-29 16:27:51,835	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.59.106:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=4926, ip=192.168.28.30)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=4926, ip=192.168.28.30)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5408.076 ms //end
//Log Large1 init network: 123018.0 //end
//Log Large2 init network: 106438.0 //end
//Log Large3 init network: 140983.0 //end
//Log Large4 init network: 117152.0 //end
//Log Server init network: 40977608.0 //end
//Log Initialization Communication Cost (MB): 39.54 //end
Pretrain start time recorded.
//pretrain_time: 8.048 ms//end
//Log Max memory for Large1: 1682587648.0 //end
//Log Max memory for Large2: 1263816704.0 //end
//Log Max memory for Large3: 1685082112.0 //end
//Log Max memory for Large4: 1260707840.0 //end
//Log Max memory for Server: 2282328064.0 //end
//Log Large1 network: 707806.0 //end
//Log Large2 network: 619519.0 //end
//Log Large3 network: 651792.0 //end
//Log Large4 network: 602243.0 //end
//Log Server network: 1285364.0 //end
//Log Total Actual Pretrain Comm Cost: 3.69 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.3550
Round 2: Global Test Accuracy = 0.3450
Round 3: Global Test Accuracy = 0.3250
Round 4: Global Test Accuracy = 0.3300
Round 5: Global Test Accuracy = 0.3570
Round 6: Global Test Accuracy = 0.3660
Round 7: Global Test Accuracy = 0.3780
Round 8: Global Test Accuracy = 0.3800
Round 9: Global Test Accuracy = 0.3930
Round 10: Global Test Accuracy = 0.3990
Round 11: Global Test Accuracy = 0.3990
Round 12: Global Test Accuracy = 0.4020
Round 13: Global Test Accuracy = 0.4000
Round 14: Global Test Accuracy = 0.4070
Round 15: Global Test Accuracy = 0.4070
Round 16: Global Test Accuracy = 0.4070
Round 17: Global Test Accuracy = 0.4070
Round 18: Global Test Accuracy = 0.4070
Round 19: Global Test Accuracy = 0.4080
Round 20: Global Test Accuracy = 0.4080
Round 21: Global Test Accuracy = 0.4060
Round 22: Global Test Accuracy = 0.4070
Round 23: Global Test Accuracy = 0.4070
Round 24: Global Test Accuracy = 0.4080
Round 25: Global Test Accuracy = 0.4090
Round 26: Global Test Accuracy = 0.4080
Round 27: Global Test Accuracy = 0.4080
Round 28: Global Test Accuracy = 0.4090
Round 29: Global Test Accuracy = 0.4080
Round 30: Global Test Accuracy = 0.4080
Round 31: Global Test Accuracy = 0.4080
Round 32: Global Test Accuracy = 0.4080
Round 33: Global Test Accuracy = 0.4080
Round 34: Global Test Accuracy = 0.4080
Round 35: Global Test Accuracy = 0.4070
Round 36: Global Test Accuracy = 0.4080
Round 37: Global Test Accuracy = 0.4070
Round 38: Global Test Accuracy = 0.4070
Round 39: Global Test Accuracy = 0.4070
Round 40: Global Test Accuracy = 0.4070
Round 41: Global Test Accuracy = 0.4070
Round 42: Global Test Accuracy = 0.4070
Round 43: Global Test Accuracy = 0.4070
Round 44: Global Test Accuracy = 0.4070
Round 45: Global Test Accuracy = 0.4070
Round 46: Global Test Accuracy = 0.4070
Round 47: Global Test Accuracy = 0.4070
Round 48: Global Test Accuracy = 0.4070
Round 49: Global Test Accuracy = 0.4070
Round 50: Global Test Accuracy = 0.4070
Round 51: Global Test Accuracy = 0.4070
Round 52: Global Test Accuracy = 0.4070
Round 53: Global Test Accuracy = 0.4070
Round 54: Global Test Accuracy = 0.4070
Round 55: Global Test Accuracy = 0.4070
Round 56: Global Test Accuracy = 0.4070
Round 57: Global Test Accuracy = 0.4070
Round 58: Global Test Accuracy = 0.4070
Round 59: Global Test Accuracy = 0.4070
Round 60: Global Test Accuracy = 0.4070
Round 61: Global Test Accuracy = 0.4070
Round 62: Global Test Accuracy = 0.4070
Round 63: Global Test Accuracy = 0.4070
Round 64: Global Test Accuracy = 0.4070
Round 65: Global Test Accuracy = 0.4070
Round 66: Global Test Accuracy = 0.4070
Round 67: Global Test Accuracy = 0.4070
Round 68: Global Test Accuracy = 0.4070
Round 69: Global Test Accuracy = 0.4070
Round 70: Global Test Accuracy = 0.4070
Round 71: Global Test Accuracy = 0.4070
Round 72: Global Test Accuracy = 0.4070
Round 73: Global Test Accuracy = 0.4070
Round 74: Global Test Accuracy = 0.4070
Round 75: Global Test Accuracy = 0.4070
Round 76: Global Test Accuracy = 0.4070
Round 77: Global Test Accuracy = 0.4070
Round 78: Global Test Accuracy = 0.4070
Round 79: Global Test Accuracy = 0.4070
Round 80: Global Test Accuracy = 0.4070
Round 81: Global Test Accuracy = 0.4070
Round 82: Global Test Accuracy = 0.4070
Round 83: Global Test Accuracy = 0.4070
Round 84: Global Test Accuracy = 0.4070
Round 85: Global Test Accuracy = 0.4080
Round 86: Global Test Accuracy = 0.4070
Round 87: Global Test Accuracy = 0.4070
Round 88: Global Test Accuracy = 0.4070
Round 89: Global Test Accuracy = 0.4070
Round 90: Global Test Accuracy = 0.4070
Round 91: Global Test Accuracy = 0.4070
Round 92: Global Test Accuracy = 0.4070
Round 93: Global Test Accuracy = 0.4070
Round 94: Global Test Accuracy = 0.4070
Round 95: Global Test Accuracy = 0.4070
Round 96: Global Test Accuracy = 0.4070
Round 97: Global Test Accuracy = 0.4070
Round 98: Global Test Accuracy = 0.4070
Round 99: Global Test Accuracy = 0.4070
Round 100: Global Test Accuracy = 0.4070
Round 101: Global Test Accuracy = 0.4070
Round 102: Global Test Accuracy = 0.4070
Round 103: Global Test Accuracy = 0.4070
Round 104: Global Test Accuracy = 0.4070
Round 105: Global Test Accuracy = 0.4070
Round 106: Global Test Accuracy = 0.4070
Round 107: Global Test Accuracy = 0.4070
Round 108: Global Test Accuracy = 0.4070
Round 109: Global Test Accuracy = 0.4070
Round 110: Global Test Accuracy = 0.4070
Round 111: Global Test Accuracy = 0.4070
Round 112: Global Test Accuracy = 0.4070
Round 113: Global Test Accuracy = 0.4070
Round 114: Global Test Accuracy = 0.4080
Round 115: Global Test Accuracy = 0.4070
Round 116: Global Test Accuracy = 0.4070
Round 117: Global Test Accuracy = 0.4070
Round 118: Global Test Accuracy = 0.4080
Round 119: Global Test Accuracy = 0.4080
Round 120: Global Test Accuracy = 0.4080
Round 121: Global Test Accuracy = 0.4070
Round 122: Global Test Accuracy = 0.4090
Round 123: Global Test Accuracy = 0.4110
Round 124: Global Test Accuracy = 0.4100
Round 125: Global Test Accuracy = 0.4100
Round 126: Global Test Accuracy = 0.4100
Round 127: Global Test Accuracy = 0.4090
Round 128: Global Test Accuracy = 0.4120
Round 129: Global Test Accuracy = 0.4120
Round 130: Global Test Accuracy = 0.4130
Round 131: Global Test Accuracy = 0.4130
Round 132: Global Test Accuracy = 0.4110
Round 133: Global Test Accuracy = 0.4130
Round 134: Global Test Accuracy = 0.4130
Round 135: Global Test Accuracy = 0.4120
Round 136: Global Test Accuracy = 0.4170
Round 137: Global Test Accuracy = 0.4120
Round 138: Global Test Accuracy = 0.4130
Round 139: Global Test Accuracy = 0.4120
Round 140: Global Test Accuracy = 0.4160
Round 141: Global Test Accuracy = 0.4120
Round 142: Global Test Accuracy = 0.4120
Round 143: Global Test Accuracy = 0.4130
Round 144: Global Test Accuracy = 0.4140
Round 145: Global Test Accuracy = 0.4150
Round 146: Global Test Accuracy = 0.4190
Round 147: Global Test Accuracy = 0.4190
Round 148: Global Test Accuracy = 0.4190
Round 149: Global Test Accuracy = 0.4190
Round 150: Global Test Accuracy = 0.4200
Round 151: Global Test Accuracy = 0.4180
Round 152: Global Test Accuracy = 0.4180
Round 153: Global Test Accuracy = 0.4180
Round 154: Global Test Accuracy = 0.4180
Round 155: Global Test Accuracy = 0.4190
Round 156: Global Test Accuracy = 0.4200
Round 157: Global Test Accuracy = 0.4210
Round 158: Global Test Accuracy = 0.4170
Round 159: Global Test Accuracy = 0.4210
Round 160: Global Test Accuracy = 0.4190
Round 161: Global Test Accuracy = 0.4240
Round 162: Global Test Accuracy = 0.4230
Round 163: Global Test Accuracy = 0.4210
Round 164: Global Test Accuracy = 0.4180
Round 165: Global Test Accuracy = 0.4290
Round 166: Global Test Accuracy = 0.4260
Round 167: Global Test Accuracy = 0.4270
Round 168: Global Test Accuracy = 0.4290
Round 169: Global Test Accuracy = 0.4290
Round 170: Global Test Accuracy = 0.4230
Round 171: Global Test Accuracy = 0.4270
Round 172: Global Test Accuracy = 0.4330
Round 173: Global Test Accuracy = 0.4200
Round 174: Global Test Accuracy = 0.4210
Round 175: Global Test Accuracy = 0.4240
Round 176: Global Test Accuracy = 0.4200
Round 177: Global Test Accuracy = 0.4240
Round 178: Global Test Accuracy = 0.4230
Round 179: Global Test Accuracy = 0.4190
Round 180: Global Test Accuracy = 0.4200
Round 181: Global Test Accuracy = 0.4190
Round 182: Global Test Accuracy = 0.4170
Round 183: Global Test Accuracy = 0.4190
Round 184: Global Test Accuracy = 0.4170
Round 185: Global Test Accuracy = 0.4180
Round 186: Global Test Accuracy = 0.4240
Round 187: Global Test Accuracy = 0.4200
Round 188: Global Test Accuracy = 0.4260
Round 189: Global Test Accuracy = 0.4260
Round 190: Global Test Accuracy = 0.4270
Round 191: Global Test Accuracy = 0.4270
Round 192: Global Test Accuracy = 0.4230
Round 193: Global Test Accuracy = 0.4230
Round 194: Global Test Accuracy = 0.4290
Round 195: Global Test Accuracy = 0.4270
Round 196: Global Test Accuracy = 0.4270
Round 197: Global Test Accuracy = 0.4300
Round 198: Global Test Accuracy = 0.4410
Round 199: Global Test Accuracy = 0.4330
Round 200: Global Test Accuracy = 0.4270
//train_time: 4743.299 ms//end
//Log Max memory for Large1: 1709912064.0 //end
//Log Max memory for Large2: 1281867776.0 //end
//Log Max memory for Large3: 1716137984.0 //end
//Log Max memory for Large4: 1279053824.0 //end
//Log Max memory for Server: 2356977664.0 //end
//Log Large1 network: 22313791.0 //end
//Log Large2 network: 15104855.0 //end
//Log Large3 network: 22353799.0 //end
//Log Large4 network: 15066879.0 //end
//Log Server network: 75081292.0 //end
//Log Total Actual Train Comm Cost: 142.98 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: pubmed, Batch Size: -1, Trainers: 10, Hops: 0, IID Beta: 100.0 => Training Time = 34.74 seconds
average_final_test_loss, 1.0680594795942306
Average test accuracy, 0.427

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge
----------------------------------------------------------------------------------------------------
0        666.5        2028     968      0.329        0.689
1        663.0        1996     933      0.332        0.711
2        665.0        2063     904      0.322        0.736
3        664.2        1756     654      0.378        1.016
4        667.1        2034     930      0.328        0.717
5        665.2        2015     1002     0.330        0.664
6        666.3        2174     1188     0.306        0.561
7        663.6        1861     828      0.357        0.801
8        665.3        1907     840      0.349        0.792
9        663.4        1883     764      0.352        0.868
====================================================================================================
Total Memory Usage: 6649.6 MB (6.49 GB)
Total Nodes: 19717, Total Edges: 9011
Average Memory per Trainer: 665.0 MB
Average Nodes per Trainer: 1971.7
Average Edges per Trainer: 901.1
Max Memory: 667.1 MB (Trainer 4)
Min Memory: 663.0 MB (Trainer 1)
Overall Memory/Node Ratio: 0.337 MB/node
Overall Memory/Edge Ratio: 0.738 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 123.09 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
pubmed,100.0,-1,70.2,0.43,34.8,123.1,667.1,0.174,0.031,0
================================================================================
[36m(Trainer pid=4973, ip=192.168.54.57)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=4973, ip=192.168.54.57)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: pubmed, Trainers: 10, Distribution: average, IID Beta: 10.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'pubmed', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
File already exists: ./data/pubmed/raw/ind.pubmed.x
File already exists: ./data/pubmed/raw/ind.pubmed.tx
File already exists: ./data/pubmed/raw/ind.pubmed.allx
File already exists: ./data/pubmed/raw/ind.pubmed.y
File already exists: ./data/pubmed/raw/ind.pubmed.ty
File already exists: ./data/pubmed/raw/ind.pubmed.ally
File already exists: ./data/pubmed/raw/ind.pubmed.graph
File already exists: ./data/pubmed/raw/ind.pubmed.test.index
Initialization start: network data collected.
2025-07-29 16:29:12,962	INFO worker.py:1429 -- Using address 192.168.59.106:6379 set in the environment variable RAY_ADDRESS
2025-07-29 16:29:12,963	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.59.106:6379...
2025-07-29 16:29:12,969	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.59.106:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=5503, ip=192.168.28.30)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=5503, ip=192.168.28.30)[0m   return torch.load(io.BytesIO(b))
//Log init_time: 5422.664 ms //end
//Log Large1 init network: 110905.0 //end
//Log Large2 init network: 125788.0 //end
//Log Large3 init network: 104263.0 //end
//Log Large4 init network: 141238.0 //end
//Log Server init network: 40989246.0 //end
//Log Initialization Communication Cost (MB): 39.55 //end
Pretrain start time recorded.
//pretrain_time: 11.152000000000001 ms//end
//Log Max memory for Large1: 1279803392.0 //end
//Log Max memory for Large2: 1692069888.0 //end
//Log Max memory for Large3: 1276911616.0 //end
//Log Max memory for Large4: 1688231936.0 //end
//Log Max memory for Server: 2386649088.0 //end
//Log Large1 network: 606911.0 //end
//Log Large2 network: 717422.0 //end
//Log Large3 network: 612887.0 //end
//Log Large4 network: 747806.0 //end
//Log Server network: 1308786.0 //end
//Log Total Actual Pretrain Comm Cost: 3.81 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.3330
Round 2: Global Test Accuracy = 0.3120
Round 3: Global Test Accuracy = 0.2630
Round 4: Global Test Accuracy = 0.2290
Round 5: Global Test Accuracy = 0.2310
Round 6: Global Test Accuracy = 0.2040
Round 7: Global Test Accuracy = 0.2130
Round 8: Global Test Accuracy = 0.2420
Round 9: Global Test Accuracy = 0.2300
Round 10: Global Test Accuracy = 0.2300
Round 11: Global Test Accuracy = 0.2290
Round 12: Global Test Accuracy = 0.2180
Round 13: Global Test Accuracy = 0.2310
Round 14: Global Test Accuracy = 0.2730
Round 15: Global Test Accuracy = 0.2730
Round 16: Global Test Accuracy = 0.3060
Round 17: Global Test Accuracy = 0.3160
Round 18: Global Test Accuracy = 0.2930
Round 19: Global Test Accuracy = 0.3030
Round 20: Global Test Accuracy = 0.3320
Round 21: Global Test Accuracy = 0.3210
Round 22: Global Test Accuracy = 0.3180
Round 23: Global Test Accuracy = 0.3390
Round 24: Global Test Accuracy = 0.3360
Round 25: Global Test Accuracy = 0.3350
Round 26: Global Test Accuracy = 0.3260
Round 27: Global Test Accuracy = 0.3130
Round 28: Global Test Accuracy = 0.3080
Round 29: Global Test Accuracy = 0.3360
Round 30: Global Test Accuracy = 0.3000
Round 31: Global Test Accuracy = 0.3020
Round 32: Global Test Accuracy = 0.2880
Round 33: Global Test Accuracy = 0.2610
Round 34: Global Test Accuracy = 0.2770
Round 35: Global Test Accuracy = 0.3150
Round 36: Global Test Accuracy = 0.3390
Round 37: Global Test Accuracy = 0.3390
Round 38: Global Test Accuracy = 0.3640
Round 39: Global Test Accuracy = 0.3660
Round 40: Global Test Accuracy = 0.3810
Round 41: Global Test Accuracy = 0.3630
Round 42: Global Test Accuracy = 0.3390
Round 43: Global Test Accuracy = 0.3280
Round 44: Global Test Accuracy = 0.3180
Round 45: Global Test Accuracy = 0.3350
Round 46: Global Test Accuracy = 0.3470
Round 47: Global Test Accuracy = 0.3470
Round 48: Global Test Accuracy = 0.3440
Round 49: Global Test Accuracy = 0.3430
Round 50: Global Test Accuracy = 0.3410
Round 51: Global Test Accuracy = 0.3760
Round 52: Global Test Accuracy = 0.3770
Round 53: Global Test Accuracy = 0.3850
Round 54: Global Test Accuracy = 0.3840
Round 55: Global Test Accuracy = 0.3850
Round 56: Global Test Accuracy = 0.3870
Round 57: Global Test Accuracy = 0.3880
Round 58: Global Test Accuracy = 0.3890
Round 59: Global Test Accuracy = 0.3810
Round 60: Global Test Accuracy = 0.3700
Round 61: Global Test Accuracy = 0.3920
Round 62: Global Test Accuracy = 0.3880
Round 63: Global Test Accuracy = 0.3800
Round 64: Global Test Accuracy = 0.3890
Round 65: Global Test Accuracy = 0.3900
Round 66: Global Test Accuracy = 0.3730
Round 67: Global Test Accuracy = 0.3600
Round 68: Global Test Accuracy = 0.3810
Round 69: Global Test Accuracy = 0.3800
Round 70: Global Test Accuracy = 0.3850
Round 71: Global Test Accuracy = 0.3860
Round 72: Global Test Accuracy = 0.3930
Round 73: Global Test Accuracy = 0.3840
Round 74: Global Test Accuracy = 0.3750
Round 75: Global Test Accuracy = 0.3870
Round 76: Global Test Accuracy = 0.3980
Round 77: Global Test Accuracy = 0.3990
Round 78: Global Test Accuracy = 0.4030
Round 79: Global Test Accuracy = 0.3890
Round 80: Global Test Accuracy = 0.3820
Round 81: Global Test Accuracy = 0.3770
Round 82: Global Test Accuracy = 0.3870
Round 83: Global Test Accuracy = 0.3910
Round 84: Global Test Accuracy = 0.3920
Round 85: Global Test Accuracy = 0.4110
Round 86: Global Test Accuracy = 0.4080
Round 87: Global Test Accuracy = 0.4090
Round 88: Global Test Accuracy = 0.4070
Round 89: Global Test Accuracy = 0.4120
Round 90: Global Test Accuracy = 0.4110
Round 91: Global Test Accuracy = 0.4120
Round 92: Global Test Accuracy = 0.4050
Round 93: Global Test Accuracy = 0.4070
Round 94: Global Test Accuracy = 0.4020
Round 95: Global Test Accuracy = 0.4080
Round 96: Global Test Accuracy = 0.4050
Round 97: Global Test Accuracy = 0.4040
Round 98: Global Test Accuracy = 0.4120
Round 99: Global Test Accuracy = 0.4120
Round 100: Global Test Accuracy = 0.4200
Round 101: Global Test Accuracy = 0.4160
Round 102: Global Test Accuracy = 0.4160
Round 103: Global Test Accuracy = 0.4050
Round 104: Global Test Accuracy = 0.4150
Round 105: Global Test Accuracy = 0.4140
Round 106: Global Test Accuracy = 0.4160
Round 107: Global Test Accuracy = 0.4210
Round 108: Global Test Accuracy = 0.4160
Round 109: Global Test Accuracy = 0.4190
Round 110: Global Test Accuracy = 0.4160
Round 111: Global Test Accuracy = 0.4090
Round 112: Global Test Accuracy = 0.3690
Round 113: Global Test Accuracy = 0.3860
Round 114: Global Test Accuracy = 0.4000
Round 115: Global Test Accuracy = 0.3840
Round 116: Global Test Accuracy = 0.3630
Round 117: Global Test Accuracy = 0.3800
Round 118: Global Test Accuracy = 0.4020
Round 119: Global Test Accuracy = 0.4100
Round 120: Global Test Accuracy = 0.4300
Round 121: Global Test Accuracy = 0.4270
Round 122: Global Test Accuracy = 0.4310
Round 123: Global Test Accuracy = 0.4260
Round 124: Global Test Accuracy = 0.4320
Round 125: Global Test Accuracy = 0.4090
Round 126: Global Test Accuracy = 0.4350
Round 127: Global Test Accuracy = 0.4370
Round 128: Global Test Accuracy = 0.4400
Round 129: Global Test Accuracy = 0.4350
Round 130: Global Test Accuracy = 0.4350
Round 131: Global Test Accuracy = 0.4350
Round 132: Global Test Accuracy = 0.4250
Round 133: Global Test Accuracy = 0.4180
Round 134: Global Test Accuracy = 0.3850
Round 135: Global Test Accuracy = 0.4090
Round 136: Global Test Accuracy = 0.4340
Round 137: Global Test Accuracy = 0.4390
Round 138: Global Test Accuracy = 0.4240
Round 139: Global Test Accuracy = 0.4090
Round 140: Global Test Accuracy = 0.4080
Round 141: Global Test Accuracy = 0.4140
Round 142: Global Test Accuracy = 0.4060
Round 143: Global Test Accuracy = 0.4240
Round 144: Global Test Accuracy = 0.4210
Round 145: Global Test Accuracy = 0.3950
Round 146: Global Test Accuracy = 0.4320
Round 147: Global Test Accuracy = 0.3980
Round 148: Global Test Accuracy = 0.4070
Round 149: Global Test Accuracy = 0.3900
Round 150: Global Test Accuracy = 0.4300
Round 151: Global Test Accuracy = 0.4100
Round 152: Global Test Accuracy = 0.4050
Round 153: Global Test Accuracy = 0.4120
Round 154: Global Test Accuracy = 0.4450
Round 155: Global Test Accuracy = 0.4570
Round 156: Global Test Accuracy = 0.4560
Round 157: Global Test Accuracy = 0.4490
Round 158: Global Test Accuracy = 0.4570
Round 159: Global Test Accuracy = 0.4590
Round 160: Global Test Accuracy = 0.4530
Round 161: Global Test Accuracy = 0.4280
Round 162: Global Test Accuracy = 0.4030
Round 163: Global Test Accuracy = 0.4200
Round 164: Global Test Accuracy = 0.4200
Round 165: Global Test Accuracy = 0.4170
Round 166: Global Test Accuracy = 0.4270
Round 167: Global Test Accuracy = 0.4470
Round 168: Global Test Accuracy = 0.4470
Round 169: Global Test Accuracy = 0.4530
Round 170: Global Test Accuracy = 0.4650
Round 171: Global Test Accuracy = 0.4470
Round 172: Global Test Accuracy = 0.4410
Round 173: Global Test Accuracy = 0.4350
Round 174: Global Test Accuracy = 0.4170
Round 175: Global Test Accuracy = 0.4500
Round 176: Global Test Accuracy = 0.4430
Round 177: Global Test Accuracy = 0.4480
Round 178: Global Test Accuracy = 0.4490
Round 179: Global Test Accuracy = 0.4330
Round 180: Global Test Accuracy = 0.4050
Round 181: Global Test Accuracy = 0.4180
Round 182: Global Test Accuracy = 0.4010
Round 183: Global Test Accuracy = 0.4090
Round 184: Global Test Accuracy = 0.3630
Round 185: Global Test Accuracy = 0.3580
Round 186: Global Test Accuracy = 0.3500
Round 187: Global Test Accuracy = 0.3560
Round 188: Global Test Accuracy = 0.3400
Round 189: Global Test Accuracy = 0.3570
Round 190: Global Test Accuracy = 0.3610
Round 191: Global Test Accuracy = 0.4110
Round 192: Global Test Accuracy = 0.4310
Round 193: Global Test Accuracy = 0.4590
Round 194: Global Test Accuracy = 0.4540
Round 195: Global Test Accuracy = 0.4460
Round 196: Global Test Accuracy = 0.4430
Round 197: Global Test Accuracy = 0.4650
Round 198: Global Test Accuracy = 0.4580
Round 199: Global Test Accuracy = 0.4560
Round 200: Global Test Accuracy = 0.4310
//train_time: 4702.384 ms//end
//Log Max memory for Large1: 1295601664.0 //end
//Log Max memory for Large2: 1717923840.0 //end
//Log Max memory for Large3: 1294233600.0 //end
//Log Max memory for Large4: 1714454528.0 //end
//Log Max memory for Server: 2465017856.0 //end
//Log Large1 network: 15072279.0 //end
//Log Large2 network: 22405310.0 //end
//Log Large3 network: 15065518.0 //end
//Log Large4 network: 22356964.0 //end
//Log Server network: 75137480.0 //end
//Log Total Actual Train Comm Cost: 143.09 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: pubmed, Batch Size: -1, Trainers: 10, Hops: 0, IID Beta: 10.0 => Training Time = 34.70 seconds
average_final_test_loss, 1.0973254605531693
Average test accuracy, 0.431

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge
----------------------------------------------------------------------------------------------------
0        663.7        1821     838      0.364        0.792
1        664.8        1687     633      0.394        1.050
2        664.2        2284     1415     0.291        0.469
3        666.0        1811     824      0.368        0.808
4        663.2        1620     550      0.409        1.206
5        666.5        2521     1490     0.264        0.447
6        663.7        1989     930      0.334        0.714
7        666.8        2173     1108     0.307        0.602
8        668.9        2341     1079     0.286        0.620
9        662.6        1470     470      0.451        1.410
====================================================================================================
Total Memory Usage: 6650.3 MB (6.49 GB)
Total Nodes: 19717, Total Edges: 9337
Average Memory per Trainer: 665.0 MB
Average Nodes per Trainer: 1971.7
Average Edges per Trainer: 933.7
Max Memory: 668.9 MB (Trainer 8)
Min Memory: 662.6 MB (Trainer 9)
Overall Memory/Node Ratio: 0.337 MB/node
Overall Memory/Edge Ratio: 0.712 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 123.09 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
pubmed,10.0,-1,70.1,0.43,34.7,123.1,668.9,0.174,0.031,0
================================================================================
[36m(Trainer pid=5468, ip=192.168.54.57)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=5468, ip=192.168.54.57)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: ogbn-arxiv, Trainers: 10, Distribution: average, IID Beta: 10000.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'ogbn-arxiv', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 10000.0, 'distribution_type': 'average', 'gpu': False}
ogbn-arxiv has been updated.
Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip

  0%|          | 0/81 [00:00<?, ?it/s]
Downloaded 0.00 GB:   0%|          | 0/81 [00:00<?, ?it/s]
Downloaded 0.00 GB:   1%|          | 1/81 [00:00<01:19,  1.00it/s]
Downloaded 0.00 GB:   1%|          | 1/81 [00:01<01:19,  1.00it/s]
Downloaded 0.00 GB:   2%|▏         | 2/81 [00:01<00:46,  1.70it/s]
Downloaded 0.00 GB:   2%|▏         | 2/81 [00:01<00:46,  1.70it/s]
Downloaded 0.00 GB:   4%|▎         | 3/81 [00:01<00:29,  2.61it/s]
Downloaded 0.00 GB:   4%|▎         | 3/81 [00:01<00:29,  2.61it/s]
Downloaded 0.00 GB:   5%|▍         | 4/81 [00:01<00:21,  3.51it/s]
Downloaded 0.00 GB:   5%|▍         | 4/81 [00:01<00:21,  3.51it/s]
Downloaded 0.01 GB:   5%|▍         | 4/81 [00:01<00:21,  3.51it/s]
Downloaded 0.01 GB:   7%|▋         | 6/81 [00:01<00:13,  5.77it/s]
Downloaded 0.01 GB:   7%|▋         | 6/81 [00:01<00:13,  5.77it/s]
Downloaded 0.01 GB:   7%|▋         | 6/81 [00:01<00:13,  5.77it/s]
Downloaded 0.01 GB:   7%|▋         | 6/81 [00:01<00:13,  5.77it/s]
Downloaded 0.01 GB:  11%|█         | 9/81 [00:01<00:07,  9.51it/s]
Downloaded 0.01 GB:  11%|█         | 9/81 [00:01<00:07,  9.51it/s]
Downloaded 0.01 GB:  11%|█         | 9/81 [00:01<00:07,  9.51it/s]
Downloaded 0.01 GB:  11%|█         | 9/81 [00:01<00:07,  9.51it/s]
Downloaded 0.01 GB:  11%|█         | 9/81 [00:01<00:07,  9.51it/s]
Downloaded 0.01 GB:  11%|█         | 9/81 [00:01<00:07,  9.51it/s]
Downloaded 0.01 GB:  17%|█▋        | 14/81 [00:01<00:04, 16.12it/s]
Downloaded 0.01 GB:  17%|█▋        | 14/81 [00:02<00:04, 16.12it/s]
Downloaded 0.02 GB:  17%|█▋        | 14/81 [00:02<00:04, 16.12it/s]
Downloaded 0.02 GB:  17%|█▋        | 14/81 [00:02<00:04, 16.12it/s]
Downloaded 0.02 GB:  17%|█▋        | 14/81 [00:02<00:04, 16.12it/s]
Downloaded 0.02 GB:  17%|█▋        | 14/81 [00:02<00:04, 16.12it/s]
Downloaded 0.02 GB:  17%|█▋        | 14/81 [00:02<00:04, 16.12it/s]
Downloaded 0.02 GB:  25%|██▍       | 20/81 [00:02<00:02, 23.08it/s]
Downloaded 0.02 GB:  25%|██▍       | 20/81 [00:02<00:02, 23.08it/s]
Downloaded 0.02 GB:  25%|██▍       | 20/81 [00:02<00:02, 23.08it/s]
Downloaded 0.02 GB:  25%|██▍       | 20/81 [00:02<00:02, 23.08it/s]
Downloaded 0.02 GB:  25%|██▍       | 20/81 [00:02<00:02, 23.08it/s]
Downloaded 0.02 GB:  25%|██▍       | 20/81 [00:02<00:02, 23.08it/s]
Downloaded 0.03 GB:  25%|██▍       | 20/81 [00:02<00:02, 23.08it/s]
Downloaded 0.03 GB:  32%|███▏      | 26/81 [00:02<00:01, 28.29it/s]
Downloaded 0.03 GB:  32%|███▏      | 26/81 [00:02<00:01, 28.29it/s]
Downloaded 0.03 GB:  32%|███▏      | 26/81 [00:02<00:01, 28.29it/s]
Downloaded 0.03 GB:  32%|███▏      | 26/81 [00:02<00:01, 28.29it/s]
Downloaded 0.03 GB:  32%|███▏      | 26/81 [00:02<00:01, 28.29it/s]
Downloaded 0.03 GB:  32%|███▏      | 26/81 [00:02<00:01, 28.29it/s]
Downloaded 0.03 GB:  32%|███▏      | 26/81 [00:02<00:01, 28.29it/s]
Downloaded 0.03 GB:  40%|███▉      | 32/81 [00:02<00:01, 32.13it/s]
Downloaded 0.03 GB:  40%|███▉      | 32/81 [00:02<00:01, 32.13it/s]
Downloaded 0.03 GB:  40%|███▉      | 32/81 [00:02<00:01, 32.13it/s]
Downloaded 0.03 GB:  40%|███▉      | 32/81 [00:02<00:01, 32.13it/s]
Downloaded 0.04 GB:  40%|███▉      | 32/81 [00:02<00:01, 32.13it/s]
Downloaded 0.04 GB:  40%|███▉      | 32/81 [00:02<00:01, 32.13it/s]
Downloaded 0.04 GB:  40%|███▉      | 32/81 [00:02<00:01, 32.13it/s]
Downloaded 0.04 GB:  47%|████▋     | 38/81 [00:02<00:01, 34.96it/s]
Downloaded 0.04 GB:  47%|████▋     | 38/81 [00:02<00:01, 34.96it/s]
Downloaded 0.04 GB:  47%|████▋     | 38/81 [00:02<00:01, 34.96it/s]
Downloaded 0.04 GB:  47%|████▋     | 38/81 [00:02<00:01, 34.96it/s]
Downloaded 0.04 GB:  47%|████▋     | 38/81 [00:02<00:01, 34.96it/s]
Downloaded 0.04 GB:  47%|████▋     | 38/81 [00:02<00:01, 34.96it/s]
Downloaded 0.04 GB:  53%|█████▎    | 43/81 [00:02<00:00, 38.17it/s]
Downloaded 0.04 GB:  53%|█████▎    | 43/81 [00:02<00:00, 38.17it/s]
Downloaded 0.04 GB:  53%|█████▎    | 43/81 [00:02<00:00, 38.17it/s]
Downloaded 0.04 GB:  53%|█████▎    | 43/81 [00:02<00:00, 38.17it/s]
Downloaded 0.05 GB:  53%|█████▎    | 43/81 [00:02<00:00, 38.17it/s]
Downloaded 0.05 GB:  53%|█████▎    | 43/81 [00:02<00:00, 38.17it/s]
Downloaded 0.05 GB:  59%|█████▉    | 48/81 [00:02<00:00, 38.83it/s]
Downloaded 0.05 GB:  59%|█████▉    | 48/81 [00:02<00:00, 38.83it/s]
Downloaded 0.05 GB:  59%|█████▉    | 48/81 [00:02<00:00, 38.83it/s]
Downloaded 0.05 GB:  59%|█████▉    | 48/81 [00:02<00:00, 38.83it/s]
Downloaded 0.05 GB:  59%|█████▉    | 48/81 [00:02<00:00, 38.83it/s]
Downloaded 0.05 GB:  59%|█████▉    | 48/81 [00:02<00:00, 38.83it/s]
Downloaded 0.05 GB:  65%|██████▌   | 53/81 [00:02<00:00, 39.49it/s]
Downloaded 0.05 GB:  65%|██████▌   | 53/81 [00:02<00:00, 39.49it/s]
Downloaded 0.05 GB:  65%|██████▌   | 53/81 [00:02<00:00, 39.49it/s]
Downloaded 0.05 GB:  65%|██████▌   | 53/81 [00:02<00:00, 39.49it/s]
Downloaded 0.06 GB:  65%|██████▌   | 53/81 [00:03<00:00, 39.49it/s]
Downloaded 0.06 GB:  65%|██████▌   | 53/81 [00:03<00:00, 39.49it/s]
Downloaded 0.06 GB:  72%|███████▏  | 58/81 [00:03<00:00, 40.68it/s]
Downloaded 0.06 GB:  72%|███████▏  | 58/81 [00:03<00:00, 40.68it/s]
Downloaded 0.06 GB:  72%|███████▏  | 58/81 [00:03<00:00, 40.68it/s]
Downloaded 0.06 GB:  72%|███████▏  | 58/81 [00:03<00:00, 40.68it/s]
Downloaded 0.06 GB:  72%|███████▏  | 58/81 [00:03<00:00, 40.68it/s]
Downloaded 0.06 GB:  72%|███████▏  | 58/81 [00:03<00:00, 40.68it/s]
Downloaded 0.06 GB:  78%|███████▊  | 63/81 [00:03<00:00, 40.24it/s]
Downloaded 0.06 GB:  78%|███████▊  | 63/81 [00:03<00:00, 40.24it/s]
Downloaded 0.06 GB:  78%|███████▊  | 63/81 [00:03<00:00, 40.24it/s]
Downloaded 0.06 GB:  78%|███████▊  | 63/81 [00:03<00:00, 40.24it/s]
Downloaded 0.07 GB:  78%|███████▊  | 63/81 [00:03<00:00, 40.24it/s]
Downloaded 0.07 GB:  78%|███████▊  | 63/81 [00:03<00:00, 40.24it/s]
Downloaded 0.07 GB:  84%|████████▍ | 68/81 [00:03<00:00, 42.45it/s]
Downloaded 0.07 GB:  84%|████████▍ | 68/81 [00:03<00:00, 42.45it/s]
Downloaded 0.07 GB:  84%|████████▍ | 68/81 [00:03<00:00, 42.45it/s]
Downloaded 0.07 GB:  84%|████████▍ | 68/81 [00:03<00:00, 42.45it/s]
Downloaded 0.07 GB:  84%|████████▍ | 68/81 [00:03<00:00, 42.45it/s]
Downloaded 0.07 GB:  84%|████████▍ | 68/81 [00:03<00:00, 42.45it/s]
Downloaded 0.07 GB:  90%|█████████ | 73/81 [00:03<00:00, 40.37it/s]
Downloaded 0.07 GB:  90%|█████████ | 73/81 [00:03<00:00, 40.37it/s]
Downloaded 0.07 GB:  90%|█████████ | 73/81 [00:03<00:00, 40.37it/s]
Downloaded 0.07 GB:  90%|█████████ | 73/81 [00:03<00:00, 40.37it/s]
Downloaded 0.08 GB:  90%|█████████ | 73/81 [00:03<00:00, 40.37it/s]
Downloaded 0.08 GB:  90%|█████████ | 73/81 [00:03<00:00, 40.37it/s]
Downloaded 0.08 GB:  96%|█████████▋| 78/81 [00:03<00:00, 40.55it/s]
Downloaded 0.08 GB:  96%|█████████▋| 78/81 [00:03<00:00, 40.55it/s]
Downloaded 0.08 GB:  96%|█████████▋| 78/81 [00:03<00:00, 40.55it/s]
Downloaded 0.08 GB:  96%|█████████▋| 78/81 [00:03<00:00, 40.55it/s]
Downloaded 0.08 GB: 100%|██████████| 81/81 [00:03<00:00, 22.89it/s]
Extracting dataset/arxiv.zip
Processing...
Loading necessary files...
This might take a while.
Processing graphs...

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 23431.87it/s]
Converting graphs into PyG objects...

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 6186.29it/s]
Saving...
Done!
/usr/local/lib/python3.11/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
Initialization start: network data collected.
2025-07-29 16:30:37,464	INFO worker.py:1429 -- Using address 192.168.59.106:6379 set in the environment variable RAY_ADDRESS
2025-07-29 16:30:37,465	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.59.106:6379...
2025-07-29 16:30:37,471	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.59.106:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=6007, ip=192.168.28.30)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=6007, ip=192.168.28.30)[0m   return torch.load(io.BytesIO(b))
/usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
[36m(Trainer pid=6039, ip=192.168.52.89)[0m Running GCN_arxiv
Running GCN_arxiv
//Log init_time: 5553.793 ms //end
//Log Large1 init network: 351968.0 //end
//Log Large2 init network: 167882.0 //end
//Log Large3 init network: 173371.0 //end
//Log Large4 init network: 137477.0 //end
//Log Server init network: 98309674.0 //end
//Log Initialization Communication Cost (MB): 94.55 //end
Pretrain start time recorded.
//pretrain_time: 7.438000000000001 ms//end
//Log Max memory for Large1: 1731743744.0 //end
//Log Max memory for Large2: 1299841024.0 //end
//Log Max memory for Large3: 1734197248.0 //end
//Log Max memory for Large4: 1294774272.0 //end
//Log Max memory for Server: 2628685824.0 //end
//Log Large1 network: 885877.0 //end
//Log Large2 network: 867743.0 //end
//Log Large3 network: 1091231.0 //end
//Log Large4 network: 863856.0 //end
//Log Server network: 2884402.0 //end
//Log Total Actual Pretrain Comm Cost: 6.29 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.0882
Round 2: Global Test Accuracy = 0.0918
Round 3: Global Test Accuracy = 0.0698
Round 4: Global Test Accuracy = 0.0801
Round 5: Global Test Accuracy = 0.1421
Round 6: Global Test Accuracy = 0.2233
Round 7: Global Test Accuracy = 0.2514
Round 8: Global Test Accuracy = 0.2626
Round 9: Global Test Accuracy = 0.2707
Round 10: Global Test Accuracy = 0.2786
Round 11: Global Test Accuracy = 0.2866
Round 12: Global Test Accuracy = 0.2940
Round 13: Global Test Accuracy = 0.3019
Round 14: Global Test Accuracy = 0.3097
Round 15: Global Test Accuracy = 0.3156
Round 16: Global Test Accuracy = 0.3222
Round 17: Global Test Accuracy = 0.3297
Round 18: Global Test Accuracy = 0.3365
Round 19: Global Test Accuracy = 0.3432
Round 20: Global Test Accuracy = 0.3504
Round 21: Global Test Accuracy = 0.3572
Round 22: Global Test Accuracy = 0.3633
Round 23: Global Test Accuracy = 0.3695
Round 24: Global Test Accuracy = 0.3762
Round 25: Global Test Accuracy = 0.3830
Round 26: Global Test Accuracy = 0.3889
Round 27: Global Test Accuracy = 0.3956
Round 28: Global Test Accuracy = 0.4003
Round 29: Global Test Accuracy = 0.4049
Round 30: Global Test Accuracy = 0.4106
Round 31: Global Test Accuracy = 0.4156
Round 32: Global Test Accuracy = 0.4207
Round 33: Global Test Accuracy = 0.4255
Round 34: Global Test Accuracy = 0.4287
Round 35: Global Test Accuracy = 0.4330
Round 36: Global Test Accuracy = 0.4371
Round 37: Global Test Accuracy = 0.4386
Round 38: Global Test Accuracy = 0.4417
Round 39: Global Test Accuracy = 0.4460
Round 40: Global Test Accuracy = 0.4487
Round 41: Global Test Accuracy = 0.4516
Round 42: Global Test Accuracy = 0.4542
Round 43: Global Test Accuracy = 0.4580
Round 44: Global Test Accuracy = 0.4599
Round 45: Global Test Accuracy = 0.4613
Round 46: Global Test Accuracy = 0.4636
Round 47: Global Test Accuracy = 0.4660
Round 48: Global Test Accuracy = 0.4679
Round 49: Global Test Accuracy = 0.4686
Round 50: Global Test Accuracy = 0.4720
Round 51: Global Test Accuracy = 0.4733
Round 52: Global Test Accuracy = 0.4755
Round 53: Global Test Accuracy = 0.4777
Round 54: Global Test Accuracy = 0.4792
Round 55: Global Test Accuracy = 0.4815
Round 56: Global Test Accuracy = 0.4844
Round 57: Global Test Accuracy = 0.4852
Round 58: Global Test Accuracy = 0.4859
Round 59: Global Test Accuracy = 0.4871
Round 60: Global Test Accuracy = 0.4895
Round 61: Global Test Accuracy = 0.4904
Round 62: Global Test Accuracy = 0.4914
Round 63: Global Test Accuracy = 0.4921
Round 64: Global Test Accuracy = 0.4928
Round 65: Global Test Accuracy = 0.4933
Round 66: Global Test Accuracy = 0.4945
Round 67: Global Test Accuracy = 0.4954
Round 68: Global Test Accuracy = 0.4963
Round 69: Global Test Accuracy = 0.4973
Round 70: Global Test Accuracy = 0.4982
Round 71: Global Test Accuracy = 0.4989
Round 72: Global Test Accuracy = 0.5007
Round 73: Global Test Accuracy = 0.5007
Round 74: Global Test Accuracy = 0.5026
Round 75: Global Test Accuracy = 0.5026
Round 76: Global Test Accuracy = 0.5042
Round 77: Global Test Accuracy = 0.5047
Round 78: Global Test Accuracy = 0.5052
Round 79: Global Test Accuracy = 0.5056
Round 80: Global Test Accuracy = 0.5063
Round 81: Global Test Accuracy = 0.5058
Round 82: Global Test Accuracy = 0.5075
Round 83: Global Test Accuracy = 0.5087
Round 84: Global Test Accuracy = 0.5091
Round 85: Global Test Accuracy = 0.5098
Round 86: Global Test Accuracy = 0.5094
Round 87: Global Test Accuracy = 0.5107
Round 88: Global Test Accuracy = 0.5103
Round 89: Global Test Accuracy = 0.5122
Round 90: Global Test Accuracy = 0.5122
Round 91: Global Test Accuracy = 0.5130
Round 92: Global Test Accuracy = 0.5138
Round 93: Global Test Accuracy = 0.5148
Round 94: Global Test Accuracy = 0.5147
Round 95: Global Test Accuracy = 0.5158
Round 96: Global Test Accuracy = 0.5172
Round 97: Global Test Accuracy = 0.5172
Round 98: Global Test Accuracy = 0.5175
Round 99: Global Test Accuracy = 0.5175
Round 100: Global Test Accuracy = 0.5173
Round 101: Global Test Accuracy = 0.5177
Round 102: Global Test Accuracy = 0.5184
Round 103: Global Test Accuracy = 0.5185
Round 104: Global Test Accuracy = 0.5202
Round 105: Global Test Accuracy = 0.5202
Round 106: Global Test Accuracy = 0.5210
Round 107: Global Test Accuracy = 0.5212
Round 108: Global Test Accuracy = 0.5217
Round 109: Global Test Accuracy = 0.5224
Round 110: Global Test Accuracy = 0.5227
Round 111: Global Test Accuracy = 0.5231
Round 112: Global Test Accuracy = 0.5236
Round 113: Global Test Accuracy = 0.5232
Round 114: Global Test Accuracy = 0.5228
Round 115: Global Test Accuracy = 0.5229
Round 116: Global Test Accuracy = 0.5233
Round 117: Global Test Accuracy = 0.5242
Round 118: Global Test Accuracy = 0.5245
Round 119: Global Test Accuracy = 0.5249
Round 120: Global Test Accuracy = 0.5248
Round 121: Global Test Accuracy = 0.5254
Round 122: Global Test Accuracy = 0.5264
Round 123: Global Test Accuracy = 0.5265
Round 124: Global Test Accuracy = 0.5274
Round 125: Global Test Accuracy = 0.5279
Round 126: Global Test Accuracy = 0.5289
Round 127: Global Test Accuracy = 0.5291
Round 128: Global Test Accuracy = 0.5284
Round 129: Global Test Accuracy = 0.5279
Round 130: Global Test Accuracy = 0.5283
Round 131: Global Test Accuracy = 0.5299
Round 132: Global Test Accuracy = 0.5298
Round 133: Global Test Accuracy = 0.5299
Round 134: Global Test Accuracy = 0.5301
Round 135: Global Test Accuracy = 0.5304
Round 136: Global Test Accuracy = 0.5302
Round 137: Global Test Accuracy = 0.5298
Round 138: Global Test Accuracy = 0.5307
Round 139: Global Test Accuracy = 0.5312
Round 140: Global Test Accuracy = 0.5316
Round 141: Global Test Accuracy = 0.5319
Round 142: Global Test Accuracy = 0.5321
Round 143: Global Test Accuracy = 0.5322
Round 144: Global Test Accuracy = 0.5322
Round 145: Global Test Accuracy = 0.5321
Round 146: Global Test Accuracy = 0.5320
Round 147: Global Test Accuracy = 0.5330
Round 148: Global Test Accuracy = 0.5333
Round 149: Global Test Accuracy = 0.5339
Round 150: Global Test Accuracy = 0.5336
Round 151: Global Test Accuracy = 0.5338
Round 152: Global Test Accuracy = 0.5336
Round 153: Global Test Accuracy = 0.5344
Round 154: Global Test Accuracy = 0.5340
Round 155: Global Test Accuracy = 0.5347
Round 156: Global Test Accuracy = 0.5350
Round 157: Global Test Accuracy = 0.5356
Round 158: Global Test Accuracy = 0.5353
Round 159: Global Test Accuracy = 0.5352
Round 160: Global Test Accuracy = 0.5351
Round 161: Global Test Accuracy = 0.5355
Round 162: Global Test Accuracy = 0.5349
Round 163: Global Test Accuracy = 0.5353
Round 164: Global Test Accuracy = 0.5357
Round 165: Global Test Accuracy = 0.5364
Round 166: Global Test Accuracy = 0.5360
Round 167: Global Test Accuracy = 0.5361
Round 168: Global Test Accuracy = 0.5373
Round 169: Global Test Accuracy = 0.5376
Round 170: Global Test Accuracy = 0.5373
Round 171: Global Test Accuracy = 0.5371
Round 172: Global Test Accuracy = 0.5369
Round 173: Global Test Accuracy = 0.5372
Round 174: Global Test Accuracy = 0.5372
Round 175: Global Test Accuracy = 0.5374
Round 176: Global Test Accuracy = 0.5379
Round 177: Global Test Accuracy = 0.5378
Round 178: Global Test Accuracy = 0.5380
Round 179: Global Test Accuracy = 0.5377
Round 180: Global Test Accuracy = 0.5390
Round 181: Global Test Accuracy = 0.5391
Round 182: Global Test Accuracy = 0.5389
Round 183: Global Test Accuracy = 0.5388
Round 184: Global Test Accuracy = 0.5390
Round 185: Global Test Accuracy = 0.5394
Round 186: Global Test Accuracy = 0.5393
Round 187: Global Test Accuracy = 0.5398
Round 188: Global Test Accuracy = 0.5399
Round 189: Global Test Accuracy = 0.5401
Round 190: Global Test Accuracy = 0.5401
Round 191: Global Test Accuracy = 0.5406
Round 192: Global Test Accuracy = 0.5406
Round 193: Global Test Accuracy = 0.5409
Round 194: Global Test Accuracy = 0.5401
Round 195: Global Test Accuracy = 0.5406
Round 196: Global Test Accuracy = 0.5407
Round 197: Global Test Accuracy = 0.5410
Round 198: Global Test Accuracy = 0.5414
Round 199: Global Test Accuracy = 0.5414
Round 200: Global Test Accuracy = 0.5409
//train_time: 50295.835 ms//end
//Log Max memory for Large1: 2629128192.0 //end
//Log Max memory for Large2: 1761566720.0 //end
//Log Max memory for Large3: 2597904384.0 //end
//Log Max memory for Large4: 1681973248.0 //end
//Log Max memory for Server: 2627895296.0 //end
//Log Large1 network: 112205020.0 //end
//Log Large2 network: 75336140.0 //end
//Log Large3 network: 112304376.0 //end
//Log Large4 network: 75372405.0 //end
//Log Server network: 372321625.0 //end
//Log Total Actual Train Comm Cost: 712.91 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: ogbn-arxiv, Batch Size: -1, Trainers: 10, Hops: 0, IID Beta: 10000.0 => Training Time = 80.30 seconds
average_final_test_loss, 1.6860679970173558
Average test accuracy, 0.5409336872209535

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge
----------------------------------------------------------------------------------------------------
0        773.7        16871    21632    0.046        0.036
1        850.5        16806    24764    0.051        0.034
2        796.8        17039    22922    0.047        0.035
3        766.9        16870    23360    0.045        0.033
4        797.7        17001    23446    0.047        0.034
5        985.6        16865    23676    0.058        0.042
6        976.7        16986    24104    0.058        0.041
7        906.9        16956    21480    0.053        0.042
8        864.9        16952    22390    0.051        0.039
9        826.1        16997    23020    0.049        0.036
====================================================================================================
Total Memory Usage: 8545.9 MB (8.35 GB)
Total Nodes: 169343, Total Edges: 230794
Average Memory per Trainer: 854.6 MB
Average Nodes per Trainer: 16934.3
Average Edges per Trainer: 23079.4
Max Memory: 985.6 MB (Trainer 5)
Min Memory: 766.9 MB (Trainer 3)
Overall Memory/Node Ratio: 0.050 MB/node
Overall Memory/Edge Ratio: 0.037 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 668.58 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
ogbn-arxiv,10000.0,-1,115.9,0.54,80.4,668.6,985.6,0.402,0.167,0
================================================================================
[36m(Trainer pid=6051, ip=192.168.31.174)[0m Running GCN_arxiv[32m [repeated 9x across cluster][0m
[36m(Trainer pid=6056, ip=192.168.54.57)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=6056, ip=192.168.54.57)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: ogbn-arxiv, Trainers: 10, Distribution: average, IID Beta: 100.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'ogbn-arxiv', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 100.0, 'distribution_type': 'average', 'gpu': False}
/usr/local/lib/python3.11/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):
/usr/local/lib/python3.11/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):
/usr/local/lib/python3.11/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
Initialization start: network data collected.
2025-07-29 16:32:39,539	INFO worker.py:1429 -- Using address 192.168.59.106:6379 set in the environment variable RAY_ADDRESS
2025-07-29 16:32:39,539	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.59.106:6379...
2025-07-29 16:32:39,546	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.59.106:8265 [39m[22m
Changing method to FedAvg
/usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
[36m(Trainer pid=6710, ip=192.168.31.174)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=6710, ip=192.168.31.174)[0m   return torch.load(io.BytesIO(b))
[36m(Trainer pid=6710, ip=192.168.31.174)[0m Running GCN_arxiv
Running GCN_arxiv
//Log init_time: 5719.648 ms //end
//Log Large1 init network: 126727.0 //end
//Log Large2 init network: 217950.0 //end
//Log Large3 init network: 174986.0 //end
//Log Large4 init network: 158311.0 //end
//Log Server init network: 98296674.0 //end
//Log Initialization Communication Cost (MB): 94.39 //end
Pretrain start time recorded.
//pretrain_time: 7.359 ms//end
//Log Max memory for Large1: 1283002368.0 //end
//Log Max memory for Large2: 1718300672.0 //end
//Log Max memory for Large3: 1287147520.0 //end
//Log Max memory for Large4: 1713848320.0 //end
//Log Max memory for Server: 2610061312.0 //end
//Log Large1 network: 849200.0 //end
//Log Large2 network: 1028476.0 //end
//Log Large3 network: 805269.0 //end
//Log Large4 network: 1073384.0 //end
//Log Server network: 2872973.0 //end
//Log Total Actual Pretrain Comm Cost: 6.32 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.0884
Round 2: Global Test Accuracy = 0.0930
Round 3: Global Test Accuracy = 0.0694
Round 4: Global Test Accuracy = 0.0775
Round 5: Global Test Accuracy = 0.1344
Round 6: Global Test Accuracy = 0.2177
Round 7: Global Test Accuracy = 0.2498
Round 8: Global Test Accuracy = 0.2625
Round 9: Global Test Accuracy = 0.2711
Round 10: Global Test Accuracy = 0.2788
Round 11: Global Test Accuracy = 0.2858
Round 12: Global Test Accuracy = 0.2939
Round 13: Global Test Accuracy = 0.3015
Round 14: Global Test Accuracy = 0.3093
Round 15: Global Test Accuracy = 0.3162
Round 16: Global Test Accuracy = 0.3236
Round 17: Global Test Accuracy = 0.3307
Round 18: Global Test Accuracy = 0.3380
Round 19: Global Test Accuracy = 0.3437
Round 20: Global Test Accuracy = 0.3498
Round 21: Global Test Accuracy = 0.3564
Round 22: Global Test Accuracy = 0.3627
Round 23: Global Test Accuracy = 0.3695
Round 24: Global Test Accuracy = 0.3763
Round 25: Global Test Accuracy = 0.3832
Round 26: Global Test Accuracy = 0.3885
Round 27: Global Test Accuracy = 0.3942
Round 28: Global Test Accuracy = 0.4003
Round 29: Global Test Accuracy = 0.4053
Round 30: Global Test Accuracy = 0.4094
Round 31: Global Test Accuracy = 0.4142
Round 32: Global Test Accuracy = 0.4183
Round 33: Global Test Accuracy = 0.4218
Round 34: Global Test Accuracy = 0.4254
Round 35: Global Test Accuracy = 0.4302
Round 36: Global Test Accuracy = 0.4338
Round 37: Global Test Accuracy = 0.4397
Round 38: Global Test Accuracy = 0.4425
Round 39: Global Test Accuracy = 0.4478
Round 40: Global Test Accuracy = 0.4511
Round 41: Global Test Accuracy = 0.4555
Round 42: Global Test Accuracy = 0.4575
Round 43: Global Test Accuracy = 0.4599
Round 44: Global Test Accuracy = 0.4612
Round 45: Global Test Accuracy = 0.4638
Round 46: Global Test Accuracy = 0.4672
Round 47: Global Test Accuracy = 0.4696
Round 48: Global Test Accuracy = 0.4708
Round 49: Global Test Accuracy = 0.4714
Round 50: Global Test Accuracy = 0.4727
Round 51: Global Test Accuracy = 0.4751
Round 52: Global Test Accuracy = 0.4769
Round 53: Global Test Accuracy = 0.4794
Round 54: Global Test Accuracy = 0.4821
Round 55: Global Test Accuracy = 0.4844
Round 56: Global Test Accuracy = 0.4858
Round 57: Global Test Accuracy = 0.4872
Round 58: Global Test Accuracy = 0.4881
Round 59: Global Test Accuracy = 0.4896
Round 60: Global Test Accuracy = 0.4907
Round 61: Global Test Accuracy = 0.4926
Round 62: Global Test Accuracy = 0.4934
Round 63: Global Test Accuracy = 0.4946
Round 64: Global Test Accuracy = 0.4955
Round 65: Global Test Accuracy = 0.4963
Round 66: Global Test Accuracy = 0.4971
Round 67: Global Test Accuracy = 0.4976
Round 68: Global Test Accuracy = 0.4979
Round 69: Global Test Accuracy = 0.4998
Round 70: Global Test Accuracy = 0.5010
Round 71: Global Test Accuracy = 0.5014
Round 72: Global Test Accuracy = 0.5032
Round 73: Global Test Accuracy = 0.5045
Round 74: Global Test Accuracy = 0.5047
Round 75: Global Test Accuracy = 0.5059
Round 76: Global Test Accuracy = 0.5062
Round 77: Global Test Accuracy = 0.5066
Round 78: Global Test Accuracy = 0.5071
Round 79: Global Test Accuracy = 0.5084
Round 80: Global Test Accuracy = 0.5091
Round 81: Global Test Accuracy = 0.5097
Round 82: Global Test Accuracy = 0.5111
Round 83: Global Test Accuracy = 0.5112
Round 84: Global Test Accuracy = 0.5122
Round 85: Global Test Accuracy = 0.5141
Round 86: Global Test Accuracy = 0.5144
Round 87: Global Test Accuracy = 0.5142
Round 88: Global Test Accuracy = 0.5149
Round 89: Global Test Accuracy = 0.5151
Round 90: Global Test Accuracy = 0.5142
Round 91: Global Test Accuracy = 0.5153
Round 92: Global Test Accuracy = 0.5157
Round 93: Global Test Accuracy = 0.5165
Round 94: Global Test Accuracy = 0.5176
Round 95: Global Test Accuracy = 0.5181
Round 96: Global Test Accuracy = 0.5184
Round 97: Global Test Accuracy = 0.5197
Round 98: Global Test Accuracy = 0.5201
Round 99: Global Test Accuracy = 0.5201
Round 100: Global Test Accuracy = 0.5204
Round 101: Global Test Accuracy = 0.5207
Round 102: Global Test Accuracy = 0.5210
Round 103: Global Test Accuracy = 0.5221
Round 104: Global Test Accuracy = 0.5230
Round 105: Global Test Accuracy = 0.5230
Round 106: Global Test Accuracy = 0.5231
Round 107: Global Test Accuracy = 0.5233
Round 108: Global Test Accuracy = 0.5238
Round 109: Global Test Accuracy = 0.5250
Round 110: Global Test Accuracy = 0.5254
Round 111: Global Test Accuracy = 0.5266
Round 112: Global Test Accuracy = 0.5269
Round 113: Global Test Accuracy = 0.5266
Round 114: Global Test Accuracy = 0.5261
Round 115: Global Test Accuracy = 0.5259
Round 116: Global Test Accuracy = 0.5264
Round 117: Global Test Accuracy = 0.5264
Round 118: Global Test Accuracy = 0.5274
Round 119: Global Test Accuracy = 0.5280
Round 120: Global Test Accuracy = 0.5288
Round 121: Global Test Accuracy = 0.5296
Round 122: Global Test Accuracy = 0.5292
Round 123: Global Test Accuracy = 0.5298
Round 124: Global Test Accuracy = 0.5309
Round 125: Global Test Accuracy = 0.5315
Round 126: Global Test Accuracy = 0.5310
Round 127: Global Test Accuracy = 0.5314
Round 128: Global Test Accuracy = 0.5317
Round 129: Global Test Accuracy = 0.5325
Round 130: Global Test Accuracy = 0.5331
Round 131: Global Test Accuracy = 0.5332
Round 132: Global Test Accuracy = 0.5335
Round 133: Global Test Accuracy = 0.5332
Round 134: Global Test Accuracy = 0.5334
Round 135: Global Test Accuracy = 0.5334
Round 136: Global Test Accuracy = 0.5344
Round 137: Global Test Accuracy = 0.5350
Round 138: Global Test Accuracy = 0.5356
Round 139: Global Test Accuracy = 0.5360
Round 140: Global Test Accuracy = 0.5357
Round 141: Global Test Accuracy = 0.5360
Round 142: Global Test Accuracy = 0.5363
Round 143: Global Test Accuracy = 0.5367
Round 144: Global Test Accuracy = 0.5364
Round 145: Global Test Accuracy = 0.5364
Round 146: Global Test Accuracy = 0.5369
Round 147: Global Test Accuracy = 0.5369
Round 148: Global Test Accuracy = 0.5368
Round 149: Global Test Accuracy = 0.5369
Round 150: Global Test Accuracy = 0.5370
Round 151: Global Test Accuracy = 0.5375
Round 152: Global Test Accuracy = 0.5380
Round 153: Global Test Accuracy = 0.5377
Round 154: Global Test Accuracy = 0.5382
Round 155: Global Test Accuracy = 0.5383
Round 156: Global Test Accuracy = 0.5387
Round 157: Global Test Accuracy = 0.5388
Round 158: Global Test Accuracy = 0.5388
Round 159: Global Test Accuracy = 0.5389
Round 160: Global Test Accuracy = 0.5382
Round 161: Global Test Accuracy = 0.5382
Round 162: Global Test Accuracy = 0.5387
Round 163: Global Test Accuracy = 0.5390
Round 164: Global Test Accuracy = 0.5394
Round 165: Global Test Accuracy = 0.5397
Round 166: Global Test Accuracy = 0.5399
Round 167: Global Test Accuracy = 0.5400
Round 168: Global Test Accuracy = 0.5403
Round 169: Global Test Accuracy = 0.5404
Round 170: Global Test Accuracy = 0.5408
Round 171: Global Test Accuracy = 0.5411
Round 172: Global Test Accuracy = 0.5411
Round 173: Global Test Accuracy = 0.5410
Round 174: Global Test Accuracy = 0.5413
Round 175: Global Test Accuracy = 0.5414
Round 176: Global Test Accuracy = 0.5416
Round 177: Global Test Accuracy = 0.5417
Round 178: Global Test Accuracy = 0.5419
Round 179: Global Test Accuracy = 0.5423
Round 180: Global Test Accuracy = 0.5425
Round 181: Global Test Accuracy = 0.5430
Round 182: Global Test Accuracy = 0.5430
Round 183: Global Test Accuracy = 0.5424
Round 184: Global Test Accuracy = 0.5430
Round 185: Global Test Accuracy = 0.5430
Round 186: Global Test Accuracy = 0.5428
Round 187: Global Test Accuracy = 0.5434
Round 188: Global Test Accuracy = 0.5431
Round 189: Global Test Accuracy = 0.5433
Round 190: Global Test Accuracy = 0.5435
Round 191: Global Test Accuracy = 0.5433
Round 192: Global Test Accuracy = 0.5432
Round 193: Global Test Accuracy = 0.5434
Round 194: Global Test Accuracy = 0.5436
Round 195: Global Test Accuracy = 0.5439
Round 196: Global Test Accuracy = 0.5441
Round 197: Global Test Accuracy = 0.5443
Round 198: Global Test Accuracy = 0.5443
Round 199: Global Test Accuracy = 0.5441
Round 200: Global Test Accuracy = 0.5444
//train_time: 51261.454999999994 ms//end
//Log Max memory for Large1: 1676812288.0 //end
//Log Max memory for Large2: 2336256000.0 //end
//Log Max memory for Large3: 1687732224.0 //end
//Log Max memory for Large4: 2359857152.0 //end
//Log Max memory for Server: 2620862464.0 //end
//Log Large1 network: 75334683.0 //end
//Log Large2 network: 112225005.0 //end
//Log Large3 network: 75390288.0 //end
//Log Large4 network: 112355880.0 //end
//Log Server network: 372253583.0 //end
//Log Total Actual Train Comm Cost: 712.93 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: ogbn-arxiv, Batch Size: -1, Trainers: 10, Hops: 0, IID Beta: 100.0 => Training Time = 81.26 seconds
average_final_test_loss, 1.6858488666818232
Average test accuracy, 0.5443696891138408

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge
----------------------------------------------------------------------------------------------------
0        888.2        17006    20892    0.052        0.043
1        759.7        17081    23684    0.044        0.032
2        755.5        17045    26144    0.044        0.029
3        847.4        16512    23238    0.051        0.036
4        821.2        17173    22990    0.048        0.036
5        845.6        17124    26944    0.049        0.031
6        816.4        16886    21266    0.048        0.038
7        854.9        16384    23084    0.052        0.037
8        899.8        17065    25160    0.053        0.036
9        885.6        17067    20556    0.052        0.043
====================================================================================================
Total Memory Usage: 8374.3 MB (8.18 GB)
Total Nodes: 169343, Total Edges: 233958
Average Memory per Trainer: 837.4 MB
Average Nodes per Trainer: 16934.3
Average Edges per Trainer: 23395.8
Max Memory: 899.8 MB (Trainer 8)
Min Memory: 755.5 MB (Trainer 2)
Overall Memory/Node Ratio: 0.049 MB/node
Overall Memory/Edge Ratio: 0.036 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 668.58 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
ogbn-arxiv,100.0,-1,117.1,0.54,81.3,668.6,899.8,0.407,0.167,0
================================================================================
[36m(Trainer pid=6778, ip=192.168.52.89)[0m Running GCN_arxiv[32m [repeated 9x across cluster][0m
[36m(Trainer pid=6719, ip=192.168.54.57)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=6719, ip=192.168.54.57)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m

--------------------------------------------------------------------------------
Running experiment 1/1:
Dataset: ogbn-arxiv, Trainers: 10, Distribution: average, IID Beta: 10.0, Hops: 0, Batch Size: -1
--------------------------------------------------------------------------------

config:  {'fedgraph_task': 'NC', 'num_cpus_per_trainer': 3, 'num_gpus_per_trainer': 0, 'use_cluster': True, 'global_rounds': 200, 'local_step': 1, 'learning_rate': 0.1, 'num_layers': 2, 'logdir': './runs', 'use_huggingface': False, 'saveto_huggingface': False, 'use_encryption': False, 'dataset': 'ogbn-arxiv', 'method': 'FedAvg', 'batch_size': -1, 'n_trainer': 10, 'num_hops': 0, 'iid_beta': 10.0, 'distribution_type': 'average', 'gpu': False}
/usr/local/lib/python3.11/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):
/usr/local/lib/python3.11/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):
/usr/local/lib/python3.11/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.data, self.slices = torch.load(self.processed_paths[0])
Initialization start: network data collected.
2025-07-29 16:34:42,799	INFO worker.py:1429 -- Using address 192.168.59.106:6379 set in the environment variable RAY_ADDRESS
2025-07-29 16:34:42,800	INFO worker.py:1564 -- Connecting to existing Ray cluster at address: 192.168.59.106:6379...
2025-07-29 16:34:42,807	INFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://192.168.59.106:8265 [39m[22m
Changing method to FedAvg
[36m(Trainer pid=7469, ip=192.168.31.174)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(Trainer pid=7469, ip=192.168.31.174)[0m   return torch.load(io.BytesIO(b))
/usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
[36m(Trainer pid=7432, ip=192.168.28.30)[0m Running GCN_arxiv
Running GCN_arxiv
//Log init_time: 5642.955 ms //end
//Log Large1 init network: 202172.0 //end
//Log Large2 init network: 128212.0 //end
//Log Large3 init network: 163814.0 //end
//Log Large4 init network: 166021.0 //end
//Log Server init network: 98566300.0 //end
//Log Initialization Communication Cost (MB): 94.63 //end
Pretrain start time recorded.
//pretrain_time: 6.845000000000001 ms//end
//Log Max memory for Large1: 1723650048.0 //end
//Log Max memory for Large2: 1289830400.0 //end
//Log Max memory for Large3: 1719332864.0 //end
//Log Max memory for Large4: 1275772928.0 //end
//Log Max memory for Server: 2644639744.0 //end
//Log Large1 network: 1013464.0 //end
//Log Large2 network: 847494.0 //end
//Log Large3 network: 1069879.0 //end
//Log Large4 network: 798147.0 //end
//Log Server network: 2874083.0 //end
//Log Total Actual Pretrain Comm Cost: 6.30 MB //end
Pretrain end time recorded and duration set to gauge.
Train start: network data collected.
global_rounds 200
Round 1: Global Test Accuracy = 0.0886
Round 2: Global Test Accuracy = 0.0921
Round 3: Global Test Accuracy = 0.0698
Round 4: Global Test Accuracy = 0.0801
Round 5: Global Test Accuracy = 0.1397
Round 6: Global Test Accuracy = 0.2206
Round 7: Global Test Accuracy = 0.2533
Round 8: Global Test Accuracy = 0.2637
Round 9: Global Test Accuracy = 0.2715
Round 10: Global Test Accuracy = 0.2788
Round 11: Global Test Accuracy = 0.2854
Round 12: Global Test Accuracy = 0.2941
Round 13: Global Test Accuracy = 0.3021
Round 14: Global Test Accuracy = 0.3094
Round 15: Global Test Accuracy = 0.3167
Round 16: Global Test Accuracy = 0.3234
Round 17: Global Test Accuracy = 0.3314
Round 18: Global Test Accuracy = 0.3382
Round 19: Global Test Accuracy = 0.3453
Round 20: Global Test Accuracy = 0.3510
Round 21: Global Test Accuracy = 0.3569
Round 22: Global Test Accuracy = 0.3639
Round 23: Global Test Accuracy = 0.3717
Round 24: Global Test Accuracy = 0.3788
Round 25: Global Test Accuracy = 0.3856
Round 26: Global Test Accuracy = 0.3920
Round 27: Global Test Accuracy = 0.3973
Round 28: Global Test Accuracy = 0.4021
Round 29: Global Test Accuracy = 0.4056
Round 30: Global Test Accuracy = 0.4103
Round 31: Global Test Accuracy = 0.4152
Round 32: Global Test Accuracy = 0.4193
Round 33: Global Test Accuracy = 0.4251
Round 34: Global Test Accuracy = 0.4301
Round 35: Global Test Accuracy = 0.4347
Round 36: Global Test Accuracy = 0.4385
Round 37: Global Test Accuracy = 0.4430
Round 38: Global Test Accuracy = 0.4458
Round 39: Global Test Accuracy = 0.4489
Round 40: Global Test Accuracy = 0.4522
Round 41: Global Test Accuracy = 0.4533
Round 42: Global Test Accuracy = 0.4559
Round 43: Global Test Accuracy = 0.4579
Round 44: Global Test Accuracy = 0.4612
Round 45: Global Test Accuracy = 0.4653
Round 46: Global Test Accuracy = 0.4682
Round 47: Global Test Accuracy = 0.4707
Round 48: Global Test Accuracy = 0.4719
Round 49: Global Test Accuracy = 0.4746
Round 50: Global Test Accuracy = 0.4766
Round 51: Global Test Accuracy = 0.4775
Round 52: Global Test Accuracy = 0.4795
Round 53: Global Test Accuracy = 0.4818
Round 54: Global Test Accuracy = 0.4826
Round 55: Global Test Accuracy = 0.4847
Round 56: Global Test Accuracy = 0.4860
Round 57: Global Test Accuracy = 0.4874
Round 58: Global Test Accuracy = 0.4891
Round 59: Global Test Accuracy = 0.4889
Round 60: Global Test Accuracy = 0.4900
Round 61: Global Test Accuracy = 0.4915
Round 62: Global Test Accuracy = 0.4937
Round 63: Global Test Accuracy = 0.4938
Round 64: Global Test Accuracy = 0.4953
Round 65: Global Test Accuracy = 0.4962
Round 66: Global Test Accuracy = 0.4975
Round 67: Global Test Accuracy = 0.4994
Round 68: Global Test Accuracy = 0.4997
Round 69: Global Test Accuracy = 0.5010
Round 70: Global Test Accuracy = 0.5013
Round 71: Global Test Accuracy = 0.5019
Round 72: Global Test Accuracy = 0.5026
Round 73: Global Test Accuracy = 0.5044
Round 74: Global Test Accuracy = 0.5054
Round 75: Global Test Accuracy = 0.5054
Round 76: Global Test Accuracy = 0.5067
Round 77: Global Test Accuracy = 0.5066
Round 78: Global Test Accuracy = 0.5082
Round 79: Global Test Accuracy = 0.5091
Round 80: Global Test Accuracy = 0.5104
Round 81: Global Test Accuracy = 0.5104
Round 82: Global Test Accuracy = 0.5112
Round 83: Global Test Accuracy = 0.5123
Round 84: Global Test Accuracy = 0.5127
Round 85: Global Test Accuracy = 0.5132
Round 86: Global Test Accuracy = 0.5130
Round 87: Global Test Accuracy = 0.5149
Round 88: Global Test Accuracy = 0.5154
Round 89: Global Test Accuracy = 0.5162
Round 90: Global Test Accuracy = 0.5168
Round 91: Global Test Accuracy = 0.5168
Round 92: Global Test Accuracy = 0.5164
Round 93: Global Test Accuracy = 0.5172
Round 94: Global Test Accuracy = 0.5172
Round 95: Global Test Accuracy = 0.5180
Round 96: Global Test Accuracy = 0.5193
Round 97: Global Test Accuracy = 0.5193
Round 98: Global Test Accuracy = 0.5196
Round 99: Global Test Accuracy = 0.5199
Round 100: Global Test Accuracy = 0.5203
Round 101: Global Test Accuracy = 0.5204
Round 102: Global Test Accuracy = 0.5212
Round 103: Global Test Accuracy = 0.5219
Round 104: Global Test Accuracy = 0.5221
Round 105: Global Test Accuracy = 0.5222
Round 106: Global Test Accuracy = 0.5218
Round 107: Global Test Accuracy = 0.5223
Round 108: Global Test Accuracy = 0.5230
Round 109: Global Test Accuracy = 0.5235
Round 110: Global Test Accuracy = 0.5245
Round 111: Global Test Accuracy = 0.5245
Round 112: Global Test Accuracy = 0.5250
Round 113: Global Test Accuracy = 0.5254
Round 114: Global Test Accuracy = 0.5254
Round 115: Global Test Accuracy = 0.5265
Round 116: Global Test Accuracy = 0.5277
Round 117: Global Test Accuracy = 0.5280
Round 118: Global Test Accuracy = 0.5278
Round 119: Global Test Accuracy = 0.5285
Round 120: Global Test Accuracy = 0.5282
Round 121: Global Test Accuracy = 0.5282
Round 122: Global Test Accuracy = 0.5287
Round 123: Global Test Accuracy = 0.5276
Round 124: Global Test Accuracy = 0.5277
Round 125: Global Test Accuracy = 0.5281
Round 126: Global Test Accuracy = 0.5285
Round 127: Global Test Accuracy = 0.5287
Round 128: Global Test Accuracy = 0.5298
Round 129: Global Test Accuracy = 0.5306
Round 130: Global Test Accuracy = 0.5304
Round 131: Global Test Accuracy = 0.5311
Round 132: Global Test Accuracy = 0.5314
Round 133: Global Test Accuracy = 0.5319
Round 134: Global Test Accuracy = 0.5326
Round 135: Global Test Accuracy = 0.5326
Round 136: Global Test Accuracy = 0.5341
Round 137: Global Test Accuracy = 0.5338
Round 138: Global Test Accuracy = 0.5337
Round 139: Global Test Accuracy = 0.5335
Round 140: Global Test Accuracy = 0.5339
Round 141: Global Test Accuracy = 0.5345
Round 142: Global Test Accuracy = 0.5335
Round 143: Global Test Accuracy = 0.5339
Round 144: Global Test Accuracy = 0.5346
Round 145: Global Test Accuracy = 0.5352
Round 146: Global Test Accuracy = 0.5350
Round 147: Global Test Accuracy = 0.5359
Round 148: Global Test Accuracy = 0.5361
Round 149: Global Test Accuracy = 0.5358
Round 150: Global Test Accuracy = 0.5363
Round 151: Global Test Accuracy = 0.5363
Round 152: Global Test Accuracy = 0.5358
Round 153: Global Test Accuracy = 0.5358
Round 154: Global Test Accuracy = 0.5371
Round 155: Global Test Accuracy = 0.5370
Round 156: Global Test Accuracy = 0.5370
Round 157: Global Test Accuracy = 0.5378
Round 158: Global Test Accuracy = 0.5383
Round 159: Global Test Accuracy = 0.5389
Round 160: Global Test Accuracy = 0.5393
Round 161: Global Test Accuracy = 0.5394
Round 162: Global Test Accuracy = 0.5387
Round 163: Global Test Accuracy = 0.5388
Round 164: Global Test Accuracy = 0.5395
Round 165: Global Test Accuracy = 0.5391
Round 166: Global Test Accuracy = 0.5386
Round 167: Global Test Accuracy = 0.5388
Round 168: Global Test Accuracy = 0.5392
Round 169: Global Test Accuracy = 0.5391
Round 170: Global Test Accuracy = 0.5393
Round 171: Global Test Accuracy = 0.5395
Round 172: Global Test Accuracy = 0.5399
Round 173: Global Test Accuracy = 0.5402
Round 174: Global Test Accuracy = 0.5409
Round 175: Global Test Accuracy = 0.5410
Round 176: Global Test Accuracy = 0.5416
Round 177: Global Test Accuracy = 0.5417
Round 178: Global Test Accuracy = 0.5417
Round 179: Global Test Accuracy = 0.5418
Round 180: Global Test Accuracy = 0.5419
Round 181: Global Test Accuracy = 0.5419
Round 182: Global Test Accuracy = 0.5417
Round 183: Global Test Accuracy = 0.5415
Round 184: Global Test Accuracy = 0.5420
Round 185: Global Test Accuracy = 0.5423
Round 186: Global Test Accuracy = 0.5427
Round 187: Global Test Accuracy = 0.5426
Round 188: Global Test Accuracy = 0.5429
Round 189: Global Test Accuracy = 0.5427
Round 190: Global Test Accuracy = 0.5429
Round 191: Global Test Accuracy = 0.5423
Round 192: Global Test Accuracy = 0.5427
Round 193: Global Test Accuracy = 0.5429
Round 194: Global Test Accuracy = 0.5430
Round 195: Global Test Accuracy = 0.5433
Round 196: Global Test Accuracy = 0.5439
Round 197: Global Test Accuracy = 0.5443
Round 198: Global Test Accuracy = 0.5444
Round 199: Global Test Accuracy = 0.5451
Round 200: Global Test Accuracy = 0.5450
//train_time: 54191.168 ms//end
//Log Max memory for Large1: 2314264576.0 //end
//Log Max memory for Large2: 1709047808.0 //end
//Log Max memory for Large3: 2504142848.0 //end
//Log Max memory for Large4: 1817780224.0 //end
//Log Max memory for Server: 2645434368.0 //end
//Log Large1 network: 112292864.0 //end
//Log Large2 network: 75427913.0 //end
//Log Large3 network: 112454446.0 //end
//Log Large4 network: 75488030.0 //end
//Log Server network: 372575857.0 //end
//Log Total Actual Train Comm Cost: 713.58 MB //end
Train end time recorded and duration set to gauge.
[Training Time] Dataset: ogbn-arxiv, Batch Size: -1, Trainers: 10, Hops: 0, IID Beta: 10.0 => Training Time = 84.19 seconds
average_final_test_loss, 1.687911433552851
Average test accuracy, 0.5450075098244964

================================================================================
INDIVIDUAL TRAINER MEMORY USAGE
================================================================================

====================================================================================================
TRAINER MEMORY vs LOCAL GRAPH SIZE
====================================================================================================
Trainer  Memory(MB)   Nodes    Edges    Memory/Node  Memory/Edge
----------------------------------------------------------------------------------------------------
0        861.1        17136    21762    0.050        0.040
1        941.3        17184    24406    0.055        0.039
2        870.6        16025    19848    0.054        0.044
3        805.3        16523    18038    0.049        0.045
4        837.5        17314    19186    0.048        0.044
5        754.4        16798    21392    0.045        0.035
6        876.3        17000    36984    0.052        0.024
7        897.9        16953    26674    0.053        0.034
8        877.3        17264    32386    0.051        0.027
9        860.7        17146    29612    0.050        0.029
====================================================================================================
Total Memory Usage: 8582.3 MB (8.38 GB)
Total Nodes: 169343, Total Edges: 250288
Average Memory per Trainer: 858.2 MB
Average Nodes per Trainer: 16934.3
Average Edges per Trainer: 25028.8
Max Memory: 941.3 MB (Trainer 1)
Min Memory: 754.4 MB (Trainer 5)
Overall Memory/Node Ratio: 0.051 MB/node
Overall Memory/Edge Ratio: 0.034 MB/edge
====================================================================================================
//Log Theoretical Pretrain Comm Cost: 0.00 MB //end
//Log Theoretical Train Comm Cost: 668.58 MB //end

================================================================================
CSV FORMAT RESULT:
DS,IID,BS,Time[s],FinalAcc[%],CompTime[s],CommCost[MB],PeakMem[MB],AvgRoundTime[s],ModelSize[MB],TotalParams
ogbn-arxiv,10.0,-1,119.9,0.55,84.3,668.6,941.3,0.421,0.167,0
================================================================================
[36m(Trainer pid=7470, ip=192.168.31.174)[0m Running GCN_arxiv[32m [repeated 9x across cluster][0m
[36m(Trainer pid=7474, ip=192.168.54.57)[0m /usr/local/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.[32m [repeated 9x across cluster][0m
[36m(Trainer pid=7474, ip=192.168.54.57)[0m   return torch.load(io.BytesIO(b))[32m [repeated 9x across cluster][0m
Benchmark completed.

------------------------------------------
Job 'raysubmit_QXevCUFTcSACnJti' succeeded
------------------------------------------
